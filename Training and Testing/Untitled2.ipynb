{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc25b0-04ff-4f9d-9848-1f31944f9bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41744 samples for train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▎                                                                  | 5130/41744 [00:17<02:13, 273.53it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True  # Better for performance\n",
    "\n",
    "set_seed()\n",
    "\n",
    "class MDVRDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.classes = ['hc', 'pd']\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        split_dir = os.path.join(self.root_dir, self.split)\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            for patient_folder in os.listdir(class_dir):\n",
    "                patient_path = os.path.join(class_dir, patient_folder)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    for img_name in os.listdir(patient_path):\n",
    "                        if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            img_path = os.path.join(patient_path, img_name)\n",
    "                            self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "        print(f\"Loaded {len(self.samples)} samples for {self.split} split\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class SpecAugment:\n",
    "    def __init__(self, freq_mask=30, time_mask=50, num_masks=2):\n",
    "        self.freq_mask = freq_mask\n",
    "        self.time_mask = time_mask\n",
    "        self.num_masks = num_masks\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        _, n_mels, n_time = img_tensor.shape\n",
    "        \n",
    "        # Frequency masking\n",
    "        for _ in range(self.num_masks):\n",
    "            f = np.random.randint(1, self.freq_mask)\n",
    "            f0 = np.random.randint(0, n_mels - f)\n",
    "            img_tensor[:, f0:f0+f, :] = 0\n",
    "            \n",
    "        # Time masking\n",
    "        for _ in range(self.num_masks):\n",
    "            t = np.random.randint(1, self.time_mask)\n",
    "            t0 = np.random.randint(0, n_time - t)\n",
    "            img_tensor[:, :, t0:t0+t] = 0\n",
    "            \n",
    "        return transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "class GaussianNoise:\n",
    "    def __init__(self, std=0.02):\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn_like(tensor) * self.std\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_ch, in_ch, kernel_size, \n",
    "                                 groups=in_ch, padding='same')\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))\n",
    "\n",
    "class ParkinsonNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            DepthwiseSeparableConv(1, 32, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            DepthwiseSeparableConv(32, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2,4)),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, device='cuda', patience=5):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, verbose=True)\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloaders[phase], desc=f'{phase} phase'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, 1)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_acc)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                    early_stop_counter = 0\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "            \n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "            \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, history\n",
    "\n",
    "def run_experiment(data_dir, batch_size=64, num_epochs=50):\n",
    "    # Calculate dataset statistics\n",
    "    temp_dataset = MDVRDataset(data_dir, 'train', transforms.Compose([\n",
    "        transforms.Resize((496, 200)), transforms.ToTensor()\n",
    "    ]))\n",
    "    pixels = torch.cat([img.view(-1) for img, _ in tqdm(temp_dataset)])\n",
    "    mean, std = pixels.mean().item(), pixels.std().item()\n",
    "    \n",
    "    # Enhanced transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((496, 200)),\n",
    "        SpecAugment(freq_mask=30, time_mask=50),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        GaussianNoise(std=0.02)\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((496, 200)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MDVRDataset(data_dir, 'train', train_transform)\n",
    "    val_dataset = MDVRDataset(data_dir, 'val', test_transform)\n",
    "    test_dataset = MDVRDataset(data_dir, 'test', test_transform)\n",
    "    \n",
    "    # Create dataloaders with optimized settings\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size, shuffle=True, \n",
    "                          num_workers=4, pin_memory=True, persistent_workers=True),\n",
    "        'val': DataLoader(val_dataset, batch_size, num_workers=4),\n",
    "        'test': DataLoader(test_dataset, batch_size, num_workers=4)\n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ParkinsonNet()\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Reduces overconfidence\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    # Train\n",
    "    model, history = train_model(\n",
    "        model, dataloaders, criterion, optimizer, \n",
    "        num_epochs=num_epochs, patience=7\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = test_model(model, dataloaders['test'], criterion)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Val')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Val')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    \n",
    "    return model, history, (test_loss, test_acc)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, test_results = run_experiment(\n",
    "        data_dir=\"C:/NPersonal/Projects/SDP/Prediction Stuff/Dataset/MDVR\",\n",
    "        batch_size=64,\n",
    "        num_epochs=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807caa3-bc83-4629-8be6-3a97e925d1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
