{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdcb5a2-4cac-4a0b-b095-bc398d0f2731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 154732 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/train', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/train']\n",
      "✅ Loaded 37807 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/val', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/val']\n",
      "✅ Loaded 41886 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/test', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/test']\n",
      "✅ DataLoaders ready! Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOTS = [\n",
    "    r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR\",\n",
    "    r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian\"\n",
    "]\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0  # Optimize based on CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ParkinsonSpectrogramDataset(Dataset):\n",
    "    def __init__(self, root_dirs, transform=None):\n",
    "        if isinstance(root_dirs, str):  # If a single path is given, convert it to a list\n",
    "            root_dirs = [root_dirs]\n",
    "        self.root_dirs = root_dirs\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for root_dir in self.root_dirs:\n",
    "            for class_name in ['HC', 'PD']:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                if not os.path.exists(class_dir):\n",
    "                    continue\n",
    "\n",
    "                # Traverse subfolders inside HC/PD\n",
    "                for patient_folder in os.listdir(class_dir):\n",
    "                    patient_path = os.path.join(class_dir, patient_folder)\n",
    "                    if os.path.isdir(patient_path):  # Ensure it's a directory\n",
    "                        for img_file in os.listdir(patient_path):\n",
    "                            if img_file.lower().endswith('.png'):  # Only PNG images\n",
    "                                img_path = os.path.join(patient_path, img_file)\n",
    "                                samples.append((img_path, 0 if class_name == 'HC' else 1))\n",
    "\n",
    "        print(f\"✅ Loaded {len(samples)} samples from {self.root_dirs}\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Data Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Only slight translations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize between -1 and 1\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Create Datasets (Loading from both MDVR & Italian datasets)\n",
    "train_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'train') for root in DATA_ROOTS], transform=train_transform)\n",
    "val_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'val') for root in DATA_ROOTS], transform=test_transform)\n",
    "test_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'test') for root in DATA_ROOTS], transform=test_transform)\n",
    "\n",
    "# Optimized DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"✅ DataLoaders ready! Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c81c7-e3de-4760-94e9-1c9df210654f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97065c92-540c-4bd0-bb9b-5a12c9566f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c2933-ce5b-4931-b935-943c56b9787d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714e7be7-09d0-4f56-aa3c-f3e99a807514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1650\n",
      "Total GPU memory: 3.90 GB\n",
      "Available GPU memory: 0.00 GB reserved\n",
      "Model created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nigmu/pytorch_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.6568, Acc: 61.50%\n",
      "Batch 100/19342, Loss: 0.6138, Acc: 65.62%\n",
      "Batch 150/19342, Loss: 0.5842, Acc: 68.17%\n",
      "Batch 200/19342, Loss: 0.5869, Acc: 68.62%\n",
      "Batch 250/19342, Loss: 0.5763, Acc: 69.70%\n",
      "Batch 300/19342, Loss: 0.5626, Acc: 70.71%\n",
      "Batch 350/19342, Loss: 0.5540, Acc: 71.46%\n",
      "Batch 400/19342, Loss: 0.5463, Acc: 72.12%\n",
      "Batch 450/19342, Loss: 0.5400, Acc: 72.25%\n",
      "Batch 500/19342, Loss: 0.5301, Acc: 72.85%\n",
      "Batch 550/19342, Loss: 0.5232, Acc: 73.14%\n",
      "Batch 600/19342, Loss: 0.5202, Acc: 73.58%\n",
      "Batch 650/19342, Loss: 0.5189, Acc: 73.67%\n",
      "Batch 700/19342, Loss: 0.5131, Acc: 74.11%\n",
      "Batch 750/19342, Loss: 0.5094, Acc: 74.23%\n",
      "Batch 800/19342, Loss: 0.5060, Acc: 74.52%\n",
      "Batch 850/19342, Loss: 0.5017, Acc: 74.91%\n",
      "Batch 900/19342, Loss: 0.4960, Acc: 75.42%\n",
      "Batch 950/19342, Loss: 0.4964, Acc: 75.34%\n",
      "Batch 1000/19342, Loss: 0.4919, Acc: 75.66%\n",
      "Batch 1050/19342, Loss: 0.4887, Acc: 75.82%\n",
      "Batch 1100/19342, Loss: 0.4860, Acc: 76.01%\n",
      "Batch 1150/19342, Loss: 0.4802, Acc: 76.32%\n",
      "Batch 1200/19342, Loss: 0.4814, Acc: 76.32%\n",
      "Batch 1250/19342, Loss: 0.4802, Acc: 76.39%\n",
      "Batch 1300/19342, Loss: 0.4775, Acc: 76.55%\n",
      "Batch 1350/19342, Loss: 0.4767, Acc: 76.69%\n",
      "Batch 1400/19342, Loss: 0.4755, Acc: 76.73%\n",
      "Batch 1450/19342, Loss: 0.4733, Acc: 76.86%\n",
      "Batch 1500/19342, Loss: 0.4732, Acc: 76.93%\n",
      "Batch 1550/19342, Loss: 0.4712, Acc: 77.01%\n",
      "Batch 1600/19342, Loss: 0.4705, Acc: 77.00%\n",
      "Batch 1650/19342, Loss: 0.4693, Acc: 77.14%\n",
      "Batch 1700/19342, Loss: 0.4678, Acc: 77.26%\n",
      "Batch 1750/19342, Loss: 0.4666, Acc: 77.31%\n",
      "Batch 1800/19342, Loss: 0.4651, Acc: 77.31%\n",
      "Batch 1850/19342, Loss: 0.4640, Acc: 77.47%\n",
      "Batch 1900/19342, Loss: 0.4624, Acc: 77.61%\n",
      "Batch 1950/19342, Loss: 0.4608, Acc: 77.75%\n",
      "Batch 2000/19342, Loss: 0.4600, Acc: 77.76%\n",
      "Batch 2050/19342, Loss: 0.4585, Acc: 77.92%\n",
      "Batch 2100/19342, Loss: 0.4568, Acc: 77.98%\n",
      "Batch 2150/19342, Loss: 0.4553, Acc: 78.02%\n",
      "Batch 2200/19342, Loss: 0.4536, Acc: 78.09%\n",
      "Batch 2250/19342, Loss: 0.4524, Acc: 78.15%\n",
      "Batch 2300/19342, Loss: 0.4510, Acc: 78.22%\n",
      "Batch 2350/19342, Loss: 0.4512, Acc: 78.24%\n",
      "Batch 2400/19342, Loss: 0.4499, Acc: 78.32%\n",
      "Batch 2450/19342, Loss: 0.4490, Acc: 78.38%\n",
      "Batch 2500/19342, Loss: 0.4482, Acc: 78.42%\n",
      "Batch 2550/19342, Loss: 0.4474, Acc: 78.43%\n",
      "Batch 2600/19342, Loss: 0.4466, Acc: 78.47%\n",
      "Batch 2650/19342, Loss: 0.4453, Acc: 78.55%\n",
      "Batch 2700/19342, Loss: 0.4445, Acc: 78.60%\n",
      "Batch 2750/19342, Loss: 0.4430, Acc: 78.69%\n",
      "Batch 2800/19342, Loss: 0.4423, Acc: 78.74%\n",
      "Batch 2850/19342, Loss: 0.4408, Acc: 78.88%\n",
      "Batch 2900/19342, Loss: 0.4396, Acc: 78.94%\n",
      "Batch 2950/19342, Loss: 0.4391, Acc: 78.97%\n",
      "Batch 3000/19342, Loss: 0.4392, Acc: 79.01%\n",
      "Batch 3050/19342, Loss: 0.4382, Acc: 79.08%\n",
      "Batch 3100/19342, Loss: 0.4366, Acc: 79.17%\n",
      "Batch 3150/19342, Loss: 0.4360, Acc: 79.17%\n",
      "Batch 3200/19342, Loss: 0.4352, Acc: 79.21%\n",
      "Batch 3250/19342, Loss: 0.4356, Acc: 79.18%\n",
      "Batch 3300/19342, Loss: 0.4357, Acc: 79.18%\n",
      "Batch 3350/19342, Loss: 0.4351, Acc: 79.22%\n",
      "Batch 3400/19342, Loss: 0.4337, Acc: 79.28%\n",
      "Batch 3450/19342, Loss: 0.4332, Acc: 79.32%\n",
      "Batch 3500/19342, Loss: 0.4331, Acc: 79.33%\n",
      "Batch 3550/19342, Loss: 0.4321, Acc: 79.42%\n",
      "Batch 3600/19342, Loss: 0.4305, Acc: 79.53%\n",
      "Batch 3650/19342, Loss: 0.4306, Acc: 79.58%\n",
      "Batch 3700/19342, Loss: 0.4300, Acc: 79.61%\n",
      "Batch 3750/19342, Loss: 0.4293, Acc: 79.64%\n",
      "Batch 3800/19342, Loss: 0.4284, Acc: 79.68%\n",
      "Batch 3850/19342, Loss: 0.4274, Acc: 79.72%\n",
      "Batch 3900/19342, Loss: 0.4267, Acc: 79.74%\n",
      "Batch 3950/19342, Loss: 0.4265, Acc: 79.78%\n",
      "Batch 4000/19342, Loss: 0.4259, Acc: 79.82%\n",
      "Batch 4050/19342, Loss: 0.4260, Acc: 79.82%\n",
      "Batch 4100/19342, Loss: 0.4254, Acc: 79.85%\n",
      "Batch 4150/19342, Loss: 0.4247, Acc: 79.88%\n",
      "Batch 4200/19342, Loss: 0.4244, Acc: 79.90%\n",
      "Batch 4250/19342, Loss: 0.4237, Acc: 79.94%\n",
      "Batch 4300/19342, Loss: 0.4237, Acc: 79.95%\n",
      "Batch 4350/19342, Loss: 0.4230, Acc: 79.97%\n",
      "Batch 4400/19342, Loss: 0.4231, Acc: 79.97%\n",
      "Batch 4450/19342, Loss: 0.4227, Acc: 80.00%\n",
      "Batch 4500/19342, Loss: 0.4227, Acc: 80.00%\n",
      "Batch 4550/19342, Loss: 0.4227, Acc: 80.00%\n",
      "Batch 4600/19342, Loss: 0.4221, Acc: 80.02%\n",
      "Batch 4650/19342, Loss: 0.4211, Acc: 80.08%\n",
      "Batch 4700/19342, Loss: 0.4208, Acc: 80.09%\n",
      "Batch 4750/19342, Loss: 0.4197, Acc: 80.14%\n",
      "Batch 4800/19342, Loss: 0.4188, Acc: 80.20%\n",
      "Batch 4850/19342, Loss: 0.4179, Acc: 80.23%\n",
      "Batch 4900/19342, Loss: 0.4176, Acc: 80.25%\n",
      "Batch 4950/19342, Loss: 0.4172, Acc: 80.28%\n",
      "Batch 5000/19342, Loss: 0.4168, Acc: 80.29%\n",
      "Batch 5050/19342, Loss: 0.4161, Acc: 80.33%\n",
      "Batch 5100/19342, Loss: 0.4157, Acc: 80.35%\n",
      "Batch 5150/19342, Loss: 0.4156, Acc: 80.35%\n",
      "Batch 5200/19342, Loss: 0.4159, Acc: 80.34%\n",
      "Batch 5250/19342, Loss: 0.4155, Acc: 80.36%\n",
      "Batch 5300/19342, Loss: 0.4151, Acc: 80.38%\n",
      "Batch 5350/19342, Loss: 0.4147, Acc: 80.43%\n",
      "Batch 5400/19342, Loss: 0.4139, Acc: 80.48%\n",
      "Batch 5450/19342, Loss: 0.4134, Acc: 80.52%\n",
      "Batch 5500/19342, Loss: 0.4124, Acc: 80.58%\n",
      "Batch 5550/19342, Loss: 0.4124, Acc: 80.59%\n",
      "Batch 5600/19342, Loss: 0.4122, Acc: 80.61%\n",
      "Batch 5650/19342, Loss: 0.4115, Acc: 80.64%\n",
      "Batch 5700/19342, Loss: 0.4108, Acc: 80.68%\n",
      "Batch 5750/19342, Loss: 0.4102, Acc: 80.71%\n",
      "Batch 5800/19342, Loss: 0.4101, Acc: 80.71%\n",
      "Batch 5850/19342, Loss: 0.4097, Acc: 80.74%\n",
      "Batch 5900/19342, Loss: 0.4090, Acc: 80.78%\n",
      "Batch 5950/19342, Loss: 0.4085, Acc: 80.80%\n",
      "Batch 6000/19342, Loss: 0.4081, Acc: 80.80%\n",
      "Batch 6050/19342, Loss: 0.4076, Acc: 80.82%\n",
      "Batch 6100/19342, Loss: 0.4072, Acc: 80.82%\n",
      "Batch 6150/19342, Loss: 0.4069, Acc: 80.83%\n",
      "Batch 6200/19342, Loss: 0.4065, Acc: 80.85%\n",
      "Batch 6250/19342, Loss: 0.4058, Acc: 80.90%\n",
      "Batch 6300/19342, Loss: 0.4055, Acc: 80.91%\n",
      "Batch 6350/19342, Loss: 0.4051, Acc: 80.93%\n",
      "Batch 6400/19342, Loss: 0.4047, Acc: 80.96%\n",
      "Batch 6450/19342, Loss: 0.4045, Acc: 80.98%\n",
      "Batch 6500/19342, Loss: 0.4045, Acc: 81.00%\n",
      "Batch 6550/19342, Loss: 0.4037, Acc: 81.04%\n",
      "Batch 6600/19342, Loss: 0.4030, Acc: 81.07%\n",
      "Batch 6650/19342, Loss: 0.4024, Acc: 81.10%\n",
      "Batch 6700/19342, Loss: 0.4017, Acc: 81.14%\n",
      "Batch 6750/19342, Loss: 0.4017, Acc: 81.14%\n",
      "Batch 6800/19342, Loss: 0.4008, Acc: 81.21%\n",
      "Batch 6850/19342, Loss: 0.4001, Acc: 81.25%\n",
      "Batch 6900/19342, Loss: 0.3997, Acc: 81.26%\n",
      "Batch 6950/19342, Loss: 0.3991, Acc: 81.31%\n",
      "Batch 7000/19342, Loss: 0.3987, Acc: 81.33%\n",
      "Batch 7050/19342, Loss: 0.3984, Acc: 81.35%\n",
      "Batch 7100/19342, Loss: 0.3980, Acc: 81.36%\n",
      "Batch 7150/19342, Loss: 0.3980, Acc: 81.36%\n",
      "Batch 7200/19342, Loss: 0.3979, Acc: 81.37%\n",
      "Batch 7250/19342, Loss: 0.3976, Acc: 81.38%\n",
      "Batch 7300/19342, Loss: 0.3972, Acc: 81.40%\n",
      "Batch 7350/19342, Loss: 0.3968, Acc: 81.41%\n",
      "Batch 7400/19342, Loss: 0.3964, Acc: 81.43%\n",
      "Batch 7450/19342, Loss: 0.3961, Acc: 81.43%\n",
      "Batch 7500/19342, Loss: 0.3958, Acc: 81.47%\n",
      "Batch 7550/19342, Loss: 0.3956, Acc: 81.48%\n",
      "Batch 7600/19342, Loss: 0.3953, Acc: 81.50%\n",
      "Batch 7650/19342, Loss: 0.3945, Acc: 81.55%\n",
      "Batch 7700/19342, Loss: 0.3939, Acc: 81.58%\n",
      "Batch 7750/19342, Loss: 0.3937, Acc: 81.58%\n",
      "Batch 7800/19342, Loss: 0.3932, Acc: 81.60%\n",
      "Batch 7850/19342, Loss: 0.3933, Acc: 81.59%\n",
      "Batch 7900/19342, Loss: 0.3930, Acc: 81.60%\n",
      "Batch 7950/19342, Loss: 0.3926, Acc: 81.62%\n",
      "Batch 8000/19342, Loss: 0.3922, Acc: 81.64%\n",
      "Batch 8050/19342, Loss: 0.3920, Acc: 81.65%\n",
      "Batch 8100/19342, Loss: 0.3916, Acc: 81.67%\n",
      "Batch 8150/19342, Loss: 0.3913, Acc: 81.68%\n",
      "Batch 8200/19342, Loss: 0.3909, Acc: 81.70%\n",
      "Batch 8250/19342, Loss: 0.3905, Acc: 81.73%\n",
      "Batch 8300/19342, Loss: 0.3903, Acc: 81.73%\n",
      "Batch 8350/19342, Loss: 0.3896, Acc: 81.77%\n",
      "Batch 8400/19342, Loss: 0.3893, Acc: 81.79%\n",
      "Batch 8450/19342, Loss: 0.3889, Acc: 81.80%\n",
      "Batch 8500/19342, Loss: 0.3882, Acc: 81.84%\n",
      "Batch 8550/19342, Loss: 0.3879, Acc: 81.85%\n",
      "Batch 8600/19342, Loss: 0.3872, Acc: 81.88%\n",
      "Batch 8650/19342, Loss: 0.3871, Acc: 81.90%\n",
      "Batch 8700/19342, Loss: 0.3866, Acc: 81.93%\n",
      "Batch 8750/19342, Loss: 0.3862, Acc: 81.94%\n",
      "Batch 8800/19342, Loss: 0.3856, Acc: 81.96%\n",
      "Batch 8850/19342, Loss: 0.3850, Acc: 82.00%\n",
      "Batch 8900/19342, Loss: 0.3844, Acc: 82.03%\n",
      "Batch 8950/19342, Loss: 0.3841, Acc: 82.05%\n",
      "Batch 9000/19342, Loss: 0.3835, Acc: 82.09%\n",
      "Batch 9050/19342, Loss: 0.3829, Acc: 82.13%\n",
      "Batch 9100/19342, Loss: 0.3827, Acc: 82.13%\n",
      "Batch 9150/19342, Loss: 0.3824, Acc: 82.15%\n",
      "Batch 9200/19342, Loss: 0.3821, Acc: 82.17%\n",
      "Batch 9250/19342, Loss: 0.3817, Acc: 82.19%\n",
      "Batch 9300/19342, Loss: 0.3815, Acc: 82.20%\n",
      "Batch 9350/19342, Loss: 0.3811, Acc: 82.21%\n",
      "Batch 9400/19342, Loss: 0.3808, Acc: 82.23%\n",
      "Batch 9450/19342, Loss: 0.3806, Acc: 82.24%\n",
      "Batch 9500/19342, Loss: 0.3801, Acc: 82.26%\n",
      "Batch 9550/19342, Loss: 0.3798, Acc: 82.27%\n",
      "Batch 9600/19342, Loss: 0.3794, Acc: 82.30%\n",
      "Batch 9650/19342, Loss: 0.3789, Acc: 82.32%\n",
      "Batch 9700/19342, Loss: 0.3786, Acc: 82.34%\n",
      "Batch 9750/19342, Loss: 0.3783, Acc: 82.36%\n",
      "Batch 9800/19342, Loss: 0.3779, Acc: 82.38%\n",
      "Batch 9850/19342, Loss: 0.3778, Acc: 82.38%\n",
      "Batch 9900/19342, Loss: 0.3775, Acc: 82.40%\n",
      "Batch 9950/19342, Loss: 0.3774, Acc: 82.40%\n",
      "Batch 10000/19342, Loss: 0.3771, Acc: 82.42%\n",
      "Batch 10050/19342, Loss: 0.3769, Acc: 82.44%\n",
      "Batch 10100/19342, Loss: 0.3765, Acc: 82.45%\n",
      "Batch 10150/19342, Loss: 0.3761, Acc: 82.47%\n",
      "Batch 10200/19342, Loss: 0.3758, Acc: 82.48%\n",
      "Batch 10250/19342, Loss: 0.3755, Acc: 82.50%\n",
      "Batch 10300/19342, Loss: 0.3752, Acc: 82.52%\n",
      "Batch 10350/19342, Loss: 0.3747, Acc: 82.54%\n",
      "Batch 10400/19342, Loss: 0.3745, Acc: 82.55%\n",
      "Batch 10450/19342, Loss: 0.3743, Acc: 82.55%\n",
      "Batch 10500/19342, Loss: 0.3740, Acc: 82.58%\n",
      "Batch 10550/19342, Loss: 0.3739, Acc: 82.58%\n",
      "Batch 10600/19342, Loss: 0.3737, Acc: 82.59%\n",
      "Batch 10650/19342, Loss: 0.3735, Acc: 82.60%\n",
      "Batch 10700/19342, Loss: 0.3732, Acc: 82.63%\n",
      "Batch 10750/19342, Loss: 0.3730, Acc: 82.63%\n",
      "Batch 10800/19342, Loss: 0.3729, Acc: 82.65%\n",
      "Batch 10850/19342, Loss: 0.3728, Acc: 82.64%\n",
      "Batch 10900/19342, Loss: 0.3728, Acc: 82.65%\n",
      "Batch 10950/19342, Loss: 0.3724, Acc: 82.66%\n",
      "Batch 11000/19342, Loss: 0.3721, Acc: 82.68%\n",
      "Batch 11050/19342, Loss: 0.3717, Acc: 82.69%\n",
      "Batch 11100/19342, Loss: 0.3713, Acc: 82.72%\n",
      "Batch 11150/19342, Loss: 0.3710, Acc: 82.74%\n",
      "Batch 11200/19342, Loss: 0.3708, Acc: 82.74%\n",
      "Batch 11250/19342, Loss: 0.3704, Acc: 82.76%\n",
      "Batch 11300/19342, Loss: 0.3701, Acc: 82.78%\n",
      "Batch 11350/19342, Loss: 0.3700, Acc: 82.78%\n",
      "Batch 11400/19342, Loss: 0.3697, Acc: 82.80%\n",
      "Batch 11450/19342, Loss: 0.3695, Acc: 82.82%\n",
      "Batch 11500/19342, Loss: 0.3692, Acc: 82.83%\n",
      "Batch 11550/19342, Loss: 0.3689, Acc: 82.84%\n",
      "Batch 11600/19342, Loss: 0.3687, Acc: 82.86%\n",
      "Batch 11650/19342, Loss: 0.3684, Acc: 82.87%\n",
      "Batch 11700/19342, Loss: 0.3683, Acc: 82.87%\n",
      "Batch 11750/19342, Loss: 0.3680, Acc: 82.89%\n",
      "Batch 11800/19342, Loss: 0.3675, Acc: 82.91%\n",
      "Batch 11850/19342, Loss: 0.3673, Acc: 82.92%\n",
      "Batch 11900/19342, Loss: 0.3671, Acc: 82.94%\n",
      "Batch 11950/19342, Loss: 0.3668, Acc: 82.95%\n",
      "Batch 12000/19342, Loss: 0.3665, Acc: 82.97%\n",
      "Batch 12050/19342, Loss: 0.3661, Acc: 82.98%\n",
      "Batch 12100/19342, Loss: 0.3658, Acc: 83.00%\n",
      "Batch 12150/19342, Loss: 0.3654, Acc: 83.01%\n",
      "Batch 12200/19342, Loss: 0.3652, Acc: 83.03%\n",
      "Batch 12250/19342, Loss: 0.3650, Acc: 83.04%\n",
      "Batch 12300/19342, Loss: 0.3648, Acc: 83.06%\n",
      "Batch 12350/19342, Loss: 0.3647, Acc: 83.06%\n",
      "Batch 12400/19342, Loss: 0.3646, Acc: 83.06%\n",
      "Batch 12450/19342, Loss: 0.3642, Acc: 83.07%\n",
      "Batch 12500/19342, Loss: 0.3640, Acc: 83.09%\n",
      "Batch 12550/19342, Loss: 0.3639, Acc: 83.09%\n",
      "Batch 12600/19342, Loss: 0.3636, Acc: 83.11%\n",
      "Batch 12650/19342, Loss: 0.3632, Acc: 83.13%\n",
      "Batch 12700/19342, Loss: 0.3630, Acc: 83.14%\n",
      "Batch 12750/19342, Loss: 0.3628, Acc: 83.14%\n",
      "Batch 12800/19342, Loss: 0.3625, Acc: 83.16%\n",
      "Batch 12850/19342, Loss: 0.3622, Acc: 83.18%\n",
      "Batch 12900/19342, Loss: 0.3617, Acc: 83.20%\n",
      "Batch 12950/19342, Loss: 0.3614, Acc: 83.21%\n",
      "Batch 13000/19342, Loss: 0.3613, Acc: 83.22%\n",
      "Batch 13050/19342, Loss: 0.3611, Acc: 83.23%\n",
      "Batch 13100/19342, Loss: 0.3609, Acc: 83.24%\n",
      "Batch 13150/19342, Loss: 0.3606, Acc: 83.25%\n",
      "Batch 13200/19342, Loss: 0.3604, Acc: 83.26%\n",
      "Batch 13250/19342, Loss: 0.3601, Acc: 83.27%\n",
      "Batch 13300/19342, Loss: 0.3600, Acc: 83.28%\n",
      "Batch 13350/19342, Loss: 0.3596, Acc: 83.31%\n",
      "Batch 13400/19342, Loss: 0.3595, Acc: 83.32%\n",
      "Batch 13450/19342, Loss: 0.3594, Acc: 83.32%\n",
      "Batch 13500/19342, Loss: 0.3592, Acc: 83.32%\n",
      "Batch 13550/19342, Loss: 0.3589, Acc: 83.33%\n",
      "Batch 13600/19342, Loss: 0.3585, Acc: 83.35%\n",
      "Batch 13650/19342, Loss: 0.3582, Acc: 83.37%\n",
      "Batch 13700/19342, Loss: 0.3579, Acc: 83.38%\n",
      "Batch 13750/19342, Loss: 0.3576, Acc: 83.40%\n",
      "Batch 13800/19342, Loss: 0.3573, Acc: 83.41%\n",
      "Batch 13850/19342, Loss: 0.3571, Acc: 83.43%\n",
      "Batch 13900/19342, Loss: 0.3568, Acc: 83.43%\n",
      "Batch 13950/19342, Loss: 0.3565, Acc: 83.45%\n",
      "Batch 14000/19342, Loss: 0.3562, Acc: 83.46%\n",
      "Batch 14050/19342, Loss: 0.3559, Acc: 83.48%\n",
      "Batch 14100/19342, Loss: 0.3557, Acc: 83.49%\n",
      "Batch 14150/19342, Loss: 0.3555, Acc: 83.49%\n",
      "Batch 14200/19342, Loss: 0.3553, Acc: 83.50%\n",
      "Batch 14250/19342, Loss: 0.3553, Acc: 83.51%\n",
      "Batch 14300/19342, Loss: 0.3551, Acc: 83.52%\n",
      "Batch 14350/19342, Loss: 0.3549, Acc: 83.53%\n",
      "Batch 14400/19342, Loss: 0.3546, Acc: 83.54%\n",
      "Batch 14450/19342, Loss: 0.3543, Acc: 83.56%\n",
      "Batch 14500/19342, Loss: 0.3542, Acc: 83.56%\n",
      "Batch 14550/19342, Loss: 0.3540, Acc: 83.58%\n",
      "Batch 14600/19342, Loss: 0.3538, Acc: 83.59%\n",
      "Batch 14650/19342, Loss: 0.3536, Acc: 83.60%\n",
      "Batch 14700/19342, Loss: 0.3533, Acc: 83.61%\n",
      "Batch 14750/19342, Loss: 0.3529, Acc: 83.63%\n",
      "Batch 14800/19342, Loss: 0.3528, Acc: 83.63%\n",
      "Batch 14850/19342, Loss: 0.3527, Acc: 83.65%\n",
      "Batch 14900/19342, Loss: 0.3525, Acc: 83.66%\n",
      "Batch 14950/19342, Loss: 0.3523, Acc: 83.67%\n",
      "Batch 15000/19342, Loss: 0.3523, Acc: 83.66%\n",
      "Batch 15050/19342, Loss: 0.3522, Acc: 83.67%\n",
      "Batch 15100/19342, Loss: 0.3519, Acc: 83.69%\n",
      "Batch 15150/19342, Loss: 0.3518, Acc: 83.69%\n",
      "Batch 15200/19342, Loss: 0.3514, Acc: 83.71%\n",
      "Batch 15250/19342, Loss: 0.3512, Acc: 83.72%\n",
      "Batch 15300/19342, Loss: 0.3512, Acc: 83.72%\n",
      "Batch 15350/19342, Loss: 0.3511, Acc: 83.73%\n",
      "Batch 15400/19342, Loss: 0.3508, Acc: 83.74%\n",
      "Batch 15450/19342, Loss: 0.3507, Acc: 83.75%\n",
      "Batch 15500/19342, Loss: 0.3504, Acc: 83.77%\n",
      "Batch 15550/19342, Loss: 0.3502, Acc: 83.77%\n",
      "Batch 15600/19342, Loss: 0.3498, Acc: 83.79%\n",
      "Batch 15650/19342, Loss: 0.3497, Acc: 83.80%\n",
      "Batch 15700/19342, Loss: 0.3494, Acc: 83.81%\n",
      "Batch 15750/19342, Loss: 0.3492, Acc: 83.83%\n",
      "Batch 15800/19342, Loss: 0.3490, Acc: 83.84%\n",
      "Batch 15850/19342, Loss: 0.3489, Acc: 83.85%\n",
      "Batch 15900/19342, Loss: 0.3486, Acc: 83.86%\n",
      "Batch 15950/19342, Loss: 0.3484, Acc: 83.87%\n",
      "Batch 16000/19342, Loss: 0.3482, Acc: 83.88%\n",
      "Batch 16050/19342, Loss: 0.3480, Acc: 83.88%\n",
      "Batch 16100/19342, Loss: 0.3479, Acc: 83.89%\n",
      "Batch 16150/19342, Loss: 0.3478, Acc: 83.90%\n",
      "Batch 16200/19342, Loss: 0.3476, Acc: 83.91%\n",
      "Batch 16250/19342, Loss: 0.3474, Acc: 83.92%\n",
      "Batch 16300/19342, Loss: 0.3473, Acc: 83.93%\n",
      "Batch 16350/19342, Loss: 0.3473, Acc: 83.92%\n",
      "Batch 16400/19342, Loss: 0.3471, Acc: 83.94%\n",
      "Batch 16450/19342, Loss: 0.3468, Acc: 83.95%\n",
      "Batch 16500/19342, Loss: 0.3468, Acc: 83.95%\n",
      "Batch 16550/19342, Loss: 0.3467, Acc: 83.96%\n",
      "Batch 16600/19342, Loss: 0.3465, Acc: 83.96%\n",
      "Batch 16650/19342, Loss: 0.3462, Acc: 83.98%\n",
      "Batch 16700/19342, Loss: 0.3461, Acc: 83.98%\n",
      "Batch 16750/19342, Loss: 0.3459, Acc: 83.98%\n",
      "Batch 16800/19342, Loss: 0.3458, Acc: 83.99%\n",
      "Batch 16850/19342, Loss: 0.3456, Acc: 84.00%\n",
      "Batch 16900/19342, Loss: 0.3452, Acc: 84.02%\n",
      "Batch 16950/19342, Loss: 0.3451, Acc: 84.02%\n",
      "Batch 17000/19342, Loss: 0.3449, Acc: 84.03%\n",
      "Batch 17050/19342, Loss: 0.3446, Acc: 84.05%\n",
      "Batch 17100/19342, Loss: 0.3445, Acc: 84.06%\n",
      "Batch 17150/19342, Loss: 0.3443, Acc: 84.06%\n",
      "Batch 17200/19342, Loss: 0.3440, Acc: 84.07%\n",
      "Batch 17250/19342, Loss: 0.3438, Acc: 84.08%\n",
      "Batch 17300/19342, Loss: 0.3437, Acc: 84.08%\n",
      "Batch 17350/19342, Loss: 0.3435, Acc: 84.09%\n",
      "Batch 17400/19342, Loss: 0.3432, Acc: 84.11%\n",
      "Batch 17450/19342, Loss: 0.3430, Acc: 84.12%\n",
      "Batch 17500/19342, Loss: 0.3429, Acc: 84.12%\n",
      "Batch 17550/19342, Loss: 0.3427, Acc: 84.13%\n",
      "Batch 17600/19342, Loss: 0.3426, Acc: 84.13%\n",
      "Batch 17650/19342, Loss: 0.3424, Acc: 84.14%\n",
      "Batch 17700/19342, Loss: 0.3423, Acc: 84.15%\n",
      "Batch 17750/19342, Loss: 0.3421, Acc: 84.16%\n",
      "Batch 17800/19342, Loss: 0.3418, Acc: 84.17%\n",
      "Batch 17850/19342, Loss: 0.3417, Acc: 84.18%\n",
      "Batch 17900/19342, Loss: 0.3416, Acc: 84.19%\n",
      "Batch 17950/19342, Loss: 0.3416, Acc: 84.19%\n",
      "Batch 18000/19342, Loss: 0.3416, Acc: 84.19%\n",
      "Batch 18050/19342, Loss: 0.3415, Acc: 84.19%\n",
      "Batch 18100/19342, Loss: 0.3412, Acc: 84.21%\n",
      "Batch 18150/19342, Loss: 0.3410, Acc: 84.22%\n",
      "Batch 18200/19342, Loss: 0.3407, Acc: 84.23%\n",
      "Batch 18250/19342, Loss: 0.3405, Acc: 84.24%\n",
      "Batch 18300/19342, Loss: 0.3404, Acc: 84.25%\n",
      "Batch 18350/19342, Loss: 0.3403, Acc: 84.26%\n",
      "Batch 18400/19342, Loss: 0.3402, Acc: 84.26%\n",
      "Batch 18450/19342, Loss: 0.3401, Acc: 84.26%\n",
      "Batch 18500/19342, Loss: 0.3400, Acc: 84.27%\n",
      "Batch 18550/19342, Loss: 0.3398, Acc: 84.28%\n",
      "Batch 18600/19342, Loss: 0.3395, Acc: 84.30%\n",
      "Batch 18650/19342, Loss: 0.3393, Acc: 84.31%\n",
      "Batch 18700/19342, Loss: 0.3391, Acc: 84.32%\n",
      "Batch 18750/19342, Loss: 0.3389, Acc: 84.33%\n",
      "Batch 18800/19342, Loss: 0.3389, Acc: 84.33%\n",
      "Batch 18850/19342, Loss: 0.3387, Acc: 84.34%\n",
      "Batch 18900/19342, Loss: 0.3386, Acc: 84.34%\n",
      "Batch 18950/19342, Loss: 0.3385, Acc: 84.35%\n",
      "Batch 19000/19342, Loss: 0.3384, Acc: 84.35%\n",
      "Batch 19050/19342, Loss: 0.3382, Acc: 84.36%\n",
      "Batch 19100/19342, Loss: 0.3380, Acc: 84.38%\n",
      "Batch 19150/19342, Loss: 0.3378, Acc: 84.39%\n",
      "Batch 19200/19342, Loss: 0.3377, Acc: 84.39%\n",
      "Batch 19250/19342, Loss: 0.3375, Acc: 84.40%\n",
      "Batch 19300/19342, Loss: 0.3373, Acc: 84.41%\n",
      "Train Loss: 0.3370, Train Acc: 84.43%\n",
      "Val Loss: 0.3720, Val Acc: 83.53%\n",
      "Saving checkpoint with validation accuracy: 83.53%\n",
      "\n",
      "Epoch 2/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.3152, Acc: 85.00%\n",
      "Batch 100/19342, Loss: 0.2910, Acc: 86.88%\n",
      "Batch 150/19342, Loss: 0.2865, Acc: 87.42%\n",
      "Batch 200/19342, Loss: 0.2887, Acc: 87.19%\n",
      "Batch 250/19342, Loss: 0.2911, Acc: 86.95%\n",
      "Batch 300/19342, Loss: 0.2833, Acc: 87.25%\n",
      "Batch 350/19342, Loss: 0.2820, Acc: 87.36%\n",
      "Batch 400/19342, Loss: 0.2821, Acc: 87.19%\n",
      "Batch 450/19342, Loss: 0.2830, Acc: 86.97%\n",
      "Batch 500/19342, Loss: 0.2776, Acc: 87.38%\n",
      "Batch 550/19342, Loss: 0.2772, Acc: 87.27%\n",
      "Batch 600/19342, Loss: 0.2794, Acc: 87.25%\n",
      "Batch 650/19342, Loss: 0.2789, Acc: 87.27%\n",
      "Batch 700/19342, Loss: 0.2771, Acc: 87.41%\n",
      "Batch 750/19342, Loss: 0.2758, Acc: 87.55%\n",
      "Batch 800/19342, Loss: 0.2730, Acc: 87.66%\n",
      "Batch 850/19342, Loss: 0.2732, Acc: 87.68%\n",
      "Batch 900/19342, Loss: 0.2744, Acc: 87.64%\n",
      "Batch 950/19342, Loss: 0.2749, Acc: 87.62%\n",
      "Batch 1000/19342, Loss: 0.2748, Acc: 87.60%\n",
      "Batch 1050/19342, Loss: 0.2754, Acc: 87.64%\n",
      "Batch 1100/19342, Loss: 0.2756, Acc: 87.58%\n",
      "Batch 1150/19342, Loss: 0.2770, Acc: 87.43%\n",
      "Batch 1200/19342, Loss: 0.2758, Acc: 87.44%\n",
      "Batch 1250/19342, Loss: 0.2735, Acc: 87.59%\n",
      "Batch 1300/19342, Loss: 0.2724, Acc: 87.58%\n",
      "Batch 1350/19342, Loss: 0.2713, Acc: 87.63%\n",
      "Batch 1400/19342, Loss: 0.2696, Acc: 87.71%\n",
      "Batch 1450/19342, Loss: 0.2686, Acc: 87.71%\n",
      "Batch 1500/19342, Loss: 0.2695, Acc: 87.68%\n",
      "Batch 1550/19342, Loss: 0.2707, Acc: 87.60%\n",
      "Batch 1600/19342, Loss: 0.2709, Acc: 87.61%\n",
      "Batch 1650/19342, Loss: 0.2718, Acc: 87.62%\n",
      "Batch 1700/19342, Loss: 0.2714, Acc: 87.66%\n",
      "Batch 1750/19342, Loss: 0.2721, Acc: 87.64%\n",
      "Batch 1800/19342, Loss: 0.2721, Acc: 87.65%\n",
      "Batch 1850/19342, Loss: 0.2716, Acc: 87.62%\n",
      "Batch 1900/19342, Loss: 0.2716, Acc: 87.58%\n",
      "Batch 1950/19342, Loss: 0.2712, Acc: 87.61%\n",
      "Batch 2000/19342, Loss: 0.2703, Acc: 87.66%\n",
      "Batch 2050/19342, Loss: 0.2703, Acc: 87.63%\n",
      "Batch 2100/19342, Loss: 0.2696, Acc: 87.69%\n",
      "Batch 2150/19342, Loss: 0.2702, Acc: 87.67%\n",
      "Batch 2200/19342, Loss: 0.2697, Acc: 87.66%\n",
      "Batch 2250/19342, Loss: 0.2704, Acc: 87.66%\n",
      "Batch 2300/19342, Loss: 0.2710, Acc: 87.61%\n",
      "Batch 2350/19342, Loss: 0.2717, Acc: 87.57%\n",
      "Batch 2400/19342, Loss: 0.2711, Acc: 87.60%\n",
      "Batch 2450/19342, Loss: 0.2715, Acc: 87.61%\n",
      "Batch 2500/19342, Loss: 0.2720, Acc: 87.57%\n",
      "Batch 2550/19342, Loss: 0.2717, Acc: 87.63%\n",
      "Batch 2600/19342, Loss: 0.2724, Acc: 87.63%\n",
      "Batch 2650/19342, Loss: 0.2717, Acc: 87.66%\n",
      "Batch 2700/19342, Loss: 0.2713, Acc: 87.69%\n",
      "Batch 2750/19342, Loss: 0.2715, Acc: 87.69%\n",
      "Batch 2800/19342, Loss: 0.2711, Acc: 87.69%\n",
      "Batch 2850/19342, Loss: 0.2704, Acc: 87.75%\n",
      "Batch 2900/19342, Loss: 0.2696, Acc: 87.78%\n",
      "Batch 2950/19342, Loss: 0.2696, Acc: 87.77%\n",
      "Batch 3000/19342, Loss: 0.2694, Acc: 87.77%\n",
      "Batch 3050/19342, Loss: 0.2693, Acc: 87.79%\n",
      "Batch 3100/19342, Loss: 0.2692, Acc: 87.80%\n",
      "Batch 3150/19342, Loss: 0.2699, Acc: 87.77%\n",
      "Batch 3200/19342, Loss: 0.2696, Acc: 87.78%\n",
      "Batch 3250/19342, Loss: 0.2696, Acc: 87.77%\n",
      "Batch 3300/19342, Loss: 0.2696, Acc: 87.73%\n",
      "Batch 3350/19342, Loss: 0.2690, Acc: 87.72%\n",
      "Batch 3400/19342, Loss: 0.2695, Acc: 87.69%\n",
      "Batch 3450/19342, Loss: 0.2691, Acc: 87.71%\n",
      "Batch 3500/19342, Loss: 0.2686, Acc: 87.74%\n",
      "Batch 3550/19342, Loss: 0.2682, Acc: 87.76%\n",
      "Batch 3600/19342, Loss: 0.2688, Acc: 87.73%\n",
      "Batch 3650/19342, Loss: 0.2688, Acc: 87.75%\n",
      "Batch 3700/19342, Loss: 0.2682, Acc: 87.78%\n",
      "Batch 3750/19342, Loss: 0.2684, Acc: 87.79%\n",
      "Batch 3800/19342, Loss: 0.2684, Acc: 87.78%\n",
      "Batch 3850/19342, Loss: 0.2684, Acc: 87.76%\n",
      "Batch 3900/19342, Loss: 0.2684, Acc: 87.78%\n",
      "Batch 3950/19342, Loss: 0.2684, Acc: 87.75%\n",
      "Batch 4000/19342, Loss: 0.2685, Acc: 87.74%\n",
      "Batch 4050/19342, Loss: 0.2689, Acc: 87.72%\n",
      "Batch 4100/19342, Loss: 0.2689, Acc: 87.69%\n",
      "Batch 4150/19342, Loss: 0.2684, Acc: 87.70%\n",
      "Batch 4200/19342, Loss: 0.2677, Acc: 87.74%\n",
      "Batch 4250/19342, Loss: 0.2676, Acc: 87.74%\n",
      "Batch 4300/19342, Loss: 0.2677, Acc: 87.72%\n",
      "Batch 4350/19342, Loss: 0.2675, Acc: 87.72%\n",
      "Batch 4400/19342, Loss: 0.2672, Acc: 87.73%\n",
      "Batch 4450/19342, Loss: 0.2675, Acc: 87.71%\n",
      "Batch 4500/19342, Loss: 0.2674, Acc: 87.72%\n",
      "Batch 4550/19342, Loss: 0.2673, Acc: 87.74%\n",
      "Batch 4600/19342, Loss: 0.2672, Acc: 87.76%\n",
      "Batch 4650/19342, Loss: 0.2671, Acc: 87.78%\n",
      "Batch 4700/19342, Loss: 0.2670, Acc: 87.78%\n",
      "Batch 4750/19342, Loss: 0.2672, Acc: 87.77%\n",
      "Batch 4800/19342, Loss: 0.2669, Acc: 87.79%\n",
      "Batch 4850/19342, Loss: 0.2670, Acc: 87.78%\n",
      "Batch 4900/19342, Loss: 0.2670, Acc: 87.77%\n",
      "Batch 4950/19342, Loss: 0.2670, Acc: 87.76%\n",
      "Batch 5000/19342, Loss: 0.2672, Acc: 87.75%\n",
      "Batch 5050/19342, Loss: 0.2666, Acc: 87.77%\n",
      "Batch 5100/19342, Loss: 0.2668, Acc: 87.78%\n",
      "Batch 5150/19342, Loss: 0.2664, Acc: 87.80%\n",
      "Batch 5200/19342, Loss: 0.2665, Acc: 87.78%\n",
      "Batch 5250/19342, Loss: 0.2659, Acc: 87.83%\n",
      "Batch 5300/19342, Loss: 0.2659, Acc: 87.83%\n",
      "Batch 5350/19342, Loss: 0.2658, Acc: 87.82%\n",
      "Batch 5400/19342, Loss: 0.2654, Acc: 87.86%\n",
      "Batch 5450/19342, Loss: 0.2654, Acc: 87.86%\n",
      "Batch 5500/19342, Loss: 0.2653, Acc: 87.87%\n",
      "Batch 5550/19342, Loss: 0.2659, Acc: 87.83%\n",
      "Batch 5600/19342, Loss: 0.2660, Acc: 87.83%\n",
      "Batch 5650/19342, Loss: 0.2661, Acc: 87.83%\n",
      "Batch 5700/19342, Loss: 0.2660, Acc: 87.83%\n",
      "Batch 5750/19342, Loss: 0.2662, Acc: 87.80%\n",
      "Batch 5800/19342, Loss: 0.2660, Acc: 87.83%\n",
      "Batch 5850/19342, Loss: 0.2662, Acc: 87.81%\n",
      "Batch 5900/19342, Loss: 0.2658, Acc: 87.83%\n",
      "Batch 5950/19342, Loss: 0.2655, Acc: 87.86%\n",
      "Batch 6000/19342, Loss: 0.2653, Acc: 87.88%\n",
      "Batch 6050/19342, Loss: 0.2653, Acc: 87.89%\n",
      "Batch 6100/19342, Loss: 0.2656, Acc: 87.89%\n",
      "Batch 6150/19342, Loss: 0.2652, Acc: 87.91%\n",
      "Batch 6200/19342, Loss: 0.2651, Acc: 87.91%\n",
      "Batch 6250/19342, Loss: 0.2654, Acc: 87.90%\n",
      "Batch 6300/19342, Loss: 0.2649, Acc: 87.93%\n",
      "Batch 6350/19342, Loss: 0.2650, Acc: 87.93%\n",
      "Batch 6400/19342, Loss: 0.2651, Acc: 87.92%\n",
      "Batch 6450/19342, Loss: 0.2651, Acc: 87.92%\n",
      "Batch 6500/19342, Loss: 0.2652, Acc: 87.91%\n",
      "Batch 6550/19342, Loss: 0.2648, Acc: 87.93%\n",
      "Batch 6600/19342, Loss: 0.2645, Acc: 87.94%\n",
      "Batch 6650/19342, Loss: 0.2647, Acc: 87.91%\n",
      "Batch 6700/19342, Loss: 0.2643, Acc: 87.93%\n",
      "Batch 6750/19342, Loss: 0.2643, Acc: 87.93%\n",
      "Batch 6800/19342, Loss: 0.2646, Acc: 87.91%\n",
      "Batch 6850/19342, Loss: 0.2647, Acc: 87.90%\n",
      "Batch 6900/19342, Loss: 0.2644, Acc: 87.92%\n",
      "Batch 6950/19342, Loss: 0.2643, Acc: 87.93%\n",
      "Batch 7000/19342, Loss: 0.2640, Acc: 87.94%\n",
      "Batch 7050/19342, Loss: 0.2640, Acc: 87.95%\n",
      "Batch 7100/19342, Loss: 0.2640, Acc: 87.94%\n",
      "Batch 7150/19342, Loss: 0.2641, Acc: 87.94%\n",
      "Batch 7200/19342, Loss: 0.2641, Acc: 87.94%\n",
      "Batch 7250/19342, Loss: 0.2639, Acc: 87.96%\n",
      "Batch 7300/19342, Loss: 0.2641, Acc: 87.95%\n",
      "Batch 7350/19342, Loss: 0.2642, Acc: 87.93%\n",
      "Batch 7400/19342, Loss: 0.2642, Acc: 87.94%\n",
      "Batch 7450/19342, Loss: 0.2640, Acc: 87.96%\n",
      "Batch 7500/19342, Loss: 0.2638, Acc: 87.98%\n",
      "Batch 7550/19342, Loss: 0.2636, Acc: 87.99%\n",
      "Batch 7600/19342, Loss: 0.2634, Acc: 88.00%\n",
      "Batch 7650/19342, Loss: 0.2635, Acc: 88.00%\n",
      "Batch 7700/19342, Loss: 0.2633, Acc: 88.01%\n",
      "Batch 7750/19342, Loss: 0.2630, Acc: 88.02%\n",
      "Batch 7800/19342, Loss: 0.2628, Acc: 88.03%\n",
      "Batch 7850/19342, Loss: 0.2632, Acc: 88.01%\n",
      "Batch 7900/19342, Loss: 0.2631, Acc: 88.02%\n",
      "Batch 7950/19342, Loss: 0.2629, Acc: 88.04%\n",
      "Batch 8000/19342, Loss: 0.2628, Acc: 88.04%\n",
      "Batch 8050/19342, Loss: 0.2627, Acc: 88.05%\n",
      "Batch 8100/19342, Loss: 0.2625, Acc: 88.06%\n",
      "Batch 8150/19342, Loss: 0.2623, Acc: 88.07%\n",
      "Batch 8200/19342, Loss: 0.2624, Acc: 88.07%\n",
      "Batch 8250/19342, Loss: 0.2625, Acc: 88.07%\n",
      "Batch 8300/19342, Loss: 0.2625, Acc: 88.06%\n",
      "Batch 8350/19342, Loss: 0.2626, Acc: 88.05%\n",
      "Batch 8400/19342, Loss: 0.2627, Acc: 88.05%\n",
      "Batch 8450/19342, Loss: 0.2628, Acc: 88.03%\n",
      "Batch 8500/19342, Loss: 0.2625, Acc: 88.04%\n",
      "Batch 8550/19342, Loss: 0.2624, Acc: 88.05%\n",
      "Batch 8600/19342, Loss: 0.2625, Acc: 88.04%\n",
      "Batch 8650/19342, Loss: 0.2625, Acc: 88.05%\n",
      "Batch 8700/19342, Loss: 0.2624, Acc: 88.05%\n",
      "Batch 8750/19342, Loss: 0.2622, Acc: 88.06%\n",
      "Batch 8800/19342, Loss: 0.2617, Acc: 88.08%\n",
      "Batch 8850/19342, Loss: 0.2616, Acc: 88.08%\n",
      "Batch 8900/19342, Loss: 0.2614, Acc: 88.09%\n",
      "Batch 8950/19342, Loss: 0.2618, Acc: 88.08%\n",
      "Batch 9000/19342, Loss: 0.2616, Acc: 88.09%\n",
      "Batch 9050/19342, Loss: 0.2616, Acc: 88.10%\n",
      "Batch 9100/19342, Loss: 0.2614, Acc: 88.11%\n",
      "Batch 9150/19342, Loss: 0.2612, Acc: 88.11%\n",
      "Batch 9200/19342, Loss: 0.2611, Acc: 88.12%\n",
      "Batch 9250/19342, Loss: 0.2609, Acc: 88.14%\n",
      "Batch 9300/19342, Loss: 0.2609, Acc: 88.14%\n",
      "Batch 9350/19342, Loss: 0.2610, Acc: 88.13%\n",
      "Batch 9400/19342, Loss: 0.2609, Acc: 88.14%\n",
      "Batch 9450/19342, Loss: 0.2606, Acc: 88.15%\n",
      "Batch 9500/19342, Loss: 0.2608, Acc: 88.14%\n",
      "Batch 9550/19342, Loss: 0.2606, Acc: 88.14%\n",
      "Batch 9600/19342, Loss: 0.2606, Acc: 88.14%\n",
      "Batch 9650/19342, Loss: 0.2605, Acc: 88.14%\n",
      "Batch 9700/19342, Loss: 0.2606, Acc: 88.14%\n",
      "Batch 9750/19342, Loss: 0.2605, Acc: 88.13%\n",
      "Batch 9800/19342, Loss: 0.2602, Acc: 88.15%\n",
      "Batch 9850/19342, Loss: 0.2601, Acc: 88.16%\n",
      "Batch 9900/19342, Loss: 0.2601, Acc: 88.15%\n",
      "Batch 9950/19342, Loss: 0.2601, Acc: 88.14%\n",
      "Batch 10000/19342, Loss: 0.2603, Acc: 88.13%\n",
      "Batch 10050/19342, Loss: 0.2601, Acc: 88.15%\n",
      "Batch 10100/19342, Loss: 0.2598, Acc: 88.16%\n",
      "Batch 10150/19342, Loss: 0.2600, Acc: 88.15%\n",
      "Batch 10200/19342, Loss: 0.2599, Acc: 88.16%\n",
      "Batch 10250/19342, Loss: 0.2599, Acc: 88.16%\n",
      "Batch 10300/19342, Loss: 0.2599, Acc: 88.16%\n",
      "Batch 10350/19342, Loss: 0.2601, Acc: 88.15%\n",
      "Batch 10400/19342, Loss: 0.2600, Acc: 88.16%\n",
      "Batch 10450/19342, Loss: 0.2598, Acc: 88.17%\n",
      "Batch 10500/19342, Loss: 0.2600, Acc: 88.17%\n",
      "Batch 10550/19342, Loss: 0.2598, Acc: 88.18%\n",
      "Batch 10600/19342, Loss: 0.2595, Acc: 88.19%\n",
      "Batch 10650/19342, Loss: 0.2594, Acc: 88.19%\n",
      "Batch 10700/19342, Loss: 0.2595, Acc: 88.19%\n",
      "Batch 10750/19342, Loss: 0.2593, Acc: 88.21%\n",
      "Batch 10800/19342, Loss: 0.2591, Acc: 88.22%\n",
      "Batch 10850/19342, Loss: 0.2592, Acc: 88.21%\n",
      "Batch 10900/19342, Loss: 0.2593, Acc: 88.21%\n",
      "Batch 10950/19342, Loss: 0.2595, Acc: 88.21%\n",
      "Batch 11000/19342, Loss: 0.2593, Acc: 88.22%\n",
      "Batch 11050/19342, Loss: 0.2590, Acc: 88.23%\n",
      "Batch 11100/19342, Loss: 0.2590, Acc: 88.24%\n",
      "Batch 11150/19342, Loss: 0.2589, Acc: 88.25%\n",
      "Batch 11200/19342, Loss: 0.2588, Acc: 88.25%\n",
      "Batch 11250/19342, Loss: 0.2587, Acc: 88.26%\n",
      "Batch 11300/19342, Loss: 0.2588, Acc: 88.25%\n",
      "Batch 11350/19342, Loss: 0.2587, Acc: 88.26%\n",
      "Batch 11400/19342, Loss: 0.2587, Acc: 88.26%\n",
      "Batch 11450/19342, Loss: 0.2586, Acc: 88.26%\n",
      "Batch 11500/19342, Loss: 0.2587, Acc: 88.26%\n",
      "Batch 11550/19342, Loss: 0.2589, Acc: 88.25%\n",
      "Batch 11600/19342, Loss: 0.2588, Acc: 88.25%\n",
      "Batch 11650/19342, Loss: 0.2587, Acc: 88.25%\n",
      "Batch 11700/19342, Loss: 0.2586, Acc: 88.26%\n",
      "Batch 11750/19342, Loss: 0.2586, Acc: 88.25%\n",
      "Batch 11800/19342, Loss: 0.2587, Acc: 88.26%\n",
      "Batch 11850/19342, Loss: 0.2585, Acc: 88.27%\n",
      "Batch 11900/19342, Loss: 0.2585, Acc: 88.27%\n",
      "Batch 11950/19342, Loss: 0.2585, Acc: 88.27%\n",
      "Batch 12000/19342, Loss: 0.2585, Acc: 88.27%\n",
      "Batch 12050/19342, Loss: 0.2585, Acc: 88.27%\n",
      "Batch 12100/19342, Loss: 0.2585, Acc: 88.27%\n",
      "Batch 12150/19342, Loss: 0.2585, Acc: 88.26%\n",
      "Batch 12200/19342, Loss: 0.2585, Acc: 88.27%\n",
      "Batch 12250/19342, Loss: 0.2586, Acc: 88.26%\n",
      "Batch 12300/19342, Loss: 0.2584, Acc: 88.26%\n",
      "Batch 12350/19342, Loss: 0.2583, Acc: 88.27%\n",
      "Batch 12400/19342, Loss: 0.2582, Acc: 88.27%\n",
      "Batch 12450/19342, Loss: 0.2582, Acc: 88.28%\n",
      "Batch 12500/19342, Loss: 0.2581, Acc: 88.29%\n",
      "Batch 12550/19342, Loss: 0.2581, Acc: 88.29%\n",
      "Batch 12600/19342, Loss: 0.2582, Acc: 88.28%\n",
      "Batch 12650/19342, Loss: 0.2582, Acc: 88.28%\n",
      "Batch 12700/19342, Loss: 0.2581, Acc: 88.28%\n",
      "Batch 12750/19342, Loss: 0.2581, Acc: 88.28%\n",
      "Batch 12800/19342, Loss: 0.2581, Acc: 88.27%\n",
      "Batch 12850/19342, Loss: 0.2580, Acc: 88.27%\n",
      "Batch 12900/19342, Loss: 0.2579, Acc: 88.28%\n",
      "Batch 12950/19342, Loss: 0.2579, Acc: 88.28%\n",
      "Batch 13000/19342, Loss: 0.2578, Acc: 88.29%\n",
      "Batch 13050/19342, Loss: 0.2578, Acc: 88.28%\n",
      "Batch 13100/19342, Loss: 0.2575, Acc: 88.30%\n",
      "Batch 13150/19342, Loss: 0.2574, Acc: 88.31%\n",
      "Batch 13200/19342, Loss: 0.2572, Acc: 88.32%\n",
      "Batch 13250/19342, Loss: 0.2571, Acc: 88.32%\n",
      "Batch 13300/19342, Loss: 0.2572, Acc: 88.32%\n",
      "Batch 13350/19342, Loss: 0.2572, Acc: 88.32%\n",
      "Batch 13400/19342, Loss: 0.2573, Acc: 88.32%\n",
      "Batch 13450/19342, Loss: 0.2573, Acc: 88.31%\n",
      "Batch 13500/19342, Loss: 0.2572, Acc: 88.32%\n",
      "Batch 13550/19342, Loss: 0.2572, Acc: 88.31%\n",
      "Batch 13600/19342, Loss: 0.2572, Acc: 88.31%\n",
      "Batch 13650/19342, Loss: 0.2569, Acc: 88.32%\n",
      "Batch 13700/19342, Loss: 0.2569, Acc: 88.33%\n",
      "Batch 13750/19342, Loss: 0.2570, Acc: 88.33%\n",
      "Batch 13800/19342, Loss: 0.2568, Acc: 88.34%\n",
      "Batch 13850/19342, Loss: 0.2568, Acc: 88.33%\n",
      "Batch 13900/19342, Loss: 0.2567, Acc: 88.34%\n",
      "Batch 13950/19342, Loss: 0.2566, Acc: 88.34%\n",
      "Batch 14000/19342, Loss: 0.2566, Acc: 88.35%\n",
      "Batch 14050/19342, Loss: 0.2565, Acc: 88.35%\n",
      "Batch 14100/19342, Loss: 0.2568, Acc: 88.34%\n",
      "Batch 14150/19342, Loss: 0.2567, Acc: 88.34%\n",
      "Batch 14200/19342, Loss: 0.2567, Acc: 88.34%\n",
      "Batch 14250/19342, Loss: 0.2569, Acc: 88.33%\n",
      "Batch 14300/19342, Loss: 0.2570, Acc: 88.32%\n",
      "Batch 14350/19342, Loss: 0.2568, Acc: 88.33%\n",
      "Batch 14400/19342, Loss: 0.2567, Acc: 88.34%\n",
      "Batch 14450/19342, Loss: 0.2565, Acc: 88.34%\n",
      "Batch 14500/19342, Loss: 0.2564, Acc: 88.35%\n",
      "Batch 14550/19342, Loss: 0.2564, Acc: 88.35%\n",
      "Batch 14600/19342, Loss: 0.2565, Acc: 88.34%\n",
      "Batch 14650/19342, Loss: 0.2564, Acc: 88.35%\n",
      "Batch 14700/19342, Loss: 0.2564, Acc: 88.35%\n",
      "Batch 14750/19342, Loss: 0.2563, Acc: 88.35%\n",
      "Batch 14800/19342, Loss: 0.2563, Acc: 88.36%\n",
      "Batch 14850/19342, Loss: 0.2563, Acc: 88.36%\n",
      "Batch 14900/19342, Loss: 0.2563, Acc: 88.35%\n",
      "Batch 14950/19342, Loss: 0.2563, Acc: 88.35%\n",
      "Batch 15000/19342, Loss: 0.2563, Acc: 88.35%\n",
      "Batch 15050/19342, Loss: 0.2563, Acc: 88.35%\n",
      "Batch 15100/19342, Loss: 0.2562, Acc: 88.36%\n",
      "Batch 15150/19342, Loss: 0.2561, Acc: 88.37%\n",
      "Batch 15200/19342, Loss: 0.2560, Acc: 88.38%\n",
      "Batch 15250/19342, Loss: 0.2561, Acc: 88.37%\n",
      "Batch 15300/19342, Loss: 0.2560, Acc: 88.38%\n",
      "Batch 15350/19342, Loss: 0.2558, Acc: 88.38%\n",
      "Batch 15400/19342, Loss: 0.2558, Acc: 88.37%\n",
      "Batch 15450/19342, Loss: 0.2558, Acc: 88.38%\n",
      "Batch 15500/19342, Loss: 0.2559, Acc: 88.38%\n",
      "Batch 15550/19342, Loss: 0.2556, Acc: 88.39%\n",
      "Batch 15600/19342, Loss: 0.2555, Acc: 88.40%\n",
      "Batch 15650/19342, Loss: 0.2554, Acc: 88.40%\n",
      "Batch 15700/19342, Loss: 0.2554, Acc: 88.40%\n",
      "Batch 15750/19342, Loss: 0.2554, Acc: 88.40%\n",
      "Batch 15800/19342, Loss: 0.2553, Acc: 88.40%\n",
      "Batch 15850/19342, Loss: 0.2553, Acc: 88.40%\n",
      "Batch 15900/19342, Loss: 0.2553, Acc: 88.40%\n",
      "Batch 15950/19342, Loss: 0.2552, Acc: 88.41%\n",
      "Batch 16000/19342, Loss: 0.2551, Acc: 88.41%\n",
      "Batch 16050/19342, Loss: 0.2550, Acc: 88.42%\n",
      "Batch 16100/19342, Loss: 0.2550, Acc: 88.42%\n",
      "Batch 16150/19342, Loss: 0.2549, Acc: 88.43%\n",
      "Batch 16200/19342, Loss: 0.2548, Acc: 88.44%\n",
      "Batch 16250/19342, Loss: 0.2550, Acc: 88.42%\n",
      "Batch 16300/19342, Loss: 0.2550, Acc: 88.42%\n",
      "Batch 16350/19342, Loss: 0.2550, Acc: 88.43%\n",
      "Batch 16400/19342, Loss: 0.2549, Acc: 88.42%\n",
      "Batch 16450/19342, Loss: 0.2549, Acc: 88.43%\n",
      "Batch 16500/19342, Loss: 0.2548, Acc: 88.43%\n",
      "Batch 16550/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 16600/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 16650/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 16700/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 16750/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 16800/19342, Loss: 0.2549, Acc: 88.42%\n",
      "Batch 16850/19342, Loss: 0.2549, Acc: 88.42%\n",
      "Batch 16900/19342, Loss: 0.2549, Acc: 88.42%\n",
      "Batch 16950/19342, Loss: 0.2549, Acc: 88.43%\n",
      "Batch 17000/19342, Loss: 0.2548, Acc: 88.43%\n",
      "Batch 17050/19342, Loss: 0.2548, Acc: 88.43%\n",
      "Batch 17100/19342, Loss: 0.2550, Acc: 88.42%\n",
      "Batch 17150/19342, Loss: 0.2550, Acc: 88.42%\n",
      "Batch 17200/19342, Loss: 0.2551, Acc: 88.42%\n",
      "Batch 17250/19342, Loss: 0.2550, Acc: 88.42%\n",
      "Batch 17300/19342, Loss: 0.2549, Acc: 88.43%\n",
      "Batch 17350/19342, Loss: 0.2548, Acc: 88.42%\n",
      "Batch 17400/19342, Loss: 0.2549, Acc: 88.42%\n",
      "Batch 17450/19342, Loss: 0.2549, Acc: 88.42%\n",
      "Batch 17500/19342, Loss: 0.2548, Acc: 88.43%\n",
      "Batch 17550/19342, Loss: 0.2548, Acc: 88.42%\n",
      "Batch 17600/19342, Loss: 0.2549, Acc: 88.42%\n",
      "Batch 17650/19342, Loss: 0.2548, Acc: 88.42%\n",
      "Batch 17700/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 17750/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 17800/19342, Loss: 0.2546, Acc: 88.43%\n",
      "Batch 17850/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 17900/19342, Loss: 0.2546, Acc: 88.43%\n",
      "Batch 17950/19342, Loss: 0.2546, Acc: 88.44%\n",
      "Batch 18000/19342, Loss: 0.2546, Acc: 88.44%\n",
      "Batch 18050/19342, Loss: 0.2547, Acc: 88.43%\n",
      "Batch 18100/19342, Loss: 0.2546, Acc: 88.44%\n",
      "Batch 18150/19342, Loss: 0.2546, Acc: 88.44%\n",
      "Batch 18200/19342, Loss: 0.2546, Acc: 88.44%\n",
      "Batch 18250/19342, Loss: 0.2544, Acc: 88.44%\n",
      "Batch 18300/19342, Loss: 0.2545, Acc: 88.44%\n",
      "Batch 18350/19342, Loss: 0.2544, Acc: 88.45%\n",
      "Batch 18400/19342, Loss: 0.2543, Acc: 88.46%\n",
      "Batch 18450/19342, Loss: 0.2541, Acc: 88.46%\n",
      "Batch 18500/19342, Loss: 0.2539, Acc: 88.47%\n",
      "Batch 18550/19342, Loss: 0.2540, Acc: 88.46%\n",
      "Batch 18600/19342, Loss: 0.2539, Acc: 88.47%\n",
      "Batch 18650/19342, Loss: 0.2539, Acc: 88.47%\n",
      "Batch 18700/19342, Loss: 0.2538, Acc: 88.47%\n",
      "Batch 18750/19342, Loss: 0.2538, Acc: 88.47%\n",
      "Batch 18800/19342, Loss: 0.2538, Acc: 88.47%\n",
      "Batch 18850/19342, Loss: 0.2538, Acc: 88.47%\n",
      "Batch 18900/19342, Loss: 0.2536, Acc: 88.48%\n",
      "Batch 18950/19342, Loss: 0.2536, Acc: 88.48%\n",
      "Batch 19000/19342, Loss: 0.2536, Acc: 88.48%\n",
      "Batch 19050/19342, Loss: 0.2536, Acc: 88.48%\n",
      "Batch 19100/19342, Loss: 0.2536, Acc: 88.48%\n",
      "Batch 19150/19342, Loss: 0.2537, Acc: 88.48%\n",
      "Batch 19200/19342, Loss: 0.2537, Acc: 88.48%\n",
      "Batch 19250/19342, Loss: 0.2537, Acc: 88.47%\n",
      "Batch 19300/19342, Loss: 0.2537, Acc: 88.48%\n",
      "Train Loss: 0.2536, Train Acc: 88.48%\n",
      "Val Loss: 0.3631, Val Acc: 83.79%\n",
      "Saving checkpoint with validation accuracy: 83.79%\n",
      "\n",
      "Epoch 3/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.2188, Acc: 91.00%\n",
      "Batch 100/19342, Loss: 0.2432, Acc: 89.25%\n",
      "Batch 150/19342, Loss: 0.2355, Acc: 89.67%\n",
      "Batch 200/19342, Loss: 0.2457, Acc: 89.12%\n",
      "Batch 250/19342, Loss: 0.2455, Acc: 89.20%\n",
      "Batch 300/19342, Loss: 0.2405, Acc: 89.58%\n",
      "Batch 350/19342, Loss: 0.2402, Acc: 89.39%\n",
      "Batch 400/19342, Loss: 0.2349, Acc: 89.47%\n",
      "Batch 450/19342, Loss: 0.2303, Acc: 89.67%\n",
      "Batch 500/19342, Loss: 0.2299, Acc: 89.72%\n",
      "Batch 550/19342, Loss: 0.2305, Acc: 89.50%\n",
      "Batch 600/19342, Loss: 0.2298, Acc: 89.44%\n",
      "Batch 650/19342, Loss: 0.2315, Acc: 89.44%\n",
      "Batch 700/19342, Loss: 0.2352, Acc: 89.29%\n",
      "Batch 750/19342, Loss: 0.2345, Acc: 89.22%\n",
      "Batch 800/19342, Loss: 0.2335, Acc: 89.14%\n",
      "Batch 850/19342, Loss: 0.2319, Acc: 89.19%\n",
      "Batch 900/19342, Loss: 0.2318, Acc: 89.19%\n",
      "Batch 950/19342, Loss: 0.2300, Acc: 89.34%\n",
      "Batch 1000/19342, Loss: 0.2274, Acc: 89.45%\n",
      "Batch 1050/19342, Loss: 0.2272, Acc: 89.45%\n",
      "Batch 1100/19342, Loss: 0.2272, Acc: 89.44%\n",
      "Batch 1150/19342, Loss: 0.2280, Acc: 89.41%\n",
      "Batch 1200/19342, Loss: 0.2280, Acc: 89.34%\n",
      "Batch 1250/19342, Loss: 0.2288, Acc: 89.37%\n",
      "Batch 1300/19342, Loss: 0.2281, Acc: 89.43%\n",
      "Batch 1350/19342, Loss: 0.2285, Acc: 89.47%\n",
      "Batch 1400/19342, Loss: 0.2277, Acc: 89.49%\n",
      "Batch 1450/19342, Loss: 0.2284, Acc: 89.49%\n",
      "Batch 1500/19342, Loss: 0.2278, Acc: 89.57%\n",
      "Batch 1550/19342, Loss: 0.2289, Acc: 89.47%\n",
      "Batch 1600/19342, Loss: 0.2287, Acc: 89.46%\n",
      "Batch 1650/19342, Loss: 0.2283, Acc: 89.52%\n",
      "Batch 1700/19342, Loss: 0.2286, Acc: 89.51%\n",
      "Batch 1750/19342, Loss: 0.2281, Acc: 89.54%\n",
      "Batch 1800/19342, Loss: 0.2283, Acc: 89.55%\n",
      "Batch 1850/19342, Loss: 0.2277, Acc: 89.58%\n",
      "Batch 1900/19342, Loss: 0.2284, Acc: 89.54%\n",
      "Batch 1950/19342, Loss: 0.2296, Acc: 89.51%\n",
      "Batch 2000/19342, Loss: 0.2297, Acc: 89.53%\n",
      "Batch 2050/19342, Loss: 0.2304, Acc: 89.43%\n",
      "Batch 2100/19342, Loss: 0.2297, Acc: 89.48%\n",
      "Batch 2150/19342, Loss: 0.2312, Acc: 89.40%\n",
      "Batch 2200/19342, Loss: 0.2313, Acc: 89.40%\n",
      "Batch 2250/19342, Loss: 0.2311, Acc: 89.39%\n",
      "Batch 2300/19342, Loss: 0.2313, Acc: 89.38%\n",
      "Batch 2350/19342, Loss: 0.2317, Acc: 89.32%\n",
      "Batch 2400/19342, Loss: 0.2324, Acc: 89.30%\n",
      "Batch 2450/19342, Loss: 0.2328, Acc: 89.27%\n",
      "Batch 2500/19342, Loss: 0.2330, Acc: 89.23%\n",
      "Batch 2550/19342, Loss: 0.2333, Acc: 89.25%\n",
      "Batch 2600/19342, Loss: 0.2337, Acc: 89.23%\n",
      "Batch 2650/19342, Loss: 0.2342, Acc: 89.20%\n",
      "Batch 2700/19342, Loss: 0.2337, Acc: 89.26%\n",
      "Batch 2750/19342, Loss: 0.2328, Acc: 89.30%\n",
      "Batch 2800/19342, Loss: 0.2324, Acc: 89.31%\n",
      "Batch 2850/19342, Loss: 0.2328, Acc: 89.29%\n",
      "Batch 2900/19342, Loss: 0.2327, Acc: 89.31%\n",
      "Batch 2950/19342, Loss: 0.2326, Acc: 89.32%\n",
      "Batch 3000/19342, Loss: 0.2324, Acc: 89.34%\n",
      "Batch 3050/19342, Loss: 0.2324, Acc: 89.35%\n",
      "Batch 3100/19342, Loss: 0.2317, Acc: 89.41%\n",
      "Batch 3150/19342, Loss: 0.2319, Acc: 89.38%\n",
      "Batch 3200/19342, Loss: 0.2321, Acc: 89.36%\n",
      "Batch 3250/19342, Loss: 0.2316, Acc: 89.38%\n",
      "Batch 3300/19342, Loss: 0.2316, Acc: 89.38%\n",
      "Batch 3350/19342, Loss: 0.2311, Acc: 89.40%\n",
      "Batch 3400/19342, Loss: 0.2317, Acc: 89.39%\n",
      "Batch 3450/19342, Loss: 0.2321, Acc: 89.37%\n",
      "Batch 3500/19342, Loss: 0.2319, Acc: 89.38%\n",
      "Batch 3550/19342, Loss: 0.2322, Acc: 89.39%\n",
      "Batch 3600/19342, Loss: 0.2319, Acc: 89.39%\n",
      "Batch 3650/19342, Loss: 0.2319, Acc: 89.37%\n",
      "Batch 3700/19342, Loss: 0.2317, Acc: 89.39%\n",
      "Batch 3750/19342, Loss: 0.2317, Acc: 89.40%\n",
      "Batch 3800/19342, Loss: 0.2312, Acc: 89.45%\n",
      "Batch 3850/19342, Loss: 0.2314, Acc: 89.44%\n",
      "Batch 3900/19342, Loss: 0.2315, Acc: 89.45%\n",
      "Batch 3950/19342, Loss: 0.2315, Acc: 89.47%\n",
      "Batch 4000/19342, Loss: 0.2311, Acc: 89.49%\n",
      "Batch 4050/19342, Loss: 0.2313, Acc: 89.49%\n",
      "Batch 4100/19342, Loss: 0.2314, Acc: 89.49%\n",
      "Batch 4150/19342, Loss: 0.2310, Acc: 89.52%\n",
      "Batch 4200/19342, Loss: 0.2310, Acc: 89.52%\n",
      "Batch 4250/19342, Loss: 0.2316, Acc: 89.47%\n",
      "Batch 4300/19342, Loss: 0.2314, Acc: 89.49%\n",
      "Batch 4350/19342, Loss: 0.2315, Acc: 89.49%\n",
      "Batch 4400/19342, Loss: 0.2312, Acc: 89.51%\n",
      "Batch 4450/19342, Loss: 0.2314, Acc: 89.50%\n",
      "Batch 4500/19342, Loss: 0.2313, Acc: 89.49%\n",
      "Batch 4550/19342, Loss: 0.2316, Acc: 89.47%\n",
      "Batch 4600/19342, Loss: 0.2312, Acc: 89.50%\n",
      "Batch 4650/19342, Loss: 0.2309, Acc: 89.52%\n",
      "Batch 4700/19342, Loss: 0.2309, Acc: 89.51%\n",
      "Batch 4750/19342, Loss: 0.2309, Acc: 89.50%\n",
      "Batch 4800/19342, Loss: 0.2307, Acc: 89.51%\n",
      "Batch 4850/19342, Loss: 0.2309, Acc: 89.49%\n",
      "Batch 4900/19342, Loss: 0.2307, Acc: 89.48%\n",
      "Batch 4950/19342, Loss: 0.2306, Acc: 89.50%\n",
      "Batch 5000/19342, Loss: 0.2306, Acc: 89.51%\n",
      "Batch 5050/19342, Loss: 0.2305, Acc: 89.52%\n",
      "Batch 5100/19342, Loss: 0.2304, Acc: 89.54%\n",
      "Batch 5150/19342, Loss: 0.2306, Acc: 89.53%\n",
      "Batch 5200/19342, Loss: 0.2307, Acc: 89.53%\n",
      "Batch 5250/19342, Loss: 0.2305, Acc: 89.54%\n",
      "Batch 5300/19342, Loss: 0.2305, Acc: 89.53%\n",
      "Batch 5350/19342, Loss: 0.2305, Acc: 89.51%\n",
      "Batch 5400/19342, Loss: 0.2305, Acc: 89.52%\n",
      "Batch 5450/19342, Loss: 0.2302, Acc: 89.54%\n",
      "Batch 5500/19342, Loss: 0.2302, Acc: 89.54%\n",
      "Batch 5550/19342, Loss: 0.2301, Acc: 89.53%\n",
      "Batch 5600/19342, Loss: 0.2304, Acc: 89.52%\n",
      "Batch 5650/19342, Loss: 0.2306, Acc: 89.52%\n",
      "Batch 5700/19342, Loss: 0.2308, Acc: 89.51%\n",
      "Batch 5750/19342, Loss: 0.2307, Acc: 89.51%\n",
      "Batch 5800/19342, Loss: 0.2304, Acc: 89.51%\n",
      "Batch 5850/19342, Loss: 0.2305, Acc: 89.50%\n",
      "Batch 5900/19342, Loss: 0.2304, Acc: 89.50%\n",
      "Batch 5950/19342, Loss: 0.2305, Acc: 89.50%\n",
      "Batch 6000/19342, Loss: 0.2307, Acc: 89.49%\n",
      "Batch 6050/19342, Loss: 0.2307, Acc: 89.50%\n",
      "Batch 6100/19342, Loss: 0.2308, Acc: 89.50%\n",
      "Batch 6150/19342, Loss: 0.2305, Acc: 89.52%\n",
      "Batch 6200/19342, Loss: 0.2308, Acc: 89.51%\n",
      "Batch 6250/19342, Loss: 0.2308, Acc: 89.51%\n",
      "Batch 6300/19342, Loss: 0.2310, Acc: 89.50%\n",
      "Batch 6350/19342, Loss: 0.2311, Acc: 89.49%\n",
      "Batch 6400/19342, Loss: 0.2311, Acc: 89.48%\n",
      "Batch 6450/19342, Loss: 0.2312, Acc: 89.47%\n",
      "Batch 6500/19342, Loss: 0.2311, Acc: 89.47%\n",
      "Batch 6550/19342, Loss: 0.2311, Acc: 89.47%\n",
      "Batch 6600/19342, Loss: 0.2311, Acc: 89.47%\n",
      "Batch 6650/19342, Loss: 0.2312, Acc: 89.48%\n",
      "Batch 6700/19342, Loss: 0.2313, Acc: 89.48%\n",
      "Batch 6750/19342, Loss: 0.2312, Acc: 89.48%\n",
      "Batch 6800/19342, Loss: 0.2312, Acc: 89.47%\n",
      "Batch 6850/19342, Loss: 0.2312, Acc: 89.47%\n",
      "Batch 6900/19342, Loss: 0.2311, Acc: 89.47%\n",
      "Batch 6950/19342, Loss: 0.2313, Acc: 89.46%\n",
      "Batch 7000/19342, Loss: 0.2314, Acc: 89.45%\n",
      "Batch 7050/19342, Loss: 0.2311, Acc: 89.46%\n",
      "Batch 7100/19342, Loss: 0.2312, Acc: 89.45%\n",
      "Batch 7150/19342, Loss: 0.2312, Acc: 89.45%\n",
      "Batch 7200/19342, Loss: 0.2311, Acc: 89.45%\n",
      "Batch 7250/19342, Loss: 0.2313, Acc: 89.44%\n",
      "Batch 7300/19342, Loss: 0.2310, Acc: 89.46%\n",
      "Batch 7350/19342, Loss: 0.2314, Acc: 89.44%\n",
      "Batch 7400/19342, Loss: 0.2310, Acc: 89.46%\n",
      "Batch 7450/19342, Loss: 0.2309, Acc: 89.46%\n",
      "Batch 7500/19342, Loss: 0.2308, Acc: 89.47%\n",
      "Batch 7550/19342, Loss: 0.2306, Acc: 89.48%\n",
      "Batch 7600/19342, Loss: 0.2306, Acc: 89.47%\n",
      "Batch 7650/19342, Loss: 0.2306, Acc: 89.46%\n",
      "Batch 7700/19342, Loss: 0.2306, Acc: 89.46%\n",
      "Batch 7750/19342, Loss: 0.2308, Acc: 89.45%\n",
      "Batch 7800/19342, Loss: 0.2308, Acc: 89.46%\n",
      "Batch 7850/19342, Loss: 0.2305, Acc: 89.47%\n",
      "Batch 7900/19342, Loss: 0.2304, Acc: 89.47%\n",
      "Batch 7950/19342, Loss: 0.2306, Acc: 89.46%\n",
      "Batch 8000/19342, Loss: 0.2304, Acc: 89.47%\n",
      "Batch 8050/19342, Loss: 0.2304, Acc: 89.48%\n",
      "Batch 8100/19342, Loss: 0.2302, Acc: 89.49%\n",
      "Batch 8150/19342, Loss: 0.2299, Acc: 89.51%\n",
      "Batch 8200/19342, Loss: 0.2298, Acc: 89.51%\n",
      "Batch 8250/19342, Loss: 0.2296, Acc: 89.52%\n",
      "Batch 8300/19342, Loss: 0.2293, Acc: 89.53%\n",
      "Batch 8350/19342, Loss: 0.2292, Acc: 89.54%\n",
      "Batch 8400/19342, Loss: 0.2294, Acc: 89.52%\n",
      "Batch 8450/19342, Loss: 0.2293, Acc: 89.51%\n",
      "Batch 8500/19342, Loss: 0.2295, Acc: 89.50%\n",
      "Batch 8550/19342, Loss: 0.2295, Acc: 89.49%\n",
      "Batch 8600/19342, Loss: 0.2295, Acc: 89.49%\n",
      "Batch 8650/19342, Loss: 0.2295, Acc: 89.49%\n",
      "Batch 8700/19342, Loss: 0.2296, Acc: 89.48%\n",
      "Batch 8750/19342, Loss: 0.2297, Acc: 89.47%\n",
      "Batch 8800/19342, Loss: 0.2297, Acc: 89.47%\n",
      "Batch 8850/19342, Loss: 0.2296, Acc: 89.47%\n",
      "Batch 8900/19342, Loss: 0.2296, Acc: 89.46%\n",
      "Batch 8950/19342, Loss: 0.2297, Acc: 89.46%\n",
      "Batch 9000/19342, Loss: 0.2299, Acc: 89.44%\n",
      "Batch 9050/19342, Loss: 0.2299, Acc: 89.44%\n",
      "Batch 9100/19342, Loss: 0.2299, Acc: 89.45%\n",
      "Batch 9150/19342, Loss: 0.2300, Acc: 89.45%\n",
      "Batch 9200/19342, Loss: 0.2299, Acc: 89.46%\n",
      "Batch 9250/19342, Loss: 0.2300, Acc: 89.46%\n",
      "Batch 9300/19342, Loss: 0.2299, Acc: 89.47%\n",
      "Batch 9350/19342, Loss: 0.2299, Acc: 89.47%\n",
      "Batch 9400/19342, Loss: 0.2300, Acc: 89.47%\n",
      "Batch 9450/19342, Loss: 0.2300, Acc: 89.46%\n",
      "Batch 9500/19342, Loss: 0.2301, Acc: 89.46%\n",
      "Batch 9550/19342, Loss: 0.2299, Acc: 89.48%\n",
      "Batch 9600/19342, Loss: 0.2298, Acc: 89.48%\n",
      "Batch 9650/19342, Loss: 0.2297, Acc: 89.49%\n",
      "Batch 9700/19342, Loss: 0.2296, Acc: 89.49%\n",
      "Batch 9750/19342, Loss: 0.2294, Acc: 89.50%\n",
      "Batch 9800/19342, Loss: 0.2294, Acc: 89.50%\n",
      "Batch 9850/19342, Loss: 0.2296, Acc: 89.49%\n",
      "Batch 9900/19342, Loss: 0.2295, Acc: 89.50%\n",
      "Batch 9950/19342, Loss: 0.2296, Acc: 89.49%\n",
      "Batch 10000/19342, Loss: 0.2295, Acc: 89.51%\n",
      "Batch 10050/19342, Loss: 0.2294, Acc: 89.50%\n",
      "Batch 10100/19342, Loss: 0.2294, Acc: 89.50%\n",
      "Batch 10150/19342, Loss: 0.2292, Acc: 89.52%\n",
      "Batch 10200/19342, Loss: 0.2294, Acc: 89.51%\n",
      "Batch 10250/19342, Loss: 0.2295, Acc: 89.51%\n",
      "Batch 10300/19342, Loss: 0.2295, Acc: 89.51%\n",
      "Batch 10350/19342, Loss: 0.2297, Acc: 89.50%\n",
      "Batch 10400/19342, Loss: 0.2296, Acc: 89.51%\n",
      "Batch 10450/19342, Loss: 0.2296, Acc: 89.51%\n",
      "Batch 10500/19342, Loss: 0.2296, Acc: 89.52%\n",
      "Batch 10550/19342, Loss: 0.2293, Acc: 89.53%\n",
      "Batch 10600/19342, Loss: 0.2294, Acc: 89.54%\n",
      "Batch 10650/19342, Loss: 0.2295, Acc: 89.53%\n",
      "Batch 10700/19342, Loss: 0.2292, Acc: 89.54%\n",
      "Batch 10750/19342, Loss: 0.2291, Acc: 89.55%\n",
      "Batch 10800/19342, Loss: 0.2292, Acc: 89.55%\n",
      "Batch 10850/19342, Loss: 0.2291, Acc: 89.55%\n",
      "Batch 10900/19342, Loss: 0.2289, Acc: 89.55%\n",
      "Batch 10950/19342, Loss: 0.2290, Acc: 89.54%\n",
      "Batch 11000/19342, Loss: 0.2289, Acc: 89.54%\n",
      "Batch 11050/19342, Loss: 0.2290, Acc: 89.54%\n",
      "Batch 11100/19342, Loss: 0.2290, Acc: 89.55%\n",
      "Batch 11150/19342, Loss: 0.2289, Acc: 89.54%\n",
      "Batch 11200/19342, Loss: 0.2287, Acc: 89.55%\n",
      "Batch 11250/19342, Loss: 0.2287, Acc: 89.54%\n",
      "Batch 11300/19342, Loss: 0.2285, Acc: 89.56%\n",
      "Batch 11350/19342, Loss: 0.2285, Acc: 89.56%\n",
      "Batch 11400/19342, Loss: 0.2284, Acc: 89.56%\n",
      "Batch 11450/19342, Loss: 0.2284, Acc: 89.56%\n",
      "Batch 11500/19342, Loss: 0.2283, Acc: 89.57%\n",
      "Batch 11550/19342, Loss: 0.2283, Acc: 89.57%\n",
      "Batch 11600/19342, Loss: 0.2282, Acc: 89.59%\n",
      "Batch 11650/19342, Loss: 0.2281, Acc: 89.59%\n",
      "Batch 11700/19342, Loss: 0.2282, Acc: 89.59%\n",
      "Batch 11750/19342, Loss: 0.2281, Acc: 89.60%\n",
      "Batch 11800/19342, Loss: 0.2280, Acc: 89.60%\n",
      "Batch 11850/19342, Loss: 0.2280, Acc: 89.61%\n",
      "Batch 11900/19342, Loss: 0.2279, Acc: 89.62%\n",
      "Batch 11950/19342, Loss: 0.2278, Acc: 89.63%\n",
      "Batch 12000/19342, Loss: 0.2278, Acc: 89.62%\n",
      "Batch 12050/19342, Loss: 0.2276, Acc: 89.63%\n",
      "Batch 12100/19342, Loss: 0.2277, Acc: 89.63%\n",
      "Batch 12150/19342, Loss: 0.2277, Acc: 89.63%\n",
      "Batch 12200/19342, Loss: 0.2275, Acc: 89.63%\n",
      "Batch 12250/19342, Loss: 0.2276, Acc: 89.63%\n",
      "Batch 12300/19342, Loss: 0.2277, Acc: 89.62%\n",
      "Batch 12350/19342, Loss: 0.2277, Acc: 89.62%\n",
      "Batch 12400/19342, Loss: 0.2276, Acc: 89.62%\n",
      "Batch 12450/19342, Loss: 0.2277, Acc: 89.62%\n",
      "Batch 12500/19342, Loss: 0.2277, Acc: 89.63%\n",
      "Batch 12550/19342, Loss: 0.2275, Acc: 89.63%\n",
      "Batch 12600/19342, Loss: 0.2275, Acc: 89.63%\n",
      "Batch 12650/19342, Loss: 0.2275, Acc: 89.64%\n",
      "Batch 12700/19342, Loss: 0.2275, Acc: 89.64%\n",
      "Batch 12750/19342, Loss: 0.2272, Acc: 89.65%\n",
      "Batch 12800/19342, Loss: 0.2270, Acc: 89.66%\n",
      "Batch 12850/19342, Loss: 0.2270, Acc: 89.65%\n",
      "Batch 12900/19342, Loss: 0.2271, Acc: 89.65%\n",
      "Batch 12950/19342, Loss: 0.2270, Acc: 89.65%\n",
      "Batch 13000/19342, Loss: 0.2269, Acc: 89.65%\n",
      "Batch 13050/19342, Loss: 0.2268, Acc: 89.66%\n",
      "Batch 13100/19342, Loss: 0.2268, Acc: 89.66%\n",
      "Batch 13150/19342, Loss: 0.2268, Acc: 89.66%\n",
      "Batch 13200/19342, Loss: 0.2268, Acc: 89.66%\n",
      "Batch 13250/19342, Loss: 0.2268, Acc: 89.67%\n",
      "Batch 13300/19342, Loss: 0.2268, Acc: 89.67%\n",
      "Batch 13350/19342, Loss: 0.2266, Acc: 89.68%\n",
      "Batch 13400/19342, Loss: 0.2266, Acc: 89.68%\n",
      "Batch 13450/19342, Loss: 0.2265, Acc: 89.68%\n",
      "Batch 13500/19342, Loss: 0.2264, Acc: 89.69%\n",
      "Batch 13550/19342, Loss: 0.2265, Acc: 89.68%\n",
      "Batch 13600/19342, Loss: 0.2264, Acc: 89.69%\n",
      "Batch 13650/19342, Loss: 0.2264, Acc: 89.69%\n",
      "Batch 13700/19342, Loss: 0.2265, Acc: 89.69%\n",
      "Batch 13750/19342, Loss: 0.2264, Acc: 89.69%\n",
      "Batch 13800/19342, Loss: 0.2264, Acc: 89.69%\n",
      "Batch 13850/19342, Loss: 0.2264, Acc: 89.70%\n",
      "Batch 13900/19342, Loss: 0.2263, Acc: 89.70%\n",
      "Batch 13950/19342, Loss: 0.2262, Acc: 89.70%\n",
      "Batch 14000/19342, Loss: 0.2263, Acc: 89.70%\n",
      "Batch 14050/19342, Loss: 0.2262, Acc: 89.71%\n",
      "Batch 14100/19342, Loss: 0.2261, Acc: 89.71%\n",
      "Batch 14150/19342, Loss: 0.2261, Acc: 89.71%\n",
      "Batch 14200/19342, Loss: 0.2261, Acc: 89.71%\n",
      "Batch 14250/19342, Loss: 0.2262, Acc: 89.71%\n",
      "Batch 14300/19342, Loss: 0.2260, Acc: 89.72%\n",
      "Batch 14350/19342, Loss: 0.2260, Acc: 89.72%\n",
      "Batch 14400/19342, Loss: 0.2262, Acc: 89.71%\n",
      "Batch 14450/19342, Loss: 0.2261, Acc: 89.72%\n",
      "Batch 14500/19342, Loss: 0.2260, Acc: 89.72%\n",
      "Batch 14550/19342, Loss: 0.2260, Acc: 89.72%\n",
      "Batch 14600/19342, Loss: 0.2261, Acc: 89.71%\n",
      "Batch 14650/19342, Loss: 0.2261, Acc: 89.72%\n",
      "Batch 14700/19342, Loss: 0.2262, Acc: 89.71%\n",
      "Batch 14750/19342, Loss: 0.2263, Acc: 89.70%\n",
      "Batch 14800/19342, Loss: 0.2263, Acc: 89.70%\n",
      "Batch 14850/19342, Loss: 0.2263, Acc: 89.70%\n",
      "Batch 14900/19342, Loss: 0.2263, Acc: 89.70%\n",
      "Batch 14950/19342, Loss: 0.2263, Acc: 89.70%\n",
      "Batch 15000/19342, Loss: 0.2262, Acc: 89.71%\n",
      "Batch 15050/19342, Loss: 0.2261, Acc: 89.71%\n",
      "Batch 15100/19342, Loss: 0.2260, Acc: 89.72%\n",
      "Batch 15150/19342, Loss: 0.2259, Acc: 89.73%\n",
      "Batch 15200/19342, Loss: 0.2258, Acc: 89.73%\n",
      "Batch 15250/19342, Loss: 0.2259, Acc: 89.73%\n",
      "Batch 15300/19342, Loss: 0.2259, Acc: 89.74%\n",
      "Batch 15350/19342, Loss: 0.2259, Acc: 89.73%\n",
      "Batch 15400/19342, Loss: 0.2259, Acc: 89.74%\n",
      "Batch 15450/19342, Loss: 0.2259, Acc: 89.74%\n",
      "Batch 15500/19342, Loss: 0.2261, Acc: 89.73%\n",
      "Batch 15550/19342, Loss: 0.2261, Acc: 89.74%\n",
      "Batch 15600/19342, Loss: 0.2261, Acc: 89.74%\n",
      "Batch 15650/19342, Loss: 0.2261, Acc: 89.74%\n",
      "Batch 15700/19342, Loss: 0.2261, Acc: 89.73%\n",
      "Batch 15750/19342, Loss: 0.2259, Acc: 89.74%\n",
      "Batch 15800/19342, Loss: 0.2259, Acc: 89.74%\n",
      "Batch 15850/19342, Loss: 0.2259, Acc: 89.74%\n",
      "Batch 15900/19342, Loss: 0.2259, Acc: 89.74%\n",
      "Batch 15950/19342, Loss: 0.2258, Acc: 89.75%\n",
      "Batch 16000/19342, Loss: 0.2257, Acc: 89.75%\n",
      "Batch 16050/19342, Loss: 0.2257, Acc: 89.75%\n",
      "Batch 16100/19342, Loss: 0.2257, Acc: 89.75%\n",
      "Batch 16150/19342, Loss: 0.2256, Acc: 89.76%\n",
      "Batch 16200/19342, Loss: 0.2255, Acc: 89.76%\n",
      "Batch 16250/19342, Loss: 0.2254, Acc: 89.77%\n",
      "Batch 16300/19342, Loss: 0.2253, Acc: 89.77%\n",
      "Batch 16350/19342, Loss: 0.2253, Acc: 89.78%\n",
      "Batch 16400/19342, Loss: 0.2253, Acc: 89.78%\n",
      "Batch 16450/19342, Loss: 0.2254, Acc: 89.78%\n",
      "Batch 16500/19342, Loss: 0.2255, Acc: 89.77%\n",
      "Batch 16550/19342, Loss: 0.2253, Acc: 89.78%\n",
      "Batch 16600/19342, Loss: 0.2253, Acc: 89.79%\n",
      "Batch 16650/19342, Loss: 0.2254, Acc: 89.78%\n",
      "Batch 16700/19342, Loss: 0.2253, Acc: 89.79%\n",
      "Batch 16750/19342, Loss: 0.2252, Acc: 89.79%\n",
      "Batch 16800/19342, Loss: 0.2252, Acc: 89.79%\n",
      "Batch 16850/19342, Loss: 0.2252, Acc: 89.79%\n",
      "Batch 16900/19342, Loss: 0.2251, Acc: 89.80%\n",
      "Batch 16950/19342, Loss: 0.2251, Acc: 89.80%\n",
      "Batch 17000/19342, Loss: 0.2251, Acc: 89.80%\n",
      "Batch 17050/19342, Loss: 0.2251, Acc: 89.80%\n",
      "Batch 17100/19342, Loss: 0.2250, Acc: 89.81%\n",
      "Batch 17150/19342, Loss: 0.2250, Acc: 89.82%\n",
      "Batch 17200/19342, Loss: 0.2249, Acc: 89.82%\n",
      "Batch 17250/19342, Loss: 0.2250, Acc: 89.81%\n",
      "Batch 17300/19342, Loss: 0.2249, Acc: 89.82%\n",
      "Batch 17350/19342, Loss: 0.2248, Acc: 89.82%\n",
      "Batch 17400/19342, Loss: 0.2248, Acc: 89.82%\n",
      "Batch 17450/19342, Loss: 0.2247, Acc: 89.83%\n",
      "Batch 17500/19342, Loss: 0.2247, Acc: 89.83%\n",
      "Batch 17550/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 17600/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 17650/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 17700/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 17750/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 17800/19342, Loss: 0.2247, Acc: 89.82%\n",
      "Batch 17850/19342, Loss: 0.2247, Acc: 89.82%\n",
      "Batch 17900/19342, Loss: 0.2248, Acc: 89.82%\n",
      "Batch 17950/19342, Loss: 0.2247, Acc: 89.82%\n",
      "Batch 18000/19342, Loss: 0.2247, Acc: 89.82%\n",
      "Batch 18050/19342, Loss: 0.2248, Acc: 89.81%\n",
      "Batch 18100/19342, Loss: 0.2247, Acc: 89.82%\n",
      "Batch 18150/19342, Loss: 0.2247, Acc: 89.82%\n",
      "Batch 18200/19342, Loss: 0.2247, Acc: 89.83%\n",
      "Batch 18250/19342, Loss: 0.2247, Acc: 89.82%\n",
      "Batch 18300/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 18350/19342, Loss: 0.2247, Acc: 89.83%\n",
      "Batch 18400/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 18450/19342, Loss: 0.2246, Acc: 89.83%\n",
      "Batch 18500/19342, Loss: 0.2245, Acc: 89.84%\n",
      "Batch 18550/19342, Loss: 0.2244, Acc: 89.84%\n",
      "Batch 18600/19342, Loss: 0.2243, Acc: 89.85%\n",
      "Batch 18650/19342, Loss: 0.2243, Acc: 89.85%\n",
      "Batch 18700/19342, Loss: 0.2242, Acc: 89.85%\n",
      "Batch 18750/19342, Loss: 0.2243, Acc: 89.86%\n",
      "Batch 18800/19342, Loss: 0.2243, Acc: 89.86%\n",
      "Batch 18850/19342, Loss: 0.2242, Acc: 89.86%\n",
      "Batch 18900/19342, Loss: 0.2241, Acc: 89.87%\n",
      "Batch 18950/19342, Loss: 0.2240, Acc: 89.87%\n",
      "Batch 19000/19342, Loss: 0.2239, Acc: 89.88%\n",
      "Batch 19050/19342, Loss: 0.2239, Acc: 89.87%\n",
      "Batch 19100/19342, Loss: 0.2239, Acc: 89.87%\n",
      "Batch 19150/19342, Loss: 0.2239, Acc: 89.88%\n",
      "Batch 19200/19342, Loss: 0.2238, Acc: 89.88%\n",
      "Batch 19250/19342, Loss: 0.2237, Acc: 89.88%\n",
      "Batch 19300/19342, Loss: 0.2238, Acc: 89.88%\n",
      "Train Loss: 0.2238, Train Acc: 89.88%\n",
      "Val Loss: 0.3368, Val Acc: 85.20%\n",
      "Saving checkpoint with validation accuracy: 85.20%\n",
      "\n",
      "Epoch 4/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.2136, Acc: 90.50%\n",
      "Batch 100/19342, Loss: 0.1982, Acc: 91.38%\n",
      "Batch 150/19342, Loss: 0.1966, Acc: 91.83%\n",
      "Batch 200/19342, Loss: 0.2108, Acc: 90.88%\n",
      "Batch 250/19342, Loss: 0.2047, Acc: 90.80%\n",
      "Batch 300/19342, Loss: 0.2057, Acc: 90.88%\n",
      "Batch 350/19342, Loss: 0.2030, Acc: 91.18%\n",
      "Batch 400/19342, Loss: 0.2009, Acc: 91.16%\n",
      "Batch 450/19342, Loss: 0.2010, Acc: 91.03%\n",
      "Batch 500/19342, Loss: 0.1977, Acc: 91.17%\n",
      "Batch 550/19342, Loss: 0.1985, Acc: 91.20%\n",
      "Batch 600/19342, Loss: 0.1988, Acc: 91.27%\n",
      "Batch 650/19342, Loss: 0.1991, Acc: 91.17%\n",
      "Batch 700/19342, Loss: 0.2011, Acc: 91.00%\n",
      "Batch 750/19342, Loss: 0.2024, Acc: 90.83%\n",
      "Batch 800/19342, Loss: 0.2028, Acc: 90.81%\n",
      "Batch 850/19342, Loss: 0.2023, Acc: 90.81%\n",
      "Batch 900/19342, Loss: 0.1996, Acc: 90.94%\n",
      "Batch 950/19342, Loss: 0.2000, Acc: 90.88%\n",
      "Batch 1000/19342, Loss: 0.2027, Acc: 90.79%\n",
      "Batch 1050/19342, Loss: 0.2009, Acc: 90.82%\n",
      "Batch 1100/19342, Loss: 0.2015, Acc: 90.83%\n",
      "Batch 1150/19342, Loss: 0.2024, Acc: 90.83%\n",
      "Batch 1200/19342, Loss: 0.2033, Acc: 90.80%\n",
      "Batch 1250/19342, Loss: 0.2024, Acc: 90.86%\n",
      "Batch 1300/19342, Loss: 0.2023, Acc: 90.83%\n",
      "Batch 1350/19342, Loss: 0.2031, Acc: 90.79%\n",
      "Batch 1400/19342, Loss: 0.2019, Acc: 90.80%\n",
      "Batch 1450/19342, Loss: 0.2009, Acc: 90.92%\n",
      "Batch 1500/19342, Loss: 0.2008, Acc: 90.93%\n",
      "Batch 1550/19342, Loss: 0.2010, Acc: 90.85%\n",
      "Batch 1600/19342, Loss: 0.2019, Acc: 90.83%\n",
      "Batch 1650/19342, Loss: 0.2031, Acc: 90.77%\n",
      "Batch 1700/19342, Loss: 0.2032, Acc: 90.73%\n",
      "Batch 1750/19342, Loss: 0.2031, Acc: 90.74%\n",
      "Batch 1800/19342, Loss: 0.2035, Acc: 90.74%\n",
      "Batch 1850/19342, Loss: 0.2032, Acc: 90.78%\n",
      "Batch 1900/19342, Loss: 0.2027, Acc: 90.82%\n",
      "Batch 1950/19342, Loss: 0.2029, Acc: 90.83%\n",
      "Batch 2000/19342, Loss: 0.2039, Acc: 90.82%\n",
      "Batch 2050/19342, Loss: 0.2044, Acc: 90.79%\n",
      "Batch 2100/19342, Loss: 0.2047, Acc: 90.78%\n",
      "Batch 2150/19342, Loss: 0.2055, Acc: 90.75%\n",
      "Batch 2200/19342, Loss: 0.2058, Acc: 90.73%\n",
      "Batch 2250/19342, Loss: 0.2061, Acc: 90.73%\n",
      "Batch 2300/19342, Loss: 0.2060, Acc: 90.73%\n",
      "Batch 2350/19342, Loss: 0.2052, Acc: 90.75%\n",
      "Batch 2400/19342, Loss: 0.2048, Acc: 90.78%\n",
      "Batch 2450/19342, Loss: 0.2065, Acc: 90.67%\n",
      "Batch 2500/19342, Loss: 0.2059, Acc: 90.70%\n",
      "Batch 2550/19342, Loss: 0.2051, Acc: 90.76%\n",
      "Batch 2600/19342, Loss: 0.2056, Acc: 90.77%\n",
      "Batch 2650/19342, Loss: 0.2057, Acc: 90.78%\n",
      "Batch 2700/19342, Loss: 0.2058, Acc: 90.78%\n",
      "Batch 2750/19342, Loss: 0.2064, Acc: 90.75%\n",
      "Batch 2800/19342, Loss: 0.2068, Acc: 90.73%\n",
      "Batch 2850/19342, Loss: 0.2069, Acc: 90.73%\n",
      "Batch 2900/19342, Loss: 0.2069, Acc: 90.73%\n",
      "Batch 2950/19342, Loss: 0.2076, Acc: 90.69%\n",
      "Batch 3000/19342, Loss: 0.2073, Acc: 90.71%\n",
      "Batch 3050/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 3100/19342, Loss: 0.2075, Acc: 90.73%\n",
      "Batch 3150/19342, Loss: 0.2073, Acc: 90.74%\n",
      "Batch 3200/19342, Loss: 0.2070, Acc: 90.76%\n",
      "Batch 3250/19342, Loss: 0.2066, Acc: 90.77%\n",
      "Batch 3300/19342, Loss: 0.2063, Acc: 90.77%\n",
      "Batch 3350/19342, Loss: 0.2062, Acc: 90.77%\n",
      "Batch 3400/19342, Loss: 0.2063, Acc: 90.76%\n",
      "Batch 3450/19342, Loss: 0.2066, Acc: 90.75%\n",
      "Batch 3500/19342, Loss: 0.2063, Acc: 90.76%\n",
      "Batch 3550/19342, Loss: 0.2067, Acc: 90.76%\n",
      "Batch 3600/19342, Loss: 0.2070, Acc: 90.78%\n",
      "Batch 3650/19342, Loss: 0.2069, Acc: 90.80%\n",
      "Batch 3700/19342, Loss: 0.2067, Acc: 90.81%\n",
      "Batch 3750/19342, Loss: 0.2072, Acc: 90.81%\n",
      "Batch 3800/19342, Loss: 0.2073, Acc: 90.81%\n",
      "Batch 3850/19342, Loss: 0.2074, Acc: 90.79%\n",
      "Batch 3900/19342, Loss: 0.2072, Acc: 90.80%\n",
      "Batch 3950/19342, Loss: 0.2068, Acc: 90.83%\n",
      "Batch 4000/19342, Loss: 0.2069, Acc: 90.82%\n",
      "Batch 4050/19342, Loss: 0.2070, Acc: 90.81%\n",
      "Batch 4100/19342, Loss: 0.2071, Acc: 90.80%\n",
      "Batch 4150/19342, Loss: 0.2075, Acc: 90.78%\n",
      "Batch 4200/19342, Loss: 0.2076, Acc: 90.79%\n",
      "Batch 4250/19342, Loss: 0.2076, Acc: 90.80%\n",
      "Batch 4300/19342, Loss: 0.2074, Acc: 90.80%\n",
      "Batch 4350/19342, Loss: 0.2072, Acc: 90.81%\n",
      "Batch 4400/19342, Loss: 0.2076, Acc: 90.78%\n",
      "Batch 4450/19342, Loss: 0.2076, Acc: 90.77%\n",
      "Batch 4500/19342, Loss: 0.2076, Acc: 90.77%\n",
      "Batch 4550/19342, Loss: 0.2075, Acc: 90.79%\n",
      "Batch 4600/19342, Loss: 0.2079, Acc: 90.76%\n",
      "Batch 4650/19342, Loss: 0.2080, Acc: 90.76%\n",
      "Batch 4700/19342, Loss: 0.2086, Acc: 90.73%\n",
      "Batch 4750/19342, Loss: 0.2085, Acc: 90.73%\n",
      "Batch 4800/19342, Loss: 0.2085, Acc: 90.73%\n",
      "Batch 4850/19342, Loss: 0.2084, Acc: 90.73%\n",
      "Batch 4900/19342, Loss: 0.2084, Acc: 90.73%\n",
      "Batch 4950/19342, Loss: 0.2089, Acc: 90.70%\n",
      "Batch 5000/19342, Loss: 0.2088, Acc: 90.70%\n",
      "Batch 5050/19342, Loss: 0.2092, Acc: 90.68%\n",
      "Batch 5100/19342, Loss: 0.2088, Acc: 90.68%\n",
      "Batch 5150/19342, Loss: 0.2088, Acc: 90.69%\n",
      "Batch 5200/19342, Loss: 0.2089, Acc: 90.68%\n",
      "Batch 5250/19342, Loss: 0.2092, Acc: 90.69%\n",
      "Batch 5300/19342, Loss: 0.2091, Acc: 90.69%\n",
      "Batch 5350/19342, Loss: 0.2095, Acc: 90.68%\n",
      "Batch 5400/19342, Loss: 0.2095, Acc: 90.66%\n",
      "Batch 5450/19342, Loss: 0.2098, Acc: 90.64%\n",
      "Batch 5500/19342, Loss: 0.2098, Acc: 90.65%\n",
      "Batch 5550/19342, Loss: 0.2098, Acc: 90.66%\n",
      "Batch 5600/19342, Loss: 0.2097, Acc: 90.67%\n",
      "Batch 5650/19342, Loss: 0.2098, Acc: 90.67%\n",
      "Batch 5700/19342, Loss: 0.2099, Acc: 90.65%\n",
      "Batch 5750/19342, Loss: 0.2097, Acc: 90.66%\n",
      "Batch 5800/19342, Loss: 0.2096, Acc: 90.66%\n",
      "Batch 5850/19342, Loss: 0.2090, Acc: 90.69%\n",
      "Batch 5900/19342, Loss: 0.2091, Acc: 90.69%\n",
      "Batch 5950/19342, Loss: 0.2088, Acc: 90.70%\n",
      "Batch 6000/19342, Loss: 0.2090, Acc: 90.67%\n",
      "Batch 6050/19342, Loss: 0.2089, Acc: 90.68%\n",
      "Batch 6100/19342, Loss: 0.2084, Acc: 90.70%\n",
      "Batch 6150/19342, Loss: 0.2083, Acc: 90.72%\n",
      "Batch 6200/19342, Loss: 0.2083, Acc: 90.72%\n",
      "Batch 6250/19342, Loss: 0.2084, Acc: 90.71%\n",
      "Batch 6300/19342, Loss: 0.2084, Acc: 90.70%\n",
      "Batch 6350/19342, Loss: 0.2085, Acc: 90.70%\n",
      "Batch 6400/19342, Loss: 0.2091, Acc: 90.66%\n",
      "Batch 6450/19342, Loss: 0.2094, Acc: 90.64%\n",
      "Batch 6500/19342, Loss: 0.2092, Acc: 90.65%\n",
      "Batch 6550/19342, Loss: 0.2088, Acc: 90.68%\n",
      "Batch 6600/19342, Loss: 0.2087, Acc: 90.68%\n",
      "Batch 6650/19342, Loss: 0.2086, Acc: 90.69%\n",
      "Batch 6700/19342, Loss: 0.2089, Acc: 90.68%\n",
      "Batch 6750/19342, Loss: 0.2089, Acc: 90.68%\n",
      "Batch 6800/19342, Loss: 0.2087, Acc: 90.68%\n",
      "Batch 6850/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 6900/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 6950/19342, Loss: 0.2089, Acc: 90.67%\n",
      "Batch 7000/19342, Loss: 0.2091, Acc: 90.66%\n",
      "Batch 7050/19342, Loss: 0.2093, Acc: 90.63%\n",
      "Batch 7100/19342, Loss: 0.2091, Acc: 90.65%\n",
      "Batch 7150/19342, Loss: 0.2089, Acc: 90.66%\n",
      "Batch 7200/19342, Loss: 0.2087, Acc: 90.68%\n",
      "Batch 7250/19342, Loss: 0.2087, Acc: 90.67%\n",
      "Batch 7300/19342, Loss: 0.2086, Acc: 90.67%\n",
      "Batch 7350/19342, Loss: 0.2084, Acc: 90.68%\n",
      "Batch 7400/19342, Loss: 0.2083, Acc: 90.70%\n",
      "Batch 7450/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 7500/19342, Loss: 0.2087, Acc: 90.68%\n",
      "Batch 7550/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 7600/19342, Loss: 0.2086, Acc: 90.67%\n",
      "Batch 7650/19342, Loss: 0.2085, Acc: 90.69%\n",
      "Batch 7700/19342, Loss: 0.2087, Acc: 90.68%\n",
      "Batch 7750/19342, Loss: 0.2086, Acc: 90.69%\n",
      "Batch 7800/19342, Loss: 0.2087, Acc: 90.69%\n",
      "Batch 7850/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 7900/19342, Loss: 0.2087, Acc: 90.67%\n",
      "Batch 7950/19342, Loss: 0.2086, Acc: 90.67%\n",
      "Batch 8000/19342, Loss: 0.2086, Acc: 90.67%\n",
      "Batch 8050/19342, Loss: 0.2087, Acc: 90.66%\n",
      "Batch 8100/19342, Loss: 0.2087, Acc: 90.67%\n",
      "Batch 8150/19342, Loss: 0.2088, Acc: 90.67%\n",
      "Batch 8200/19342, Loss: 0.2089, Acc: 90.67%\n",
      "Batch 8250/19342, Loss: 0.2085, Acc: 90.69%\n",
      "Batch 8300/19342, Loss: 0.2082, Acc: 90.70%\n",
      "Batch 8350/19342, Loss: 0.2081, Acc: 90.71%\n",
      "Batch 8400/19342, Loss: 0.2081, Acc: 90.71%\n",
      "Batch 8450/19342, Loss: 0.2084, Acc: 90.69%\n",
      "Batch 8500/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 8550/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 8600/19342, Loss: 0.2088, Acc: 90.67%\n",
      "Batch 8650/19342, Loss: 0.2090, Acc: 90.66%\n",
      "Batch 8700/19342, Loss: 0.2090, Acc: 90.66%\n",
      "Batch 8750/19342, Loss: 0.2088, Acc: 90.67%\n",
      "Batch 8800/19342, Loss: 0.2090, Acc: 90.66%\n",
      "Batch 8850/19342, Loss: 0.2090, Acc: 90.67%\n",
      "Batch 8900/19342, Loss: 0.2088, Acc: 90.68%\n",
      "Batch 8950/19342, Loss: 0.2089, Acc: 90.67%\n",
      "Batch 9000/19342, Loss: 0.2090, Acc: 90.67%\n",
      "Batch 9050/19342, Loss: 0.2088, Acc: 90.68%\n",
      "Batch 9100/19342, Loss: 0.2089, Acc: 90.67%\n",
      "Batch 9150/19342, Loss: 0.2091, Acc: 90.66%\n",
      "Batch 9200/19342, Loss: 0.2090, Acc: 90.67%\n",
      "Batch 9250/19342, Loss: 0.2091, Acc: 90.66%\n",
      "Batch 9300/19342, Loss: 0.2090, Acc: 90.67%\n",
      "Batch 9350/19342, Loss: 0.2089, Acc: 90.67%\n",
      "Batch 9400/19342, Loss: 0.2089, Acc: 90.67%\n",
      "Batch 9450/19342, Loss: 0.2087, Acc: 90.67%\n",
      "Batch 9500/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 9550/19342, Loss: 0.2086, Acc: 90.68%\n",
      "Batch 9600/19342, Loss: 0.2087, Acc: 90.67%\n",
      "Batch 9650/19342, Loss: 0.2087, Acc: 90.68%\n",
      "Batch 9700/19342, Loss: 0.2088, Acc: 90.67%\n",
      "Batch 9750/19342, Loss: 0.2088, Acc: 90.66%\n",
      "Batch 9800/19342, Loss: 0.2088, Acc: 90.65%\n",
      "Batch 9850/19342, Loss: 0.2088, Acc: 90.66%\n",
      "Batch 9900/19342, Loss: 0.2088, Acc: 90.66%\n",
      "Batch 9950/19342, Loss: 0.2086, Acc: 90.67%\n",
      "Batch 10000/19342, Loss: 0.2085, Acc: 90.67%\n",
      "Batch 10050/19342, Loss: 0.2083, Acc: 90.68%\n",
      "Batch 10100/19342, Loss: 0.2083, Acc: 90.68%\n",
      "Batch 10150/19342, Loss: 0.2083, Acc: 90.67%\n",
      "Batch 10200/19342, Loss: 0.2083, Acc: 90.68%\n",
      "Batch 10250/19342, Loss: 0.2082, Acc: 90.68%\n",
      "Batch 10300/19342, Loss: 0.2082, Acc: 90.68%\n",
      "Batch 10350/19342, Loss: 0.2084, Acc: 90.66%\n",
      "Batch 10400/19342, Loss: 0.2083, Acc: 90.67%\n",
      "Batch 10450/19342, Loss: 0.2083, Acc: 90.67%\n",
      "Batch 10500/19342, Loss: 0.2082, Acc: 90.67%\n",
      "Batch 10550/19342, Loss: 0.2083, Acc: 90.67%\n",
      "Batch 10600/19342, Loss: 0.2082, Acc: 90.68%\n",
      "Batch 10650/19342, Loss: 0.2083, Acc: 90.67%\n",
      "Batch 10700/19342, Loss: 0.2082, Acc: 90.68%\n",
      "Batch 10750/19342, Loss: 0.2081, Acc: 90.68%\n",
      "Batch 10800/19342, Loss: 0.2081, Acc: 90.68%\n",
      "Batch 10850/19342, Loss: 0.2082, Acc: 90.68%\n",
      "Batch 10900/19342, Loss: 0.2081, Acc: 90.68%\n",
      "Batch 10950/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 11000/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 11050/19342, Loss: 0.2077, Acc: 90.70%\n",
      "Batch 11100/19342, Loss: 0.2077, Acc: 90.70%\n",
      "Batch 11150/19342, Loss: 0.2076, Acc: 90.70%\n",
      "Batch 11200/19342, Loss: 0.2075, Acc: 90.71%\n",
      "Batch 11250/19342, Loss: 0.2074, Acc: 90.72%\n",
      "Batch 11300/19342, Loss: 0.2075, Acc: 90.71%\n",
      "Batch 11350/19342, Loss: 0.2074, Acc: 90.72%\n",
      "Batch 11400/19342, Loss: 0.2075, Acc: 90.72%\n",
      "Batch 11450/19342, Loss: 0.2075, Acc: 90.72%\n",
      "Batch 11500/19342, Loss: 0.2076, Acc: 90.71%\n",
      "Batch 11550/19342, Loss: 0.2074, Acc: 90.72%\n",
      "Batch 11600/19342, Loss: 0.2075, Acc: 90.71%\n",
      "Batch 11650/19342, Loss: 0.2076, Acc: 90.70%\n",
      "Batch 11700/19342, Loss: 0.2078, Acc: 90.68%\n",
      "Batch 11750/19342, Loss: 0.2079, Acc: 90.68%\n",
      "Batch 11800/19342, Loss: 0.2080, Acc: 90.68%\n",
      "Batch 11850/19342, Loss: 0.2079, Acc: 90.68%\n",
      "Batch 11900/19342, Loss: 0.2079, Acc: 90.68%\n",
      "Batch 11950/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12000/19342, Loss: 0.2080, Acc: 90.69%\n",
      "Batch 12050/19342, Loss: 0.2080, Acc: 90.68%\n",
      "Batch 12100/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12150/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12200/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12250/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 12300/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 12350/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 12400/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12450/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12500/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12550/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 12600/19342, Loss: 0.2080, Acc: 90.69%\n",
      "Batch 12650/19342, Loss: 0.2081, Acc: 90.69%\n",
      "Batch 12700/19342, Loss: 0.2081, Acc: 90.68%\n",
      "Batch 12750/19342, Loss: 0.2080, Acc: 90.69%\n",
      "Batch 12800/19342, Loss: 0.2081, Acc: 90.69%\n",
      "Batch 12850/19342, Loss: 0.2081, Acc: 90.69%\n",
      "Batch 12900/19342, Loss: 0.2081, Acc: 90.69%\n",
      "Batch 12950/19342, Loss: 0.2080, Acc: 90.69%\n",
      "Batch 13000/19342, Loss: 0.2080, Acc: 90.69%\n",
      "Batch 13050/19342, Loss: 0.2080, Acc: 90.68%\n",
      "Batch 13100/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 13150/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 13200/19342, Loss: 0.2079, Acc: 90.68%\n",
      "Batch 13250/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 13300/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 13350/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 13400/19342, Loss: 0.2081, Acc: 90.68%\n",
      "Batch 13450/19342, Loss: 0.2080, Acc: 90.68%\n",
      "Batch 13500/19342, Loss: 0.2080, Acc: 90.69%\n",
      "Batch 13550/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 13600/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 13650/19342, Loss: 0.2080, Acc: 90.68%\n",
      "Batch 13700/19342, Loss: 0.2080, Acc: 90.69%\n",
      "Batch 13750/19342, Loss: 0.2079, Acc: 90.69%\n",
      "Batch 13800/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 13850/19342, Loss: 0.2078, Acc: 90.69%\n",
      "Batch 13900/19342, Loss: 0.2077, Acc: 90.69%\n",
      "Batch 13950/19342, Loss: 0.2077, Acc: 90.70%\n",
      "Batch 14000/19342, Loss: 0.2078, Acc: 90.68%\n",
      "Batch 14050/19342, Loss: 0.2076, Acc: 90.69%\n",
      "Batch 14100/19342, Loss: 0.2077, Acc: 90.68%\n",
      "Batch 14150/19342, Loss: 0.2078, Acc: 90.68%\n",
      "Batch 14200/19342, Loss: 0.2079, Acc: 90.68%\n",
      "Batch 14250/19342, Loss: 0.2078, Acc: 90.68%\n",
      "Batch 14300/19342, Loss: 0.2077, Acc: 90.69%\n",
      "Batch 14350/19342, Loss: 0.2077, Acc: 90.69%\n",
      "Batch 14400/19342, Loss: 0.2077, Acc: 90.69%\n",
      "Batch 14450/19342, Loss: 0.2076, Acc: 90.70%\n",
      "Batch 14500/19342, Loss: 0.2076, Acc: 90.70%\n",
      "Batch 14550/19342, Loss: 0.2075, Acc: 90.70%\n",
      "Batch 14600/19342, Loss: 0.2074, Acc: 90.71%\n",
      "Batch 14650/19342, Loss: 0.2073, Acc: 90.71%\n",
      "Batch 14700/19342, Loss: 0.2073, Acc: 90.71%\n",
      "Batch 14750/19342, Loss: 0.2074, Acc: 90.71%\n",
      "Batch 14800/19342, Loss: 0.2073, Acc: 90.71%\n",
      "Batch 14850/19342, Loss: 0.2072, Acc: 90.72%\n",
      "Batch 14900/19342, Loss: 0.2071, Acc: 90.73%\n",
      "Batch 14950/19342, Loss: 0.2072, Acc: 90.73%\n",
      "Batch 15000/19342, Loss: 0.2072, Acc: 90.72%\n",
      "Batch 15050/19342, Loss: 0.2073, Acc: 90.71%\n",
      "Batch 15100/19342, Loss: 0.2072, Acc: 90.72%\n",
      "Batch 15150/19342, Loss: 0.2074, Acc: 90.71%\n",
      "Batch 15200/19342, Loss: 0.2076, Acc: 90.70%\n",
      "Batch 15250/19342, Loss: 0.2077, Acc: 90.69%\n",
      "Batch 15300/19342, Loss: 0.2076, Acc: 90.70%\n",
      "Batch 15350/19342, Loss: 0.2076, Acc: 90.69%\n",
      "Batch 15400/19342, Loss: 0.2077, Acc: 90.69%\n",
      "Batch 15450/19342, Loss: 0.2076, Acc: 90.69%\n",
      "Batch 15500/19342, Loss: 0.2074, Acc: 90.71%\n",
      "Batch 15550/19342, Loss: 0.2072, Acc: 90.72%\n",
      "Batch 15600/19342, Loss: 0.2073, Acc: 90.72%\n",
      "Batch 15650/19342, Loss: 0.2072, Acc: 90.73%\n",
      "Batch 15700/19342, Loss: 0.2072, Acc: 90.73%\n",
      "Batch 15750/19342, Loss: 0.2072, Acc: 90.73%\n",
      "Batch 15800/19342, Loss: 0.2072, Acc: 90.73%\n",
      "Batch 15850/19342, Loss: 0.2072, Acc: 90.73%\n",
      "Batch 15900/19342, Loss: 0.2071, Acc: 90.73%\n",
      "Batch 15950/19342, Loss: 0.2071, Acc: 90.74%\n",
      "Batch 16000/19342, Loss: 0.2070, Acc: 90.74%\n",
      "Batch 16050/19342, Loss: 0.2071, Acc: 90.73%\n",
      "Batch 16100/19342, Loss: 0.2071, Acc: 90.73%\n",
      "Batch 16150/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16200/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16250/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16300/19342, Loss: 0.2070, Acc: 90.72%\n",
      "Batch 16350/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16400/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16450/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16500/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16550/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16600/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16650/19342, Loss: 0.2070, Acc: 90.73%\n",
      "Batch 16700/19342, Loss: 0.2070, Acc: 90.73%\n",
      "Batch 16750/19342, Loss: 0.2070, Acc: 90.73%\n",
      "Batch 16800/19342, Loss: 0.2070, Acc: 90.72%\n",
      "Batch 16850/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 16900/19342, Loss: 0.2070, Acc: 90.72%\n",
      "Batch 16950/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 17000/19342, Loss: 0.2071, Acc: 90.72%\n",
      "Batch 17050/19342, Loss: 0.2070, Acc: 90.72%\n",
      "Batch 17100/19342, Loss: 0.2069, Acc: 90.72%\n",
      "Batch 17150/19342, Loss: 0.2068, Acc: 90.73%\n",
      "Batch 17200/19342, Loss: 0.2068, Acc: 90.72%\n",
      "Batch 17250/19342, Loss: 0.2068, Acc: 90.72%\n",
      "Batch 17300/19342, Loss: 0.2067, Acc: 90.73%\n",
      "Batch 17350/19342, Loss: 0.2067, Acc: 90.73%\n",
      "Batch 17400/19342, Loss: 0.2067, Acc: 90.73%\n",
      "Batch 17450/19342, Loss: 0.2067, Acc: 90.73%\n",
      "Batch 17500/19342, Loss: 0.2065, Acc: 90.73%\n",
      "Batch 17550/19342, Loss: 0.2066, Acc: 90.73%\n",
      "Batch 17600/19342, Loss: 0.2067, Acc: 90.72%\n",
      "Batch 17650/19342, Loss: 0.2068, Acc: 90.72%\n",
      "Batch 17700/19342, Loss: 0.2067, Acc: 90.72%\n",
      "Batch 17750/19342, Loss: 0.2068, Acc: 90.71%\n",
      "Batch 17800/19342, Loss: 0.2067, Acc: 90.71%\n",
      "Batch 17850/19342, Loss: 0.2067, Acc: 90.72%\n",
      "Batch 17900/19342, Loss: 0.2067, Acc: 90.71%\n",
      "Batch 17950/19342, Loss: 0.2068, Acc: 90.71%\n",
      "Batch 18000/19342, Loss: 0.2067, Acc: 90.71%\n",
      "Batch 18050/19342, Loss: 0.2066, Acc: 90.72%\n",
      "Batch 18100/19342, Loss: 0.2066, Acc: 90.71%\n",
      "Batch 18150/19342, Loss: 0.2066, Acc: 90.71%\n",
      "Batch 18200/19342, Loss: 0.2065, Acc: 90.71%\n",
      "Batch 18250/19342, Loss: 0.2064, Acc: 90.72%\n",
      "Batch 18300/19342, Loss: 0.2063, Acc: 90.73%\n",
      "Batch 18350/19342, Loss: 0.2064, Acc: 90.72%\n",
      "Batch 18400/19342, Loss: 0.2063, Acc: 90.72%\n",
      "Batch 18450/19342, Loss: 0.2064, Acc: 90.72%\n",
      "Batch 18500/19342, Loss: 0.2063, Acc: 90.73%\n",
      "Batch 18550/19342, Loss: 0.2062, Acc: 90.73%\n",
      "Batch 18600/19342, Loss: 0.2061, Acc: 90.73%\n",
      "Batch 18650/19342, Loss: 0.2061, Acc: 90.73%\n",
      "Batch 18700/19342, Loss: 0.2061, Acc: 90.74%\n",
      "Batch 18750/19342, Loss: 0.2061, Acc: 90.74%\n",
      "Batch 18800/19342, Loss: 0.2061, Acc: 90.73%\n",
      "Batch 18850/19342, Loss: 0.2061, Acc: 90.74%\n",
      "Batch 18900/19342, Loss: 0.2060, Acc: 90.74%\n",
      "Batch 18950/19342, Loss: 0.2059, Acc: 90.74%\n",
      "Batch 19000/19342, Loss: 0.2058, Acc: 90.75%\n",
      "Batch 19050/19342, Loss: 0.2057, Acc: 90.75%\n",
      "Batch 19100/19342, Loss: 0.2056, Acc: 90.76%\n",
      "Batch 19150/19342, Loss: 0.2056, Acc: 90.76%\n",
      "Batch 19200/19342, Loss: 0.2057, Acc: 90.75%\n",
      "Batch 19250/19342, Loss: 0.2058, Acc: 90.75%\n",
      "Batch 19300/19342, Loss: 0.2057, Acc: 90.75%\n",
      "Train Loss: 0.2058, Train Acc: 90.74%\n",
      "Val Loss: 0.2748, Val Acc: 87.91%\n",
      "Saving checkpoint with validation accuracy: 87.91%\n",
      "\n",
      "Epoch 5/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.1824, Acc: 90.25%\n",
      "Batch 100/19342, Loss: 0.1955, Acc: 90.75%\n",
      "Batch 150/19342, Loss: 0.1915, Acc: 91.33%\n",
      "Batch 200/19342, Loss: 0.1984, Acc: 91.00%\n",
      "Batch 250/19342, Loss: 0.2057, Acc: 90.70%\n",
      "Batch 300/19342, Loss: 0.2050, Acc: 90.88%\n",
      "Batch 350/19342, Loss: 0.2060, Acc: 90.79%\n",
      "Batch 400/19342, Loss: 0.2061, Acc: 90.72%\n",
      "Batch 450/19342, Loss: 0.2042, Acc: 90.81%\n",
      "Batch 500/19342, Loss: 0.2101, Acc: 90.50%\n",
      "Batch 550/19342, Loss: 0.2119, Acc: 90.25%\n",
      "Batch 600/19342, Loss: 0.2098, Acc: 90.40%\n",
      "Batch 650/19342, Loss: 0.2083, Acc: 90.52%\n",
      "Batch 700/19342, Loss: 0.2098, Acc: 90.50%\n",
      "Batch 750/19342, Loss: 0.2095, Acc: 90.48%\n",
      "Batch 800/19342, Loss: 0.2079, Acc: 90.56%\n",
      "Batch 850/19342, Loss: 0.2068, Acc: 90.62%\n",
      "Batch 900/19342, Loss: 0.2035, Acc: 90.81%\n",
      "Batch 950/19342, Loss: 0.2034, Acc: 90.92%\n",
      "Batch 1000/19342, Loss: 0.2011, Acc: 91.03%\n",
      "Batch 1050/19342, Loss: 0.2013, Acc: 91.05%\n",
      "Batch 1100/19342, Loss: 0.2009, Acc: 91.09%\n",
      "Batch 1150/19342, Loss: 0.2007, Acc: 91.03%\n",
      "Batch 1200/19342, Loss: 0.2007, Acc: 91.01%\n",
      "Batch 1250/19342, Loss: 0.2009, Acc: 91.07%\n",
      "Batch 1300/19342, Loss: 0.2013, Acc: 91.06%\n",
      "Batch 1350/19342, Loss: 0.2018, Acc: 91.06%\n",
      "Batch 1400/19342, Loss: 0.2012, Acc: 91.07%\n",
      "Batch 1450/19342, Loss: 0.2011, Acc: 91.02%\n",
      "Batch 1500/19342, Loss: 0.2009, Acc: 91.03%\n",
      "Batch 1550/19342, Loss: 0.1987, Acc: 91.13%\n",
      "Batch 1600/19342, Loss: 0.1991, Acc: 91.12%\n",
      "Batch 1650/19342, Loss: 0.1981, Acc: 91.18%\n",
      "Batch 1700/19342, Loss: 0.1973, Acc: 91.19%\n",
      "Batch 1750/19342, Loss: 0.1977, Acc: 91.15%\n",
      "Batch 1800/19342, Loss: 0.1969, Acc: 91.19%\n",
      "Batch 1850/19342, Loss: 0.1981, Acc: 91.15%\n",
      "Batch 1900/19342, Loss: 0.1969, Acc: 91.18%\n",
      "Batch 1950/19342, Loss: 0.1972, Acc: 91.19%\n",
      "Batch 2000/19342, Loss: 0.1966, Acc: 91.26%\n",
      "Batch 2050/19342, Loss: 0.1966, Acc: 91.24%\n",
      "Batch 2100/19342, Loss: 0.1961, Acc: 91.26%\n",
      "Batch 2150/19342, Loss: 0.1965, Acc: 91.23%\n",
      "Batch 2200/19342, Loss: 0.1959, Acc: 91.24%\n",
      "Batch 2250/19342, Loss: 0.1961, Acc: 91.23%\n",
      "Batch 2300/19342, Loss: 0.1959, Acc: 91.28%\n",
      "Batch 2350/19342, Loss: 0.1961, Acc: 91.29%\n",
      "Batch 2400/19342, Loss: 0.1958, Acc: 91.31%\n",
      "Batch 2450/19342, Loss: 0.1961, Acc: 91.29%\n",
      "Batch 2500/19342, Loss: 0.1957, Acc: 91.31%\n",
      "Batch 2550/19342, Loss: 0.1960, Acc: 91.27%\n",
      "Batch 2600/19342, Loss: 0.1952, Acc: 91.31%\n",
      "Batch 2650/19342, Loss: 0.1947, Acc: 91.31%\n",
      "Batch 2700/19342, Loss: 0.1945, Acc: 91.29%\n",
      "Batch 2750/19342, Loss: 0.1937, Acc: 91.31%\n",
      "Batch 2800/19342, Loss: 0.1936, Acc: 91.32%\n",
      "Batch 2850/19342, Loss: 0.1944, Acc: 91.29%\n",
      "Batch 2900/19342, Loss: 0.1939, Acc: 91.30%\n",
      "Batch 2950/19342, Loss: 0.1937, Acc: 91.33%\n",
      "Batch 3000/19342, Loss: 0.1941, Acc: 91.28%\n",
      "Batch 3050/19342, Loss: 0.1943, Acc: 91.25%\n",
      "Batch 3100/19342, Loss: 0.1938, Acc: 91.29%\n",
      "Batch 3150/19342, Loss: 0.1937, Acc: 91.29%\n",
      "Batch 3200/19342, Loss: 0.1936, Acc: 91.30%\n",
      "Batch 3250/19342, Loss: 0.1932, Acc: 91.32%\n",
      "Batch 3300/19342, Loss: 0.1934, Acc: 91.30%\n",
      "Batch 3350/19342, Loss: 0.1933, Acc: 91.28%\n",
      "Batch 3400/19342, Loss: 0.1928, Acc: 91.31%\n",
      "Batch 3450/19342, Loss: 0.1925, Acc: 91.33%\n",
      "Batch 3500/19342, Loss: 0.1927, Acc: 91.33%\n",
      "Batch 3550/19342, Loss: 0.1928, Acc: 91.33%\n",
      "Batch 3600/19342, Loss: 0.1924, Acc: 91.35%\n",
      "Batch 3650/19342, Loss: 0.1925, Acc: 91.33%\n",
      "Batch 3700/19342, Loss: 0.1925, Acc: 91.32%\n",
      "Batch 3750/19342, Loss: 0.1926, Acc: 91.31%\n",
      "Batch 3800/19342, Loss: 0.1926, Acc: 91.33%\n",
      "Batch 3850/19342, Loss: 0.1923, Acc: 91.35%\n",
      "Batch 3900/19342, Loss: 0.1926, Acc: 91.35%\n",
      "Batch 3950/19342, Loss: 0.1926, Acc: 91.34%\n",
      "Batch 4000/19342, Loss: 0.1924, Acc: 91.35%\n",
      "Batch 4050/19342, Loss: 0.1920, Acc: 91.35%\n",
      "Batch 4100/19342, Loss: 0.1918, Acc: 91.36%\n",
      "Batch 4150/19342, Loss: 0.1920, Acc: 91.35%\n",
      "Batch 4200/19342, Loss: 0.1920, Acc: 91.37%\n",
      "Batch 4250/19342, Loss: 0.1920, Acc: 91.37%\n",
      "Batch 4300/19342, Loss: 0.1922, Acc: 91.37%\n",
      "Batch 4350/19342, Loss: 0.1922, Acc: 91.36%\n",
      "Batch 4400/19342, Loss: 0.1920, Acc: 91.38%\n",
      "Batch 4450/19342, Loss: 0.1919, Acc: 91.38%\n",
      "Batch 4500/19342, Loss: 0.1921, Acc: 91.36%\n",
      "Batch 4550/19342, Loss: 0.1921, Acc: 91.36%\n",
      "Batch 4600/19342, Loss: 0.1917, Acc: 91.38%\n",
      "Batch 4650/19342, Loss: 0.1916, Acc: 91.38%\n",
      "Batch 4700/19342, Loss: 0.1920, Acc: 91.38%\n",
      "Batch 4750/19342, Loss: 0.1919, Acc: 91.39%\n",
      "Batch 4800/19342, Loss: 0.1922, Acc: 91.38%\n",
      "Batch 4850/19342, Loss: 0.1919, Acc: 91.40%\n",
      "Batch 4900/19342, Loss: 0.1924, Acc: 91.40%\n",
      "Batch 4950/19342, Loss: 0.1925, Acc: 91.40%\n",
      "Batch 5000/19342, Loss: 0.1926, Acc: 91.41%\n",
      "Batch 5050/19342, Loss: 0.1929, Acc: 91.39%\n",
      "Batch 5100/19342, Loss: 0.1934, Acc: 91.36%\n",
      "Batch 5150/19342, Loss: 0.1931, Acc: 91.38%\n",
      "Batch 5200/19342, Loss: 0.1928, Acc: 91.38%\n",
      "Batch 5250/19342, Loss: 0.1927, Acc: 91.39%\n",
      "Batch 5300/19342, Loss: 0.1927, Acc: 91.38%\n",
      "Batch 5350/19342, Loss: 0.1928, Acc: 91.37%\n",
      "Batch 5400/19342, Loss: 0.1932, Acc: 91.35%\n",
      "Batch 5450/19342, Loss: 0.1932, Acc: 91.37%\n",
      "Batch 5500/19342, Loss: 0.1932, Acc: 91.37%\n",
      "Batch 5550/19342, Loss: 0.1932, Acc: 91.37%\n",
      "Batch 5600/19342, Loss: 0.1931, Acc: 91.37%\n",
      "Batch 5650/19342, Loss: 0.1933, Acc: 91.37%\n",
      "Batch 5700/19342, Loss: 0.1935, Acc: 91.36%\n",
      "Batch 5750/19342, Loss: 0.1933, Acc: 91.37%\n",
      "Batch 5800/19342, Loss: 0.1932, Acc: 91.37%\n",
      "Batch 5850/19342, Loss: 0.1929, Acc: 91.38%\n",
      "Batch 5900/19342, Loss: 0.1931, Acc: 91.38%\n",
      "Batch 5950/19342, Loss: 0.1931, Acc: 91.37%\n",
      "Batch 6000/19342, Loss: 0.1930, Acc: 91.36%\n",
      "Batch 6050/19342, Loss: 0.1934, Acc: 91.34%\n",
      "Batch 6100/19342, Loss: 0.1934, Acc: 91.34%\n",
      "Batch 6150/19342, Loss: 0.1936, Acc: 91.33%\n",
      "Batch 6200/19342, Loss: 0.1939, Acc: 91.31%\n",
      "Batch 6250/19342, Loss: 0.1943, Acc: 91.30%\n",
      "Batch 6300/19342, Loss: 0.1945, Acc: 91.29%\n",
      "Batch 6350/19342, Loss: 0.1942, Acc: 91.29%\n",
      "Batch 6400/19342, Loss: 0.1942, Acc: 91.30%\n",
      "Batch 6450/19342, Loss: 0.1943, Acc: 91.29%\n",
      "Batch 6500/19342, Loss: 0.1946, Acc: 91.28%\n",
      "Batch 6550/19342, Loss: 0.1945, Acc: 91.27%\n",
      "Batch 6600/19342, Loss: 0.1943, Acc: 91.30%\n",
      "Batch 6650/19342, Loss: 0.1940, Acc: 91.31%\n",
      "Batch 6700/19342, Loss: 0.1937, Acc: 91.32%\n",
      "Batch 6750/19342, Loss: 0.1939, Acc: 91.32%\n",
      "Batch 6800/19342, Loss: 0.1938, Acc: 91.33%\n",
      "Batch 6850/19342, Loss: 0.1941, Acc: 91.31%\n",
      "Batch 6900/19342, Loss: 0.1940, Acc: 91.30%\n",
      "Batch 6950/19342, Loss: 0.1940, Acc: 91.30%\n",
      "Batch 7000/19342, Loss: 0.1940, Acc: 91.30%\n",
      "Batch 7050/19342, Loss: 0.1939, Acc: 91.31%\n",
      "Batch 7100/19342, Loss: 0.1940, Acc: 91.30%\n",
      "Batch 7150/19342, Loss: 0.1942, Acc: 91.30%\n",
      "Batch 7200/19342, Loss: 0.1945, Acc: 91.29%\n",
      "Batch 7250/19342, Loss: 0.1943, Acc: 91.30%\n",
      "Batch 7300/19342, Loss: 0.1941, Acc: 91.31%\n",
      "Batch 7350/19342, Loss: 0.1939, Acc: 91.32%\n",
      "Batch 7400/19342, Loss: 0.1938, Acc: 91.33%\n",
      "Batch 7450/19342, Loss: 0.1939, Acc: 91.32%\n",
      "Batch 7500/19342, Loss: 0.1936, Acc: 91.33%\n",
      "Batch 7550/19342, Loss: 0.1934, Acc: 91.34%\n",
      "Batch 7600/19342, Loss: 0.1935, Acc: 91.33%\n",
      "Batch 7650/19342, Loss: 0.1936, Acc: 91.33%\n",
      "Batch 7700/19342, Loss: 0.1936, Acc: 91.33%\n",
      "Batch 7750/19342, Loss: 0.1935, Acc: 91.34%\n",
      "Batch 7800/19342, Loss: 0.1938, Acc: 91.33%\n",
      "Batch 7850/19342, Loss: 0.1936, Acc: 91.35%\n",
      "Batch 7900/19342, Loss: 0.1937, Acc: 91.35%\n",
      "Batch 7950/19342, Loss: 0.1937, Acc: 91.35%\n",
      "Batch 8000/19342, Loss: 0.1937, Acc: 91.35%\n",
      "Batch 8050/19342, Loss: 0.1938, Acc: 91.35%\n",
      "Batch 8100/19342, Loss: 0.1936, Acc: 91.35%\n",
      "Batch 8150/19342, Loss: 0.1936, Acc: 91.34%\n",
      "Batch 8200/19342, Loss: 0.1934, Acc: 91.36%\n",
      "Batch 8250/19342, Loss: 0.1935, Acc: 91.36%\n",
      "Batch 8300/19342, Loss: 0.1936, Acc: 91.36%\n",
      "Batch 8350/19342, Loss: 0.1935, Acc: 91.36%\n",
      "Batch 8400/19342, Loss: 0.1932, Acc: 91.37%\n",
      "Batch 8450/19342, Loss: 0.1931, Acc: 91.37%\n",
      "Batch 8500/19342, Loss: 0.1932, Acc: 91.38%\n",
      "Batch 8550/19342, Loss: 0.1934, Acc: 91.37%\n",
      "Batch 8600/19342, Loss: 0.1934, Acc: 91.37%\n",
      "Batch 8650/19342, Loss: 0.1933, Acc: 91.38%\n",
      "Batch 8700/19342, Loss: 0.1932, Acc: 91.39%\n",
      "Batch 8750/19342, Loss: 0.1932, Acc: 91.40%\n",
      "Batch 8800/19342, Loss: 0.1933, Acc: 91.38%\n",
      "Batch 8850/19342, Loss: 0.1936, Acc: 91.37%\n",
      "Batch 8900/19342, Loss: 0.1936, Acc: 91.38%\n",
      "Batch 8950/19342, Loss: 0.1934, Acc: 91.38%\n",
      "Batch 9000/19342, Loss: 0.1934, Acc: 91.38%\n",
      "Batch 9050/19342, Loss: 0.1934, Acc: 91.38%\n",
      "Batch 9100/19342, Loss: 0.1937, Acc: 91.36%\n",
      "Batch 9150/19342, Loss: 0.1939, Acc: 91.36%\n",
      "Batch 9200/19342, Loss: 0.1939, Acc: 91.35%\n",
      "Batch 9250/19342, Loss: 0.1941, Acc: 91.35%\n",
      "Batch 9300/19342, Loss: 0.1939, Acc: 91.36%\n",
      "Batch 9350/19342, Loss: 0.1938, Acc: 91.37%\n",
      "Batch 9400/19342, Loss: 0.1938, Acc: 91.38%\n",
      "Batch 9450/19342, Loss: 0.1938, Acc: 91.37%\n",
      "Batch 9500/19342, Loss: 0.1937, Acc: 91.38%\n",
      "Batch 9550/19342, Loss: 0.1937, Acc: 91.37%\n",
      "Batch 9600/19342, Loss: 0.1936, Acc: 91.38%\n",
      "Batch 9650/19342, Loss: 0.1935, Acc: 91.39%\n",
      "Batch 9700/19342, Loss: 0.1935, Acc: 91.39%\n",
      "Batch 9750/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 9800/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 9850/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 9900/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 9950/19342, Loss: 0.1933, Acc: 91.39%\n",
      "Batch 10000/19342, Loss: 0.1936, Acc: 91.38%\n",
      "Batch 10050/19342, Loss: 0.1936, Acc: 91.38%\n",
      "Batch 10100/19342, Loss: 0.1935, Acc: 91.38%\n",
      "Batch 10150/19342, Loss: 0.1933, Acc: 91.39%\n",
      "Batch 10200/19342, Loss: 0.1933, Acc: 91.39%\n",
      "Batch 10250/19342, Loss: 0.1934, Acc: 91.38%\n",
      "Batch 10300/19342, Loss: 0.1934, Acc: 91.38%\n",
      "Batch 10350/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 10400/19342, Loss: 0.1935, Acc: 91.39%\n",
      "Batch 10450/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 10500/19342, Loss: 0.1935, Acc: 91.39%\n",
      "Batch 10550/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 10600/19342, Loss: 0.1935, Acc: 91.39%\n",
      "Batch 10650/19342, Loss: 0.1935, Acc: 91.38%\n",
      "Batch 10700/19342, Loss: 0.1935, Acc: 91.38%\n",
      "Batch 10750/19342, Loss: 0.1935, Acc: 91.38%\n",
      "Batch 10800/19342, Loss: 0.1936, Acc: 91.38%\n",
      "Batch 10850/19342, Loss: 0.1937, Acc: 91.37%\n",
      "Batch 10900/19342, Loss: 0.1935, Acc: 91.38%\n",
      "Batch 10950/19342, Loss: 0.1935, Acc: 91.37%\n",
      "Batch 11000/19342, Loss: 0.1937, Acc: 91.37%\n",
      "Batch 11050/19342, Loss: 0.1937, Acc: 91.37%\n",
      "Batch 11100/19342, Loss: 0.1936, Acc: 91.37%\n",
      "Batch 11150/19342, Loss: 0.1936, Acc: 91.38%\n",
      "Batch 11200/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 11250/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 11300/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 11350/19342, Loss: 0.1932, Acc: 91.40%\n",
      "Batch 11400/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 11450/19342, Loss: 0.1932, Acc: 91.40%\n",
      "Batch 11500/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 11550/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 11600/19342, Loss: 0.1935, Acc: 91.40%\n",
      "Batch 11650/19342, Loss: 0.1935, Acc: 91.40%\n",
      "Batch 11700/19342, Loss: 0.1935, Acc: 91.39%\n",
      "Batch 11750/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 11800/19342, Loss: 0.1932, Acc: 91.41%\n",
      "Batch 11850/19342, Loss: 0.1931, Acc: 91.41%\n",
      "Batch 11900/19342, Loss: 0.1931, Acc: 91.42%\n",
      "Batch 11950/19342, Loss: 0.1931, Acc: 91.41%\n",
      "Batch 12000/19342, Loss: 0.1933, Acc: 91.41%\n",
      "Batch 12050/19342, Loss: 0.1933, Acc: 91.41%\n",
      "Batch 12100/19342, Loss: 0.1933, Acc: 91.41%\n",
      "Batch 12150/19342, Loss: 0.1933, Acc: 91.41%\n",
      "Batch 12200/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 12250/19342, Loss: 0.1932, Acc: 91.41%\n",
      "Batch 12300/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 12350/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 12400/19342, Loss: 0.1934, Acc: 91.39%\n",
      "Batch 12450/19342, Loss: 0.1933, Acc: 91.39%\n",
      "Batch 12500/19342, Loss: 0.1933, Acc: 91.39%\n",
      "Batch 12550/19342, Loss: 0.1932, Acc: 91.40%\n",
      "Batch 12600/19342, Loss: 0.1932, Acc: 91.41%\n",
      "Batch 12650/19342, Loss: 0.1931, Acc: 91.41%\n",
      "Batch 12700/19342, Loss: 0.1932, Acc: 91.40%\n",
      "Batch 12750/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 12800/19342, Loss: 0.1936, Acc: 91.40%\n",
      "Batch 12850/19342, Loss: 0.1936, Acc: 91.39%\n",
      "Batch 12900/19342, Loss: 0.1937, Acc: 91.38%\n",
      "Batch 12950/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 13000/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 13050/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 13100/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 13150/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 13200/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 13250/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 13300/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 13350/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 13400/19342, Loss: 0.1933, Acc: 91.40%\n",
      "Batch 13450/19342, Loss: 0.1932, Acc: 91.40%\n",
      "Batch 13500/19342, Loss: 0.1932, Acc: 91.41%\n",
      "Batch 13550/19342, Loss: 0.1931, Acc: 91.40%\n",
      "Batch 13600/19342, Loss: 0.1931, Acc: 91.41%\n",
      "Batch 13650/19342, Loss: 0.1931, Acc: 91.41%\n",
      "Batch 13700/19342, Loss: 0.1933, Acc: 91.41%\n",
      "Batch 13750/19342, Loss: 0.1934, Acc: 91.41%\n",
      "Batch 13800/19342, Loss: 0.1934, Acc: 91.41%\n",
      "Batch 13850/19342, Loss: 0.1935, Acc: 91.40%\n",
      "Batch 13900/19342, Loss: 0.1936, Acc: 91.40%\n",
      "Batch 13950/19342, Loss: 0.1936, Acc: 91.40%\n",
      "Batch 14000/19342, Loss: 0.1935, Acc: 91.40%\n",
      "Batch 14050/19342, Loss: 0.1935, Acc: 91.40%\n",
      "Batch 14100/19342, Loss: 0.1935, Acc: 91.40%\n",
      "Batch 14150/19342, Loss: 0.1935, Acc: 91.40%\n",
      "Batch 14200/19342, Loss: 0.1934, Acc: 91.40%\n",
      "Batch 14250/19342, Loss: 0.1932, Acc: 91.41%\n",
      "Batch 14300/19342, Loss: 0.1932, Acc: 91.42%\n",
      "Batch 14350/19342, Loss: 0.1931, Acc: 91.42%\n",
      "Batch 14400/19342, Loss: 0.1931, Acc: 91.42%\n",
      "Batch 14450/19342, Loss: 0.1930, Acc: 91.43%\n",
      "Batch 14500/19342, Loss: 0.1931, Acc: 91.42%\n",
      "Batch 14550/19342, Loss: 0.1930, Acc: 91.42%\n",
      "Batch 14600/19342, Loss: 0.1932, Acc: 91.42%\n",
      "Batch 14650/19342, Loss: 0.1932, Acc: 91.43%\n",
      "Batch 14700/19342, Loss: 0.1931, Acc: 91.43%\n",
      "Batch 14750/19342, Loss: 0.1931, Acc: 91.43%\n",
      "Batch 14800/19342, Loss: 0.1931, Acc: 91.43%\n",
      "Batch 14850/19342, Loss: 0.1930, Acc: 91.43%\n",
      "Batch 14900/19342, Loss: 0.1931, Acc: 91.43%\n",
      "Batch 14950/19342, Loss: 0.1930, Acc: 91.43%\n",
      "Batch 15000/19342, Loss: 0.1931, Acc: 91.43%\n",
      "Batch 15050/19342, Loss: 0.1930, Acc: 91.44%\n",
      "Batch 15100/19342, Loss: 0.1930, Acc: 91.44%\n",
      "Batch 15150/19342, Loss: 0.1929, Acc: 91.45%\n",
      "Batch 15200/19342, Loss: 0.1930, Acc: 91.45%\n",
      "Batch 15250/19342, Loss: 0.1930, Acc: 91.45%\n",
      "Batch 15300/19342, Loss: 0.1931, Acc: 91.44%\n",
      "Batch 15350/19342, Loss: 0.1931, Acc: 91.44%\n",
      "Batch 15400/19342, Loss: 0.1932, Acc: 91.44%\n",
      "Batch 15450/19342, Loss: 0.1931, Acc: 91.44%\n",
      "Batch 15500/19342, Loss: 0.1932, Acc: 91.44%\n",
      "Batch 15550/19342, Loss: 0.1931, Acc: 91.45%\n",
      "Batch 15600/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 15650/19342, Loss: 0.1931, Acc: 91.45%\n",
      "Batch 15700/19342, Loss: 0.1931, Acc: 91.45%\n",
      "Batch 15750/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 15800/19342, Loss: 0.1932, Acc: 91.44%\n",
      "Batch 15850/19342, Loss: 0.1932, Acc: 91.44%\n",
      "Batch 15900/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 15950/19342, Loss: 0.1933, Acc: 91.43%\n",
      "Batch 16000/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 16050/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 16100/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 16150/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 16200/19342, Loss: 0.1931, Acc: 91.44%\n",
      "Batch 16250/19342, Loss: 0.1931, Acc: 91.44%\n",
      "Batch 16300/19342, Loss: 0.1932, Acc: 91.44%\n",
      "Batch 16350/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 16400/19342, Loss: 0.1934, Acc: 91.44%\n",
      "Batch 16450/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 16500/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 16550/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 16600/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 16650/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 16700/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 16750/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 16800/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 16850/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 16900/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 16950/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17000/19342, Loss: 0.1931, Acc: 91.46%\n",
      "Batch 17050/19342, Loss: 0.1931, Acc: 91.45%\n",
      "Batch 17100/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 17150/19342, Loss: 0.1932, Acc: 91.44%\n",
      "Batch 17200/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17250/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17300/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17350/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17400/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17450/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17500/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17550/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17600/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17650/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17700/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17750/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 17800/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17850/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17900/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 17950/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 18000/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 18050/19342, Loss: 0.1933, Acc: 91.45%\n",
      "Batch 18100/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 18150/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 18200/19342, Loss: 0.1932, Acc: 91.45%\n",
      "Batch 18250/19342, Loss: 0.1934, Acc: 91.44%\n",
      "Batch 18300/19342, Loss: 0.1934, Acc: 91.45%\n",
      "Batch 18350/19342, Loss: 0.1934, Acc: 91.44%\n",
      "Batch 18400/19342, Loss: 0.1934, Acc: 91.44%\n",
      "Batch 18450/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 18500/19342, Loss: 0.1934, Acc: 91.44%\n",
      "Batch 18550/19342, Loss: 0.1934, Acc: 91.44%\n",
      "Batch 18600/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 18650/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 18700/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 18750/19342, Loss: 0.1933, Acc: 91.44%\n",
      "Batch 18800/19342, Loss: 0.1933, Acc: 91.43%\n",
      "Batch 18850/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 18900/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 18950/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 19000/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 19050/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 19100/19342, Loss: 0.1934, Acc: 91.43%\n",
      "Batch 19150/19342, Loss: 0.1933, Acc: 91.43%\n",
      "Batch 19200/19342, Loss: 0.1933, Acc: 91.43%\n",
      "Batch 19250/19342, Loss: 0.1933, Acc: 91.43%\n",
      "Batch 19300/19342, Loss: 0.1933, Acc: 91.43%\n",
      "Train Loss: 0.1932, Train Acc: 91.43%\n",
      "Val Loss: 0.2821, Val Acc: 87.95%\n",
      "Saving checkpoint with validation accuracy: 87.95%\n",
      "\n",
      "Epoch 6/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.1710, Acc: 92.25%\n",
      "Batch 100/19342, Loss: 0.1694, Acc: 92.50%\n",
      "Batch 150/19342, Loss: 0.1759, Acc: 92.00%\n",
      "Batch 200/19342, Loss: 0.1837, Acc: 91.50%\n",
      "Batch 250/19342, Loss: 0.1776, Acc: 91.40%\n",
      "Batch 300/19342, Loss: 0.1720, Acc: 91.92%\n",
      "Batch 350/19342, Loss: 0.1740, Acc: 91.79%\n",
      "Batch 400/19342, Loss: 0.1773, Acc: 91.75%\n",
      "Batch 450/19342, Loss: 0.1748, Acc: 91.86%\n",
      "Batch 500/19342, Loss: 0.1739, Acc: 92.10%\n",
      "Batch 550/19342, Loss: 0.1728, Acc: 92.14%\n",
      "Batch 600/19342, Loss: 0.1717, Acc: 92.33%\n",
      "Batch 650/19342, Loss: 0.1685, Acc: 92.46%\n",
      "Batch 700/19342, Loss: 0.1674, Acc: 92.48%\n",
      "Batch 750/19342, Loss: 0.1682, Acc: 92.47%\n",
      "Batch 800/19342, Loss: 0.1687, Acc: 92.38%\n",
      "Batch 850/19342, Loss: 0.1708, Acc: 92.29%\n",
      "Batch 900/19342, Loss: 0.1701, Acc: 92.33%\n",
      "Batch 950/19342, Loss: 0.1711, Acc: 92.29%\n",
      "Batch 1000/19342, Loss: 0.1712, Acc: 92.29%\n",
      "Batch 1050/19342, Loss: 0.1726, Acc: 92.30%\n",
      "Batch 1100/19342, Loss: 0.1727, Acc: 92.34%\n",
      "Batch 1150/19342, Loss: 0.1729, Acc: 92.34%\n",
      "Batch 1200/19342, Loss: 0.1729, Acc: 92.35%\n",
      "Batch 1250/19342, Loss: 0.1738, Acc: 92.37%\n",
      "Batch 1300/19342, Loss: 0.1747, Acc: 92.37%\n",
      "Batch 1350/19342, Loss: 0.1761, Acc: 92.29%\n",
      "Batch 1400/19342, Loss: 0.1757, Acc: 92.29%\n",
      "Batch 1450/19342, Loss: 0.1762, Acc: 92.28%\n",
      "Batch 1500/19342, Loss: 0.1766, Acc: 92.22%\n",
      "Batch 1550/19342, Loss: 0.1777, Acc: 92.18%\n",
      "Batch 1600/19342, Loss: 0.1789, Acc: 92.09%\n",
      "Batch 1650/19342, Loss: 0.1780, Acc: 92.11%\n",
      "Batch 1700/19342, Loss: 0.1780, Acc: 92.13%\n",
      "Batch 1750/19342, Loss: 0.1778, Acc: 92.13%\n",
      "Batch 1800/19342, Loss: 0.1772, Acc: 92.16%\n",
      "Batch 1850/19342, Loss: 0.1770, Acc: 92.12%\n",
      "Batch 1900/19342, Loss: 0.1776, Acc: 92.09%\n",
      "Batch 1950/19342, Loss: 0.1773, Acc: 92.11%\n",
      "Batch 2000/19342, Loss: 0.1770, Acc: 92.15%\n",
      "Batch 2050/19342, Loss: 0.1771, Acc: 92.16%\n",
      "Batch 2100/19342, Loss: 0.1768, Acc: 92.18%\n",
      "Batch 2150/19342, Loss: 0.1773, Acc: 92.16%\n",
      "Batch 2200/19342, Loss: 0.1774, Acc: 92.17%\n",
      "Batch 2250/19342, Loss: 0.1779, Acc: 92.13%\n",
      "Batch 2300/19342, Loss: 0.1773, Acc: 92.16%\n",
      "Batch 2350/19342, Loss: 0.1770, Acc: 92.18%\n",
      "Batch 2400/19342, Loss: 0.1769, Acc: 92.19%\n",
      "Batch 2450/19342, Loss: 0.1767, Acc: 92.19%\n",
      "Batch 2500/19342, Loss: 0.1766, Acc: 92.19%\n",
      "Batch 2550/19342, Loss: 0.1765, Acc: 92.19%\n",
      "Batch 2600/19342, Loss: 0.1770, Acc: 92.19%\n",
      "Batch 2650/19342, Loss: 0.1766, Acc: 92.22%\n",
      "Batch 2700/19342, Loss: 0.1770, Acc: 92.18%\n",
      "Batch 2750/19342, Loss: 0.1768, Acc: 92.20%\n",
      "Batch 2800/19342, Loss: 0.1771, Acc: 92.21%\n",
      "Batch 2850/19342, Loss: 0.1778, Acc: 92.17%\n",
      "Batch 2900/19342, Loss: 0.1770, Acc: 92.19%\n",
      "Batch 2950/19342, Loss: 0.1770, Acc: 92.18%\n",
      "Batch 3000/19342, Loss: 0.1777, Acc: 92.17%\n",
      "Batch 3050/19342, Loss: 0.1776, Acc: 92.17%\n",
      "Batch 3100/19342, Loss: 0.1771, Acc: 92.19%\n",
      "Batch 3150/19342, Loss: 0.1771, Acc: 92.19%\n",
      "Batch 3200/19342, Loss: 0.1772, Acc: 92.19%\n",
      "Batch 3250/19342, Loss: 0.1770, Acc: 92.18%\n",
      "Batch 3300/19342, Loss: 0.1773, Acc: 92.15%\n",
      "Batch 3350/19342, Loss: 0.1773, Acc: 92.13%\n",
      "Batch 3400/19342, Loss: 0.1779, Acc: 92.14%\n",
      "Batch 3450/19342, Loss: 0.1775, Acc: 92.17%\n",
      "Batch 3500/19342, Loss: 0.1774, Acc: 92.17%\n",
      "Batch 3550/19342, Loss: 0.1775, Acc: 92.18%\n",
      "Batch 3600/19342, Loss: 0.1780, Acc: 92.14%\n",
      "Batch 3650/19342, Loss: 0.1781, Acc: 92.14%\n",
      "Batch 3700/19342, Loss: 0.1778, Acc: 92.14%\n",
      "Batch 3750/19342, Loss: 0.1778, Acc: 92.14%\n",
      "Batch 3800/19342, Loss: 0.1778, Acc: 92.15%\n",
      "Batch 3850/19342, Loss: 0.1777, Acc: 92.17%\n",
      "Batch 3900/19342, Loss: 0.1779, Acc: 92.15%\n",
      "Batch 3950/19342, Loss: 0.1779, Acc: 92.16%\n",
      "Batch 4000/19342, Loss: 0.1778, Acc: 92.17%\n",
      "Batch 4050/19342, Loss: 0.1776, Acc: 92.18%\n",
      "Batch 4100/19342, Loss: 0.1780, Acc: 92.15%\n",
      "Batch 4150/19342, Loss: 0.1782, Acc: 92.14%\n",
      "Batch 4200/19342, Loss: 0.1783, Acc: 92.13%\n",
      "Batch 4250/19342, Loss: 0.1785, Acc: 92.11%\n",
      "Batch 4300/19342, Loss: 0.1786, Acc: 92.10%\n",
      "Batch 4350/19342, Loss: 0.1786, Acc: 92.10%\n",
      "Batch 4400/19342, Loss: 0.1783, Acc: 92.12%\n",
      "Batch 4450/19342, Loss: 0.1787, Acc: 92.10%\n",
      "Batch 4500/19342, Loss: 0.1786, Acc: 92.09%\n",
      "Batch 4550/19342, Loss: 0.1785, Acc: 92.08%\n",
      "Batch 4600/19342, Loss: 0.1786, Acc: 92.07%\n",
      "Batch 4650/19342, Loss: 0.1787, Acc: 92.05%\n",
      "Batch 4700/19342, Loss: 0.1787, Acc: 92.06%\n",
      "Batch 4750/19342, Loss: 0.1785, Acc: 92.07%\n",
      "Batch 4800/19342, Loss: 0.1783, Acc: 92.08%\n",
      "Batch 4850/19342, Loss: 0.1783, Acc: 92.08%\n",
      "Batch 4900/19342, Loss: 0.1784, Acc: 92.07%\n",
      "Batch 4950/19342, Loss: 0.1782, Acc: 92.07%\n",
      "Batch 5000/19342, Loss: 0.1780, Acc: 92.08%\n",
      "Batch 5050/19342, Loss: 0.1778, Acc: 92.08%\n",
      "Batch 5100/19342, Loss: 0.1780, Acc: 92.07%\n",
      "Batch 5150/19342, Loss: 0.1782, Acc: 92.06%\n",
      "Batch 5200/19342, Loss: 0.1779, Acc: 92.07%\n",
      "Batch 5250/19342, Loss: 0.1782, Acc: 92.07%\n",
      "Batch 5300/19342, Loss: 0.1784, Acc: 92.06%\n",
      "Batch 5350/19342, Loss: 0.1784, Acc: 92.06%\n",
      "Batch 5400/19342, Loss: 0.1785, Acc: 92.07%\n",
      "Batch 5450/19342, Loss: 0.1787, Acc: 92.05%\n",
      "Batch 5500/19342, Loss: 0.1787, Acc: 92.05%\n",
      "Batch 5550/19342, Loss: 0.1786, Acc: 92.05%\n",
      "Batch 5600/19342, Loss: 0.1786, Acc: 92.06%\n",
      "Batch 5650/19342, Loss: 0.1787, Acc: 92.06%\n",
      "Batch 5700/19342, Loss: 0.1784, Acc: 92.07%\n",
      "Batch 5750/19342, Loss: 0.1784, Acc: 92.07%\n",
      "Batch 5800/19342, Loss: 0.1780, Acc: 92.10%\n",
      "Batch 5850/19342, Loss: 0.1779, Acc: 92.10%\n",
      "Batch 5900/19342, Loss: 0.1776, Acc: 92.11%\n",
      "Batch 5950/19342, Loss: 0.1775, Acc: 92.11%\n",
      "Batch 6000/19342, Loss: 0.1778, Acc: 92.09%\n",
      "Batch 6050/19342, Loss: 0.1778, Acc: 92.09%\n",
      "Batch 6100/19342, Loss: 0.1782, Acc: 92.07%\n",
      "Batch 6150/19342, Loss: 0.1782, Acc: 92.07%\n",
      "Batch 6200/19342, Loss: 0.1787, Acc: 92.05%\n",
      "Batch 6250/19342, Loss: 0.1787, Acc: 92.05%\n",
      "Batch 6300/19342, Loss: 0.1786, Acc: 92.05%\n",
      "Batch 6350/19342, Loss: 0.1787, Acc: 92.04%\n",
      "Batch 6400/19342, Loss: 0.1791, Acc: 92.03%\n",
      "Batch 6450/19342, Loss: 0.1790, Acc: 92.04%\n",
      "Batch 6500/19342, Loss: 0.1790, Acc: 92.04%\n",
      "Batch 6550/19342, Loss: 0.1789, Acc: 92.06%\n",
      "Batch 6600/19342, Loss: 0.1787, Acc: 92.07%\n",
      "Batch 6650/19342, Loss: 0.1786, Acc: 92.07%\n",
      "Batch 6700/19342, Loss: 0.1786, Acc: 92.06%\n",
      "Batch 6750/19342, Loss: 0.1786, Acc: 92.06%\n",
      "Batch 6800/19342, Loss: 0.1786, Acc: 92.07%\n",
      "Batch 6850/19342, Loss: 0.1783, Acc: 92.09%\n",
      "Batch 6900/19342, Loss: 0.1780, Acc: 92.10%\n",
      "Batch 6950/19342, Loss: 0.1780, Acc: 92.10%\n",
      "Batch 7000/19342, Loss: 0.1781, Acc: 92.10%\n",
      "Batch 7050/19342, Loss: 0.1780, Acc: 92.10%\n",
      "Batch 7100/19342, Loss: 0.1780, Acc: 92.11%\n",
      "Batch 7150/19342, Loss: 0.1779, Acc: 92.10%\n",
      "Batch 7200/19342, Loss: 0.1782, Acc: 92.08%\n",
      "Batch 7250/19342, Loss: 0.1786, Acc: 92.06%\n",
      "Batch 7300/19342, Loss: 0.1787, Acc: 92.06%\n",
      "Batch 7350/19342, Loss: 0.1787, Acc: 92.05%\n",
      "Batch 7400/19342, Loss: 0.1789, Acc: 92.04%\n",
      "Batch 7450/19342, Loss: 0.1789, Acc: 92.05%\n",
      "Batch 7500/19342, Loss: 0.1790, Acc: 92.04%\n",
      "Batch 7550/19342, Loss: 0.1790, Acc: 92.05%\n",
      "Batch 7600/19342, Loss: 0.1790, Acc: 92.04%\n",
      "Batch 7650/19342, Loss: 0.1791, Acc: 92.05%\n",
      "Batch 7700/19342, Loss: 0.1792, Acc: 92.04%\n",
      "Batch 7750/19342, Loss: 0.1793, Acc: 92.04%\n",
      "Batch 7800/19342, Loss: 0.1792, Acc: 92.04%\n",
      "Batch 7850/19342, Loss: 0.1791, Acc: 92.04%\n",
      "Batch 7900/19342, Loss: 0.1791, Acc: 92.05%\n",
      "Batch 7950/19342, Loss: 0.1789, Acc: 92.06%\n",
      "Batch 8000/19342, Loss: 0.1787, Acc: 92.06%\n",
      "Batch 8050/19342, Loss: 0.1788, Acc: 92.06%\n",
      "Batch 8100/19342, Loss: 0.1785, Acc: 92.08%\n",
      "Batch 8150/19342, Loss: 0.1784, Acc: 92.08%\n",
      "Batch 8200/19342, Loss: 0.1784, Acc: 92.08%\n",
      "Batch 8250/19342, Loss: 0.1786, Acc: 92.07%\n",
      "Batch 8300/19342, Loss: 0.1785, Acc: 92.07%\n",
      "Batch 8350/19342, Loss: 0.1788, Acc: 92.06%\n",
      "Batch 8400/19342, Loss: 0.1787, Acc: 92.07%\n",
      "Batch 8450/19342, Loss: 0.1786, Acc: 92.06%\n",
      "Batch 8500/19342, Loss: 0.1786, Acc: 92.06%\n",
      "Batch 8550/19342, Loss: 0.1785, Acc: 92.07%\n",
      "Batch 8600/19342, Loss: 0.1785, Acc: 92.07%\n",
      "Batch 8650/19342, Loss: 0.1786, Acc: 92.07%\n",
      "Batch 8700/19342, Loss: 0.1785, Acc: 92.08%\n",
      "Batch 8750/19342, Loss: 0.1783, Acc: 92.09%\n",
      "Batch 8800/19342, Loss: 0.1783, Acc: 92.09%\n",
      "Batch 8850/19342, Loss: 0.1788, Acc: 92.06%\n",
      "Batch 8900/19342, Loss: 0.1789, Acc: 92.05%\n",
      "Batch 8950/19342, Loss: 0.1789, Acc: 92.05%\n",
      "Batch 9000/19342, Loss: 0.1791, Acc: 92.04%\n",
      "Batch 9050/19342, Loss: 0.1791, Acc: 92.04%\n",
      "Batch 9100/19342, Loss: 0.1792, Acc: 92.03%\n",
      "Batch 9150/19342, Loss: 0.1791, Acc: 92.04%\n",
      "Batch 9200/19342, Loss: 0.1791, Acc: 92.04%\n",
      "Batch 9250/19342, Loss: 0.1791, Acc: 92.05%\n",
      "Batch 9300/19342, Loss: 0.1793, Acc: 92.03%\n",
      "Batch 9350/19342, Loss: 0.1794, Acc: 92.03%\n",
      "Batch 9400/19342, Loss: 0.1794, Acc: 92.03%\n",
      "Batch 9450/19342, Loss: 0.1793, Acc: 92.04%\n",
      "Batch 9500/19342, Loss: 0.1793, Acc: 92.04%\n",
      "Batch 9550/19342, Loss: 0.1793, Acc: 92.04%\n",
      "Batch 9600/19342, Loss: 0.1793, Acc: 92.04%\n",
      "Batch 9650/19342, Loss: 0.1794, Acc: 92.03%\n",
      "Batch 9700/19342, Loss: 0.1794, Acc: 92.03%\n",
      "Batch 9750/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 9800/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 9850/19342, Loss: 0.1793, Acc: 92.03%\n",
      "Batch 9900/19342, Loss: 0.1791, Acc: 92.04%\n",
      "Batch 9950/19342, Loss: 0.1792, Acc: 92.03%\n",
      "Batch 10000/19342, Loss: 0.1793, Acc: 92.03%\n",
      "Batch 10050/19342, Loss: 0.1794, Acc: 92.01%\n",
      "Batch 10100/19342, Loss: 0.1793, Acc: 92.02%\n",
      "Batch 10150/19342, Loss: 0.1793, Acc: 92.03%\n",
      "Batch 10200/19342, Loss: 0.1792, Acc: 92.03%\n",
      "Batch 10250/19342, Loss: 0.1794, Acc: 92.02%\n",
      "Batch 10300/19342, Loss: 0.1793, Acc: 92.03%\n",
      "Batch 10350/19342, Loss: 0.1794, Acc: 92.02%\n",
      "Batch 10400/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 10450/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 10500/19342, Loss: 0.1794, Acc: 92.02%\n",
      "Batch 10550/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 10600/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 10650/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 10700/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 10750/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 10800/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 10850/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 10900/19342, Loss: 0.1794, Acc: 92.02%\n",
      "Batch 10950/19342, Loss: 0.1794, Acc: 92.01%\n",
      "Batch 11000/19342, Loss: 0.1797, Acc: 92.00%\n",
      "Batch 11050/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 11100/19342, Loss: 0.1795, Acc: 92.00%\n",
      "Batch 11150/19342, Loss: 0.1796, Acc: 92.00%\n",
      "Batch 11200/19342, Loss: 0.1796, Acc: 92.00%\n",
      "Batch 11250/19342, Loss: 0.1796, Acc: 92.00%\n",
      "Batch 11300/19342, Loss: 0.1796, Acc: 92.00%\n",
      "Batch 11350/19342, Loss: 0.1796, Acc: 92.01%\n",
      "Batch 11400/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 11450/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 11500/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 11550/19342, Loss: 0.1796, Acc: 92.00%\n",
      "Batch 11600/19342, Loss: 0.1794, Acc: 92.01%\n",
      "Batch 11650/19342, Loss: 0.1793, Acc: 92.02%\n",
      "Batch 11700/19342, Loss: 0.1793, Acc: 92.01%\n",
      "Batch 11750/19342, Loss: 0.1794, Acc: 92.01%\n",
      "Batch 11800/19342, Loss: 0.1793, Acc: 92.01%\n",
      "Batch 11850/19342, Loss: 0.1793, Acc: 92.01%\n",
      "Batch 11900/19342, Loss: 0.1792, Acc: 92.02%\n",
      "Batch 11950/19342, Loss: 0.1793, Acc: 92.01%\n",
      "Batch 12000/19342, Loss: 0.1792, Acc: 92.01%\n",
      "Batch 12050/19342, Loss: 0.1792, Acc: 92.02%\n",
      "Batch 12100/19342, Loss: 0.1791, Acc: 92.03%\n",
      "Batch 12150/19342, Loss: 0.1791, Acc: 92.03%\n",
      "Batch 12200/19342, Loss: 0.1791, Acc: 92.02%\n",
      "Batch 12250/19342, Loss: 0.1792, Acc: 92.02%\n",
      "Batch 12300/19342, Loss: 0.1794, Acc: 92.01%\n",
      "Batch 12350/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 12400/19342, Loss: 0.1797, Acc: 91.99%\n",
      "Batch 12450/19342, Loss: 0.1796, Acc: 92.00%\n",
      "Batch 12500/19342, Loss: 0.1797, Acc: 91.99%\n",
      "Batch 12550/19342, Loss: 0.1797, Acc: 91.99%\n",
      "Batch 12600/19342, Loss: 0.1797, Acc: 92.00%\n",
      "Batch 12650/19342, Loss: 0.1797, Acc: 92.00%\n",
      "Batch 12700/19342, Loss: 0.1798, Acc: 91.99%\n",
      "Batch 12750/19342, Loss: 0.1799, Acc: 91.99%\n",
      "Batch 12800/19342, Loss: 0.1799, Acc: 91.99%\n",
      "Batch 12850/19342, Loss: 0.1798, Acc: 92.00%\n",
      "Batch 12900/19342, Loss: 0.1796, Acc: 92.01%\n",
      "Batch 12950/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 13000/19342, Loss: 0.1794, Acc: 92.02%\n",
      "Batch 13050/19342, Loss: 0.1794, Acc: 92.03%\n",
      "Batch 13100/19342, Loss: 0.1795, Acc: 92.03%\n",
      "Batch 13150/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 13200/19342, Loss: 0.1796, Acc: 92.01%\n",
      "Batch 13250/19342, Loss: 0.1797, Acc: 92.01%\n",
      "Batch 13300/19342, Loss: 0.1797, Acc: 92.01%\n",
      "Batch 13350/19342, Loss: 0.1796, Acc: 92.00%\n",
      "Batch 13400/19342, Loss: 0.1796, Acc: 92.01%\n",
      "Batch 13450/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 13500/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 13550/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 13600/19342, Loss: 0.1795, Acc: 92.02%\n",
      "Batch 13650/19342, Loss: 0.1795, Acc: 92.01%\n",
      "Batch 13700/19342, Loss: 0.1797, Acc: 92.01%\n",
      "Batch 13750/19342, Loss: 0.1797, Acc: 92.01%\n",
      "Batch 13800/19342, Loss: 0.1797, Acc: 92.01%\n",
      "Batch 13850/19342, Loss: 0.1796, Acc: 92.02%\n",
      "Batch 13900/19342, Loss: 0.1797, Acc: 92.01%\n",
      "Batch 13950/19342, Loss: 0.1797, Acc: 92.02%\n",
      "Batch 14000/19342, Loss: 0.1798, Acc: 92.01%\n",
      "Batch 14050/19342, Loss: 0.1799, Acc: 92.00%\n",
      "Batch 14100/19342, Loss: 0.1799, Acc: 92.00%\n",
      "Batch 14150/19342, Loss: 0.1801, Acc: 91.99%\n",
      "Batch 14200/19342, Loss: 0.1802, Acc: 91.99%\n",
      "Batch 14250/19342, Loss: 0.1802, Acc: 91.99%\n",
      "Batch 14300/19342, Loss: 0.1802, Acc: 91.99%\n",
      "Batch 14350/19342, Loss: 0.1801, Acc: 91.99%\n",
      "Batch 14400/19342, Loss: 0.1802, Acc: 91.98%\n",
      "Batch 14450/19342, Loss: 0.1802, Acc: 91.98%\n",
      "Batch 14500/19342, Loss: 0.1801, Acc: 91.98%\n",
      "Batch 14550/19342, Loss: 0.1800, Acc: 91.98%\n",
      "Batch 14600/19342, Loss: 0.1800, Acc: 91.98%\n",
      "Batch 14650/19342, Loss: 0.1799, Acc: 91.98%\n",
      "Batch 14700/19342, Loss: 0.1798, Acc: 91.99%\n",
      "Batch 14750/19342, Loss: 0.1798, Acc: 91.99%\n",
      "Batch 14800/19342, Loss: 0.1797, Acc: 91.99%\n",
      "Batch 14850/19342, Loss: 0.1798, Acc: 92.00%\n",
      "Batch 14900/19342, Loss: 0.1800, Acc: 91.98%\n",
      "Batch 14950/19342, Loss: 0.1800, Acc: 91.98%\n",
      "Batch 15000/19342, Loss: 0.1801, Acc: 91.98%\n",
      "Batch 15050/19342, Loss: 0.1801, Acc: 91.97%\n",
      "Batch 15100/19342, Loss: 0.1799, Acc: 91.98%\n",
      "Batch 15150/19342, Loss: 0.1799, Acc: 91.97%\n",
      "Batch 15200/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15250/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 15300/19342, Loss: 0.1800, Acc: 91.96%\n",
      "Batch 15350/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15400/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15450/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 15500/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15550/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15600/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15650/19342, Loss: 0.1798, Acc: 91.96%\n",
      "Batch 15700/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15750/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 15800/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15850/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 15900/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 15950/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 16000/19342, Loss: 0.1797, Acc: 91.97%\n",
      "Batch 16050/19342, Loss: 0.1797, Acc: 91.97%\n",
      "Batch 16100/19342, Loss: 0.1797, Acc: 91.97%\n",
      "Batch 16150/19342, Loss: 0.1796, Acc: 91.98%\n",
      "Batch 16200/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16250/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16300/19342, Loss: 0.1797, Acc: 91.97%\n",
      "Batch 16350/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16400/19342, Loss: 0.1795, Acc: 91.98%\n",
      "Batch 16450/19342, Loss: 0.1795, Acc: 91.99%\n",
      "Batch 16500/19342, Loss: 0.1795, Acc: 91.99%\n",
      "Batch 16550/19342, Loss: 0.1796, Acc: 91.99%\n",
      "Batch 16600/19342, Loss: 0.1796, Acc: 91.99%\n",
      "Batch 16650/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16700/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16750/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16800/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16850/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16900/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 16950/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 17000/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 17050/19342, Loss: 0.1797, Acc: 91.98%\n",
      "Batch 17100/19342, Loss: 0.1798, Acc: 91.98%\n",
      "Batch 17150/19342, Loss: 0.1798, Acc: 91.97%\n",
      "Batch 17200/19342, Loss: 0.1800, Acc: 91.96%\n",
      "Batch 17250/19342, Loss: 0.1800, Acc: 91.96%\n",
      "Batch 17300/19342, Loss: 0.1800, Acc: 91.96%\n",
      "Batch 17350/19342, Loss: 0.1800, Acc: 91.96%\n",
      "Batch 17400/19342, Loss: 0.1800, Acc: 91.96%\n",
      "Batch 17450/19342, Loss: 0.1799, Acc: 91.96%\n",
      "Batch 17500/19342, Loss: 0.1800, Acc: 91.96%\n",
      "Batch 17550/19342, Loss: 0.1800, Acc: 91.95%\n",
      "Batch 17600/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 17650/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 17700/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 17750/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 17800/19342, Loss: 0.1803, Acc: 91.94%\n",
      "Batch 17850/19342, Loss: 0.1803, Acc: 91.94%\n",
      "Batch 17900/19342, Loss: 0.1802, Acc: 91.94%\n",
      "Batch 17950/19342, Loss: 0.1802, Acc: 91.94%\n",
      "Batch 18000/19342, Loss: 0.1802, Acc: 91.95%\n",
      "Batch 18050/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 18100/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 18150/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 18200/19342, Loss: 0.1801, Acc: 91.95%\n",
      "Batch 18250/19342, Loss: 0.1800, Acc: 91.95%\n",
      "Batch 18300/19342, Loss: 0.1800, Acc: 91.95%\n",
      "Batch 18350/19342, Loss: 0.1800, Acc: 91.95%\n",
      "Batch 18400/19342, Loss: 0.1800, Acc: 91.94%\n",
      "Batch 18450/19342, Loss: 0.1801, Acc: 91.94%\n",
      "Batch 18500/19342, Loss: 0.1800, Acc: 91.94%\n",
      "Batch 18550/19342, Loss: 0.1799, Acc: 91.95%\n",
      "Batch 18600/19342, Loss: 0.1799, Acc: 91.94%\n",
      "Batch 18650/19342, Loss: 0.1801, Acc: 91.94%\n",
      "Batch 18700/19342, Loss: 0.1801, Acc: 91.94%\n",
      "Batch 18750/19342, Loss: 0.1801, Acc: 91.94%\n",
      "Batch 18800/19342, Loss: 0.1801, Acc: 91.94%\n",
      "Batch 18850/19342, Loss: 0.1801, Acc: 91.93%\n",
      "Batch 18900/19342, Loss: 0.1801, Acc: 91.93%\n",
      "Batch 18950/19342, Loss: 0.1801, Acc: 91.93%\n",
      "Batch 19000/19342, Loss: 0.1800, Acc: 91.93%\n",
      "Batch 19050/19342, Loss: 0.1802, Acc: 91.92%\n",
      "Batch 19100/19342, Loss: 0.1800, Acc: 91.93%\n",
      "Batch 19150/19342, Loss: 0.1801, Acc: 91.92%\n",
      "Batch 19200/19342, Loss: 0.1801, Acc: 91.92%\n",
      "Batch 19250/19342, Loss: 0.1802, Acc: 91.92%\n",
      "Batch 19300/19342, Loss: 0.1802, Acc: 91.92%\n",
      "Train Loss: 0.1801, Train Acc: 91.92%\n",
      "Val Loss: 0.3565, Val Acc: 83.41%\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\n",
      "Epoch 7/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.1936, Acc: 90.25%\n",
      "Batch 100/19342, Loss: 0.1902, Acc: 91.00%\n",
      "Batch 150/19342, Loss: 0.1815, Acc: 91.58%\n",
      "Batch 200/19342, Loss: 0.1800, Acc: 91.56%\n",
      "Batch 250/19342, Loss: 0.1725, Acc: 91.85%\n",
      "Batch 300/19342, Loss: 0.1690, Acc: 92.25%\n",
      "Batch 350/19342, Loss: 0.1757, Acc: 91.86%\n",
      "Batch 400/19342, Loss: 0.1773, Acc: 91.88%\n",
      "Batch 450/19342, Loss: 0.1731, Acc: 92.00%\n",
      "Batch 500/19342, Loss: 0.1731, Acc: 92.20%\n",
      "Batch 550/19342, Loss: 0.1710, Acc: 92.30%\n",
      "Batch 600/19342, Loss: 0.1687, Acc: 92.40%\n",
      "Batch 650/19342, Loss: 0.1677, Acc: 92.46%\n",
      "Batch 700/19342, Loss: 0.1681, Acc: 92.54%\n",
      "Batch 750/19342, Loss: 0.1675, Acc: 92.58%\n",
      "Batch 800/19342, Loss: 0.1680, Acc: 92.58%\n",
      "Batch 850/19342, Loss: 0.1662, Acc: 92.60%\n",
      "Batch 900/19342, Loss: 0.1673, Acc: 92.53%\n",
      "Batch 950/19342, Loss: 0.1677, Acc: 92.55%\n",
      "Batch 1000/19342, Loss: 0.1671, Acc: 92.50%\n",
      "Batch 1050/19342, Loss: 0.1682, Acc: 92.45%\n",
      "Batch 1100/19342, Loss: 0.1710, Acc: 92.40%\n",
      "Batch 1150/19342, Loss: 0.1725, Acc: 92.30%\n",
      "Batch 1200/19342, Loss: 0.1712, Acc: 92.36%\n",
      "Batch 1250/19342, Loss: 0.1717, Acc: 92.33%\n",
      "Batch 1300/19342, Loss: 0.1719, Acc: 92.32%\n",
      "Batch 1350/19342, Loss: 0.1726, Acc: 92.28%\n",
      "Batch 1400/19342, Loss: 0.1730, Acc: 92.20%\n",
      "Batch 1450/19342, Loss: 0.1738, Acc: 92.16%\n",
      "Batch 1500/19342, Loss: 0.1738, Acc: 92.10%\n",
      "Batch 1550/19342, Loss: 0.1734, Acc: 92.15%\n",
      "Batch 1600/19342, Loss: 0.1724, Acc: 92.20%\n",
      "Batch 1650/19342, Loss: 0.1714, Acc: 92.26%\n",
      "Batch 1700/19342, Loss: 0.1723, Acc: 92.21%\n",
      "Batch 1750/19342, Loss: 0.1719, Acc: 92.23%\n",
      "Batch 1800/19342, Loss: 0.1712, Acc: 92.29%\n",
      "Batch 1850/19342, Loss: 0.1708, Acc: 92.30%\n",
      "Batch 1900/19342, Loss: 0.1703, Acc: 92.35%\n",
      "Batch 1950/19342, Loss: 0.1700, Acc: 92.36%\n",
      "Batch 2000/19342, Loss: 0.1704, Acc: 92.36%\n",
      "Batch 2050/19342, Loss: 0.1702, Acc: 92.38%\n",
      "Batch 2100/19342, Loss: 0.1703, Acc: 92.38%\n",
      "Batch 2150/19342, Loss: 0.1705, Acc: 92.36%\n",
      "Batch 2200/19342, Loss: 0.1701, Acc: 92.36%\n",
      "Batch 2250/19342, Loss: 0.1702, Acc: 92.37%\n",
      "Batch 2300/19342, Loss: 0.1710, Acc: 92.32%\n",
      "Batch 2350/19342, Loss: 0.1720, Acc: 92.28%\n",
      "Batch 2400/19342, Loss: 0.1718, Acc: 92.32%\n",
      "Batch 2450/19342, Loss: 0.1714, Acc: 92.35%\n",
      "Batch 2500/19342, Loss: 0.1702, Acc: 92.42%\n",
      "Batch 2550/19342, Loss: 0.1697, Acc: 92.43%\n",
      "Batch 2600/19342, Loss: 0.1698, Acc: 92.41%\n",
      "Batch 2650/19342, Loss: 0.1703, Acc: 92.40%\n",
      "Batch 2700/19342, Loss: 0.1707, Acc: 92.38%\n",
      "Batch 2750/19342, Loss: 0.1708, Acc: 92.38%\n",
      "Batch 2800/19342, Loss: 0.1710, Acc: 92.37%\n",
      "Batch 2850/19342, Loss: 0.1711, Acc: 92.36%\n",
      "Batch 2900/19342, Loss: 0.1706, Acc: 92.36%\n",
      "Batch 2950/19342, Loss: 0.1712, Acc: 92.34%\n",
      "Batch 3000/19342, Loss: 0.1715, Acc: 92.31%\n",
      "Batch 3050/19342, Loss: 0.1715, Acc: 92.31%\n",
      "Batch 3100/19342, Loss: 0.1711, Acc: 92.32%\n",
      "Batch 3150/19342, Loss: 0.1708, Acc: 92.34%\n",
      "Batch 3200/19342, Loss: 0.1705, Acc: 92.34%\n",
      "Batch 3250/19342, Loss: 0.1703, Acc: 92.36%\n",
      "Batch 3300/19342, Loss: 0.1708, Acc: 92.34%\n",
      "Batch 3350/19342, Loss: 0.1710, Acc: 92.33%\n",
      "Batch 3400/19342, Loss: 0.1719, Acc: 92.30%\n",
      "Batch 3450/19342, Loss: 0.1723, Acc: 92.29%\n",
      "Batch 3500/19342, Loss: 0.1723, Acc: 92.30%\n",
      "Batch 3550/19342, Loss: 0.1723, Acc: 92.29%\n",
      "Batch 3600/19342, Loss: 0.1720, Acc: 92.31%\n",
      "Batch 3650/19342, Loss: 0.1716, Acc: 92.34%\n",
      "Batch 3700/19342, Loss: 0.1711, Acc: 92.36%\n",
      "Batch 3750/19342, Loss: 0.1708, Acc: 92.37%\n",
      "Batch 3800/19342, Loss: 0.1703, Acc: 92.38%\n",
      "Batch 3850/19342, Loss: 0.1700, Acc: 92.39%\n",
      "Batch 3900/19342, Loss: 0.1702, Acc: 92.37%\n",
      "Batch 3950/19342, Loss: 0.1701, Acc: 92.39%\n",
      "Batch 4000/19342, Loss: 0.1695, Acc: 92.42%\n",
      "Batch 4050/19342, Loss: 0.1696, Acc: 92.42%\n",
      "Batch 4100/19342, Loss: 0.1701, Acc: 92.40%\n",
      "Batch 4150/19342, Loss: 0.1702, Acc: 92.40%\n",
      "Batch 4200/19342, Loss: 0.1703, Acc: 92.42%\n",
      "Batch 4250/19342, Loss: 0.1705, Acc: 92.40%\n",
      "Batch 4300/19342, Loss: 0.1709, Acc: 92.39%\n",
      "Batch 4350/19342, Loss: 0.1708, Acc: 92.39%\n",
      "Batch 4400/19342, Loss: 0.1706, Acc: 92.42%\n",
      "Batch 4450/19342, Loss: 0.1707, Acc: 92.42%\n",
      "Batch 4500/19342, Loss: 0.1707, Acc: 92.42%\n",
      "Batch 4550/19342, Loss: 0.1706, Acc: 92.43%\n",
      "Batch 4600/19342, Loss: 0.1709, Acc: 92.42%\n",
      "Batch 4650/19342, Loss: 0.1709, Acc: 92.42%\n",
      "Batch 4700/19342, Loss: 0.1709, Acc: 92.41%\n",
      "Batch 4750/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 4800/19342, Loss: 0.1712, Acc: 92.41%\n",
      "Batch 4850/19342, Loss: 0.1709, Acc: 92.42%\n",
      "Batch 4900/19342, Loss: 0.1710, Acc: 92.43%\n",
      "Batch 4950/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 5000/19342, Loss: 0.1715, Acc: 92.41%\n",
      "Batch 5050/19342, Loss: 0.1713, Acc: 92.42%\n",
      "Batch 5100/19342, Loss: 0.1717, Acc: 92.40%\n",
      "Batch 5150/19342, Loss: 0.1721, Acc: 92.39%\n",
      "Batch 5200/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 5250/19342, Loss: 0.1715, Acc: 92.41%\n",
      "Batch 5300/19342, Loss: 0.1716, Acc: 92.40%\n",
      "Batch 5350/19342, Loss: 0.1718, Acc: 92.39%\n",
      "Batch 5400/19342, Loss: 0.1719, Acc: 92.38%\n",
      "Batch 5450/19342, Loss: 0.1719, Acc: 92.38%\n",
      "Batch 5500/19342, Loss: 0.1715, Acc: 92.40%\n",
      "Batch 5550/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 5600/19342, Loss: 0.1715, Acc: 92.40%\n",
      "Batch 5650/19342, Loss: 0.1714, Acc: 92.40%\n",
      "Batch 5700/19342, Loss: 0.1713, Acc: 92.41%\n",
      "Batch 5750/19342, Loss: 0.1716, Acc: 92.38%\n",
      "Batch 5800/19342, Loss: 0.1718, Acc: 92.39%\n",
      "Batch 5850/19342, Loss: 0.1719, Acc: 92.37%\n",
      "Batch 5900/19342, Loss: 0.1718, Acc: 92.38%\n",
      "Batch 5950/19342, Loss: 0.1716, Acc: 92.38%\n",
      "Batch 6000/19342, Loss: 0.1717, Acc: 92.38%\n",
      "Batch 6050/19342, Loss: 0.1713, Acc: 92.39%\n",
      "Batch 6100/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 6150/19342, Loss: 0.1713, Acc: 92.38%\n",
      "Batch 6200/19342, Loss: 0.1711, Acc: 92.39%\n",
      "Batch 6250/19342, Loss: 0.1711, Acc: 92.39%\n",
      "Batch 6300/19342, Loss: 0.1710, Acc: 92.39%\n",
      "Batch 6350/19342, Loss: 0.1709, Acc: 92.39%\n",
      "Batch 6400/19342, Loss: 0.1708, Acc: 92.40%\n",
      "Batch 6450/19342, Loss: 0.1708, Acc: 92.40%\n",
      "Batch 6500/19342, Loss: 0.1709, Acc: 92.39%\n",
      "Batch 6550/19342, Loss: 0.1712, Acc: 92.38%\n",
      "Batch 6600/19342, Loss: 0.1712, Acc: 92.37%\n",
      "Batch 6650/19342, Loss: 0.1711, Acc: 92.37%\n",
      "Batch 6700/19342, Loss: 0.1712, Acc: 92.36%\n",
      "Batch 6750/19342, Loss: 0.1710, Acc: 92.37%\n",
      "Batch 6800/19342, Loss: 0.1710, Acc: 92.38%\n",
      "Batch 6850/19342, Loss: 0.1708, Acc: 92.40%\n",
      "Batch 6900/19342, Loss: 0.1711, Acc: 92.39%\n",
      "Batch 6950/19342, Loss: 0.1707, Acc: 92.41%\n",
      "Batch 7000/19342, Loss: 0.1707, Acc: 92.41%\n",
      "Batch 7050/19342, Loss: 0.1706, Acc: 92.42%\n",
      "Batch 7100/19342, Loss: 0.1708, Acc: 92.42%\n",
      "Batch 7150/19342, Loss: 0.1707, Acc: 92.42%\n",
      "Batch 7200/19342, Loss: 0.1708, Acc: 92.42%\n",
      "Batch 7250/19342, Loss: 0.1709, Acc: 92.42%\n",
      "Batch 7300/19342, Loss: 0.1709, Acc: 92.40%\n",
      "Batch 7350/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 7400/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 7450/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 7500/19342, Loss: 0.1709, Acc: 92.41%\n",
      "Batch 7550/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 7600/19342, Loss: 0.1715, Acc: 92.38%\n",
      "Batch 7650/19342, Loss: 0.1714, Acc: 92.38%\n",
      "Batch 7700/19342, Loss: 0.1713, Acc: 92.38%\n",
      "Batch 7750/19342, Loss: 0.1711, Acc: 92.39%\n",
      "Batch 7800/19342, Loss: 0.1710, Acc: 92.40%\n",
      "Batch 7850/19342, Loss: 0.1710, Acc: 92.40%\n",
      "Batch 7900/19342, Loss: 0.1709, Acc: 92.40%\n",
      "Batch 7950/19342, Loss: 0.1706, Acc: 92.41%\n",
      "Batch 8000/19342, Loss: 0.1706, Acc: 92.41%\n",
      "Batch 8050/19342, Loss: 0.1705, Acc: 92.42%\n",
      "Batch 8100/19342, Loss: 0.1705, Acc: 92.42%\n",
      "Batch 8150/19342, Loss: 0.1705, Acc: 92.42%\n",
      "Batch 8200/19342, Loss: 0.1704, Acc: 92.44%\n",
      "Batch 8250/19342, Loss: 0.1705, Acc: 92.43%\n",
      "Batch 8300/19342, Loss: 0.1707, Acc: 92.42%\n",
      "Batch 8350/19342, Loss: 0.1706, Acc: 92.42%\n",
      "Batch 8400/19342, Loss: 0.1707, Acc: 92.42%\n",
      "Batch 8450/19342, Loss: 0.1708, Acc: 92.42%\n",
      "Batch 8500/19342, Loss: 0.1708, Acc: 92.42%\n",
      "Batch 8550/19342, Loss: 0.1708, Acc: 92.43%\n",
      "Batch 8600/19342, Loss: 0.1712, Acc: 92.41%\n",
      "Batch 8650/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 8700/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 8750/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 8800/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 8850/19342, Loss: 0.1710, Acc: 92.41%\n",
      "Batch 8900/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 8950/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 9000/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 9050/19342, Loss: 0.1710, Acc: 92.42%\n",
      "Batch 9100/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 9150/19342, Loss: 0.1709, Acc: 92.43%\n",
      "Batch 9200/19342, Loss: 0.1708, Acc: 92.43%\n",
      "Batch 9250/19342, Loss: 0.1709, Acc: 92.43%\n",
      "Batch 9300/19342, Loss: 0.1708, Acc: 92.43%\n",
      "Batch 9350/19342, Loss: 0.1707, Acc: 92.44%\n",
      "Batch 9400/19342, Loss: 0.1709, Acc: 92.43%\n",
      "Batch 9450/19342, Loss: 0.1709, Acc: 92.43%\n",
      "Batch 9500/19342, Loss: 0.1710, Acc: 92.41%\n",
      "Batch 9550/19342, Loss: 0.1709, Acc: 92.41%\n",
      "Batch 9600/19342, Loss: 0.1712, Acc: 92.40%\n",
      "Batch 9650/19342, Loss: 0.1713, Acc: 92.40%\n",
      "Batch 9700/19342, Loss: 0.1714, Acc: 92.39%\n",
      "Batch 9750/19342, Loss: 0.1713, Acc: 92.40%\n",
      "Batch 9800/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 9850/19342, Loss: 0.1710, Acc: 92.42%\n",
      "Batch 9900/19342, Loss: 0.1709, Acc: 92.43%\n",
      "Batch 9950/19342, Loss: 0.1709, Acc: 92.42%\n",
      "Batch 10000/19342, Loss: 0.1709, Acc: 92.42%\n",
      "Batch 10050/19342, Loss: 0.1708, Acc: 92.42%\n",
      "Batch 10100/19342, Loss: 0.1709, Acc: 92.41%\n",
      "Batch 10150/19342, Loss: 0.1711, Acc: 92.40%\n",
      "Batch 10200/19342, Loss: 0.1711, Acc: 92.40%\n",
      "Batch 10250/19342, Loss: 0.1711, Acc: 92.40%\n",
      "Batch 10300/19342, Loss: 0.1710, Acc: 92.41%\n",
      "Batch 10350/19342, Loss: 0.1711, Acc: 92.40%\n",
      "Batch 10400/19342, Loss: 0.1711, Acc: 92.40%\n",
      "Batch 10450/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 10500/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 10550/19342, Loss: 0.1711, Acc: 92.41%\n",
      "Batch 10600/19342, Loss: 0.1713, Acc: 92.40%\n",
      "Batch 10650/19342, Loss: 0.1713, Acc: 92.40%\n",
      "Batch 10700/19342, Loss: 0.1713, Acc: 92.40%\n",
      "Batch 10750/19342, Loss: 0.1714, Acc: 92.40%\n",
      "Batch 10800/19342, Loss: 0.1713, Acc: 92.40%\n",
      "Batch 10850/19342, Loss: 0.1713, Acc: 92.40%\n",
      "Batch 10900/19342, Loss: 0.1711, Acc: 92.42%\n",
      "Batch 10950/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 11000/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 11050/19342, Loss: 0.1711, Acc: 92.43%\n",
      "Batch 11100/19342, Loss: 0.1710, Acc: 92.43%\n",
      "Batch 11150/19342, Loss: 0.1711, Acc: 92.43%\n",
      "Batch 11200/19342, Loss: 0.1710, Acc: 92.44%\n",
      "Batch 11250/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 11300/19342, Loss: 0.1710, Acc: 92.45%\n",
      "Batch 11350/19342, Loss: 0.1710, Acc: 92.46%\n",
      "Batch 11400/19342, Loss: 0.1710, Acc: 92.46%\n",
      "Batch 11450/19342, Loss: 0.1710, Acc: 92.46%\n",
      "Batch 11500/19342, Loss: 0.1710, Acc: 92.46%\n",
      "Batch 11550/19342, Loss: 0.1710, Acc: 92.46%\n",
      "Batch 11600/19342, Loss: 0.1712, Acc: 92.46%\n",
      "Batch 11650/19342, Loss: 0.1712, Acc: 92.46%\n",
      "Batch 11700/19342, Loss: 0.1712, Acc: 92.46%\n",
      "Batch 11750/19342, Loss: 0.1712, Acc: 92.45%\n",
      "Batch 11800/19342, Loss: 0.1712, Acc: 92.46%\n",
      "Batch 11850/19342, Loss: 0.1712, Acc: 92.46%\n",
      "Batch 11900/19342, Loss: 0.1712, Acc: 92.45%\n",
      "Batch 11950/19342, Loss: 0.1713, Acc: 92.45%\n",
      "Batch 12000/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 12050/19342, Loss: 0.1714, Acc: 92.44%\n",
      "Batch 12100/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 12150/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 12200/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 12250/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 12300/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 12350/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 12400/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 12450/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 12500/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 12550/19342, Loss: 0.1711, Acc: 92.45%\n",
      "Batch 12600/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 12650/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 12700/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 12750/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 12800/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 12850/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 12900/19342, Loss: 0.1712, Acc: 92.45%\n",
      "Batch 12950/19342, Loss: 0.1712, Acc: 92.45%\n",
      "Batch 13000/19342, Loss: 0.1713, Acc: 92.45%\n",
      "Batch 13050/19342, Loss: 0.1713, Acc: 92.45%\n",
      "Batch 13100/19342, Loss: 0.1714, Acc: 92.44%\n",
      "Batch 13150/19342, Loss: 0.1714, Acc: 92.44%\n",
      "Batch 13200/19342, Loss: 0.1713, Acc: 92.45%\n",
      "Batch 13250/19342, Loss: 0.1715, Acc: 92.44%\n",
      "Batch 13300/19342, Loss: 0.1715, Acc: 92.44%\n",
      "Batch 13350/19342, Loss: 0.1714, Acc: 92.44%\n",
      "Batch 13400/19342, Loss: 0.1714, Acc: 92.44%\n",
      "Batch 13450/19342, Loss: 0.1714, Acc: 92.43%\n",
      "Batch 13500/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 13550/19342, Loss: 0.1714, Acc: 92.43%\n",
      "Batch 13600/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 13650/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 13700/19342, Loss: 0.1714, Acc: 92.43%\n",
      "Batch 13750/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 13800/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 13850/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 13900/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 13950/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 14000/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 14050/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 14100/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 14150/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 14200/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 14250/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 14300/19342, Loss: 0.1712, Acc: 92.44%\n",
      "Batch 14350/19342, Loss: 0.1713, Acc: 92.44%\n",
      "Batch 14400/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 14450/19342, Loss: 0.1711, Acc: 92.45%\n",
      "Batch 14500/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 14550/19342, Loss: 0.1710, Acc: 92.45%\n",
      "Batch 14600/19342, Loss: 0.1710, Acc: 92.45%\n",
      "Batch 14650/19342, Loss: 0.1710, Acc: 92.45%\n",
      "Batch 14700/19342, Loss: 0.1711, Acc: 92.45%\n",
      "Batch 14750/19342, Loss: 0.1710, Acc: 92.46%\n",
      "Batch 14800/19342, Loss: 0.1709, Acc: 92.46%\n",
      "Batch 14850/19342, Loss: 0.1710, Acc: 92.46%\n",
      "Batch 14900/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 14950/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 15000/19342, Loss: 0.1709, Acc: 92.46%\n",
      "Batch 15050/19342, Loss: 0.1708, Acc: 92.46%\n",
      "Batch 15100/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 15150/19342, Loss: 0.1708, Acc: 92.45%\n",
      "Batch 15200/19342, Loss: 0.1708, Acc: 92.45%\n",
      "Batch 15250/19342, Loss: 0.1708, Acc: 92.45%\n",
      "Batch 15300/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 15350/19342, Loss: 0.1708, Acc: 92.45%\n",
      "Batch 15400/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 15450/19342, Loss: 0.1709, Acc: 92.44%\n",
      "Batch 15500/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 15550/19342, Loss: 0.1708, Acc: 92.45%\n",
      "Batch 15600/19342, Loss: 0.1708, Acc: 92.45%\n",
      "Batch 15650/19342, Loss: 0.1708, Acc: 92.46%\n",
      "Batch 15700/19342, Loss: 0.1708, Acc: 92.45%\n",
      "Batch 15750/19342, Loss: 0.1709, Acc: 92.45%\n",
      "Batch 15800/19342, Loss: 0.1710, Acc: 92.44%\n",
      "Batch 15850/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 15900/19342, Loss: 0.1711, Acc: 92.43%\n",
      "Batch 15950/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 16000/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 16050/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 16100/19342, Loss: 0.1713, Acc: 92.42%\n",
      "Batch 16150/19342, Loss: 0.1714, Acc: 92.42%\n",
      "Batch 16200/19342, Loss: 0.1714, Acc: 92.42%\n",
      "Batch 16250/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 16300/19342, Loss: 0.1713, Acc: 92.42%\n",
      "Batch 16350/19342, Loss: 0.1713, Acc: 92.43%\n",
      "Batch 16400/19342, Loss: 0.1713, Acc: 92.42%\n",
      "Batch 16450/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 16500/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 16550/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 16600/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 16650/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 16700/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 16750/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 16800/19342, Loss: 0.1710, Acc: 92.44%\n",
      "Batch 16850/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 16900/19342, Loss: 0.1712, Acc: 92.43%\n",
      "Batch 16950/19342, Loss: 0.1711, Acc: 92.44%\n",
      "Batch 17000/19342, Loss: 0.1710, Acc: 92.44%\n",
      "Batch 17050/19342, Loss: 0.1711, Acc: 92.43%\n",
      "Batch 17100/19342, Loss: 0.1712, Acc: 92.42%\n",
      "Batch 17150/19342, Loss: 0.1714, Acc: 92.42%\n",
      "Batch 17200/19342, Loss: 0.1714, Acc: 92.42%\n",
      "Batch 17250/19342, Loss: 0.1714, Acc: 92.42%\n",
      "Batch 17300/19342, Loss: 0.1714, Acc: 92.42%\n",
      "Batch 17350/19342, Loss: 0.1716, Acc: 92.41%\n",
      "Batch 17400/19342, Loss: 0.1717, Acc: 92.41%\n",
      "Batch 17450/19342, Loss: 0.1717, Acc: 92.41%\n",
      "Batch 17500/19342, Loss: 0.1717, Acc: 92.41%\n",
      "Batch 17550/19342, Loss: 0.1717, Acc: 92.41%\n",
      "Batch 17600/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 17650/19342, Loss: 0.1719, Acc: 92.40%\n",
      "Batch 17700/19342, Loss: 0.1720, Acc: 92.39%\n",
      "Batch 17750/19342, Loss: 0.1719, Acc: 92.40%\n",
      "Batch 17800/19342, Loss: 0.1719, Acc: 92.40%\n",
      "Batch 17850/19342, Loss: 0.1718, Acc: 92.41%\n",
      "Batch 17900/19342, Loss: 0.1718, Acc: 92.41%\n",
      "Batch 17950/19342, Loss: 0.1718, Acc: 92.41%\n",
      "Batch 18000/19342, Loss: 0.1717, Acc: 92.41%\n",
      "Batch 18050/19342, Loss: 0.1718, Acc: 92.41%\n",
      "Batch 18100/19342, Loss: 0.1717, Acc: 92.41%\n",
      "Batch 18150/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18200/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18250/19342, Loss: 0.1720, Acc: 92.40%\n",
      "Batch 18300/19342, Loss: 0.1720, Acc: 92.39%\n",
      "Batch 18350/19342, Loss: 0.1720, Acc: 92.39%\n",
      "Batch 18400/19342, Loss: 0.1719, Acc: 92.39%\n",
      "Batch 18450/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18500/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18550/19342, Loss: 0.1719, Acc: 92.40%\n",
      "Batch 18600/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18650/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18700/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18750/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18800/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18850/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18900/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 18950/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 19000/19342, Loss: 0.1719, Acc: 92.40%\n",
      "Batch 19050/19342, Loss: 0.1720, Acc: 92.40%\n",
      "Batch 19100/19342, Loss: 0.1720, Acc: 92.40%\n",
      "Batch 19150/19342, Loss: 0.1719, Acc: 92.40%\n",
      "Batch 19200/19342, Loss: 0.1718, Acc: 92.40%\n",
      "Batch 19250/19342, Loss: 0.1718, Acc: 92.41%\n",
      "Batch 19300/19342, Loss: 0.1717, Acc: 92.41%\n",
      "Train Loss: 0.1718, Train Acc: 92.41%\n",
      "Val Loss: 0.3235, Val Acc: 85.92%\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\n",
      "Epoch 8/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.1369, Acc: 93.75%\n",
      "Batch 100/19342, Loss: 0.1669, Acc: 92.00%\n",
      "Batch 150/19342, Loss: 0.1694, Acc: 92.00%\n",
      "Batch 200/19342, Loss: 0.1740, Acc: 91.69%\n",
      "Batch 250/19342, Loss: 0.1759, Acc: 91.65%\n",
      "Batch 300/19342, Loss: 0.1757, Acc: 91.79%\n",
      "Batch 350/19342, Loss: 0.1721, Acc: 91.96%\n",
      "Batch 400/19342, Loss: 0.1645, Acc: 92.34%\n",
      "Batch 450/19342, Loss: 0.1624, Acc: 92.42%\n",
      "Batch 500/19342, Loss: 0.1642, Acc: 92.35%\n",
      "Batch 550/19342, Loss: 0.1663, Acc: 92.27%\n",
      "Batch 600/19342, Loss: 0.1670, Acc: 92.19%\n",
      "Batch 650/19342, Loss: 0.1645, Acc: 92.27%\n",
      "Batch 700/19342, Loss: 0.1639, Acc: 92.34%\n",
      "Batch 750/19342, Loss: 0.1645, Acc: 92.35%\n",
      "Batch 800/19342, Loss: 0.1641, Acc: 92.44%\n",
      "Batch 850/19342, Loss: 0.1637, Acc: 92.50%\n",
      "Batch 900/19342, Loss: 0.1634, Acc: 92.54%\n",
      "Batch 950/19342, Loss: 0.1640, Acc: 92.50%\n",
      "Batch 1000/19342, Loss: 0.1654, Acc: 92.42%\n",
      "Batch 1050/19342, Loss: 0.1639, Acc: 92.56%\n",
      "Batch 1100/19342, Loss: 0.1628, Acc: 92.59%\n",
      "Batch 1150/19342, Loss: 0.1628, Acc: 92.67%\n",
      "Batch 1200/19342, Loss: 0.1616, Acc: 92.76%\n",
      "Batch 1250/19342, Loss: 0.1606, Acc: 92.88%\n",
      "Batch 1300/19342, Loss: 0.1596, Acc: 92.89%\n",
      "Batch 1350/19342, Loss: 0.1613, Acc: 92.87%\n",
      "Batch 1400/19342, Loss: 0.1604, Acc: 92.93%\n",
      "Batch 1450/19342, Loss: 0.1593, Acc: 93.00%\n",
      "Batch 1500/19342, Loss: 0.1605, Acc: 92.95%\n",
      "Batch 1550/19342, Loss: 0.1600, Acc: 93.00%\n",
      "Batch 1600/19342, Loss: 0.1605, Acc: 92.98%\n",
      "Batch 1650/19342, Loss: 0.1609, Acc: 92.94%\n",
      "Batch 1700/19342, Loss: 0.1607, Acc: 92.96%\n",
      "Batch 1750/19342, Loss: 0.1606, Acc: 92.99%\n",
      "Batch 1800/19342, Loss: 0.1605, Acc: 92.99%\n",
      "Batch 1850/19342, Loss: 0.1606, Acc: 92.99%\n",
      "Batch 1900/19342, Loss: 0.1613, Acc: 92.95%\n",
      "Batch 1950/19342, Loss: 0.1617, Acc: 92.93%\n",
      "Batch 2000/19342, Loss: 0.1621, Acc: 92.92%\n",
      "Batch 2050/19342, Loss: 0.1621, Acc: 92.91%\n",
      "Batch 2100/19342, Loss: 0.1619, Acc: 92.95%\n",
      "Batch 2150/19342, Loss: 0.1624, Acc: 92.90%\n",
      "Batch 2200/19342, Loss: 0.1632, Acc: 92.86%\n",
      "Batch 2250/19342, Loss: 0.1631, Acc: 92.86%\n",
      "Batch 2300/19342, Loss: 0.1627, Acc: 92.89%\n",
      "Batch 2350/19342, Loss: 0.1621, Acc: 92.89%\n",
      "Batch 2400/19342, Loss: 0.1624, Acc: 92.85%\n",
      "Batch 2450/19342, Loss: 0.1621, Acc: 92.88%\n",
      "Batch 2500/19342, Loss: 0.1625, Acc: 92.86%\n",
      "Batch 2550/19342, Loss: 0.1623, Acc: 92.85%\n",
      "Batch 2600/19342, Loss: 0.1626, Acc: 92.82%\n",
      "Batch 2650/19342, Loss: 0.1630, Acc: 92.81%\n",
      "Batch 2700/19342, Loss: 0.1624, Acc: 92.84%\n",
      "Batch 2750/19342, Loss: 0.1624, Acc: 92.86%\n",
      "Batch 2800/19342, Loss: 0.1621, Acc: 92.88%\n",
      "Batch 2850/19342, Loss: 0.1628, Acc: 92.85%\n",
      "Batch 2900/19342, Loss: 0.1633, Acc: 92.82%\n",
      "Batch 2950/19342, Loss: 0.1634, Acc: 92.84%\n",
      "Batch 3000/19342, Loss: 0.1635, Acc: 92.83%\n",
      "Batch 3050/19342, Loss: 0.1632, Acc: 92.86%\n",
      "Batch 3100/19342, Loss: 0.1628, Acc: 92.88%\n",
      "Batch 3150/19342, Loss: 0.1633, Acc: 92.86%\n",
      "Batch 3200/19342, Loss: 0.1635, Acc: 92.85%\n",
      "Batch 3250/19342, Loss: 0.1639, Acc: 92.81%\n",
      "Batch 3300/19342, Loss: 0.1639, Acc: 92.81%\n",
      "Batch 3350/19342, Loss: 0.1633, Acc: 92.83%\n",
      "Batch 3400/19342, Loss: 0.1630, Acc: 92.84%\n",
      "Batch 3450/19342, Loss: 0.1632, Acc: 92.82%\n",
      "Batch 3500/19342, Loss: 0.1636, Acc: 92.79%\n",
      "Batch 3550/19342, Loss: 0.1633, Acc: 92.81%\n",
      "Batch 3600/19342, Loss: 0.1634, Acc: 92.81%\n",
      "Batch 3650/19342, Loss: 0.1632, Acc: 92.82%\n",
      "Batch 3700/19342, Loss: 0.1631, Acc: 92.80%\n",
      "Batch 3750/19342, Loss: 0.1626, Acc: 92.82%\n",
      "Batch 3800/19342, Loss: 0.1626, Acc: 92.82%\n",
      "Batch 3850/19342, Loss: 0.1620, Acc: 92.86%\n",
      "Batch 3900/19342, Loss: 0.1621, Acc: 92.84%\n",
      "Batch 3950/19342, Loss: 0.1622, Acc: 92.84%\n",
      "Batch 4000/19342, Loss: 0.1620, Acc: 92.85%\n",
      "Batch 4050/19342, Loss: 0.1620, Acc: 92.85%\n",
      "Batch 4100/19342, Loss: 0.1621, Acc: 92.86%\n",
      "Batch 4150/19342, Loss: 0.1620, Acc: 92.85%\n",
      "Batch 4200/19342, Loss: 0.1617, Acc: 92.86%\n",
      "Batch 4250/19342, Loss: 0.1615, Acc: 92.85%\n",
      "Batch 4300/19342, Loss: 0.1612, Acc: 92.87%\n",
      "Batch 4350/19342, Loss: 0.1611, Acc: 92.89%\n",
      "Batch 4400/19342, Loss: 0.1613, Acc: 92.87%\n",
      "Batch 4450/19342, Loss: 0.1613, Acc: 92.86%\n",
      "Batch 4500/19342, Loss: 0.1615, Acc: 92.86%\n",
      "Batch 4550/19342, Loss: 0.1617, Acc: 92.85%\n",
      "Batch 4600/19342, Loss: 0.1617, Acc: 92.85%\n",
      "Batch 4650/19342, Loss: 0.1621, Acc: 92.82%\n",
      "Batch 4700/19342, Loss: 0.1619, Acc: 92.83%\n",
      "Batch 4750/19342, Loss: 0.1620, Acc: 92.83%\n",
      "Batch 4800/19342, Loss: 0.1621, Acc: 92.83%\n",
      "Batch 4850/19342, Loss: 0.1626, Acc: 92.80%\n",
      "Batch 4900/19342, Loss: 0.1625, Acc: 92.82%\n",
      "Batch 4950/19342, Loss: 0.1624, Acc: 92.82%\n",
      "Batch 5000/19342, Loss: 0.1625, Acc: 92.82%\n",
      "Batch 5050/19342, Loss: 0.1625, Acc: 92.81%\n",
      "Batch 5100/19342, Loss: 0.1622, Acc: 92.83%\n",
      "Batch 5150/19342, Loss: 0.1620, Acc: 92.84%\n",
      "Batch 5200/19342, Loss: 0.1621, Acc: 92.84%\n",
      "Batch 5250/19342, Loss: 0.1620, Acc: 92.85%\n",
      "Batch 5300/19342, Loss: 0.1622, Acc: 92.84%\n",
      "Batch 5350/19342, Loss: 0.1619, Acc: 92.85%\n",
      "Batch 5400/19342, Loss: 0.1620, Acc: 92.83%\n",
      "Batch 5450/19342, Loss: 0.1620, Acc: 92.83%\n",
      "Batch 5500/19342, Loss: 0.1618, Acc: 92.82%\n",
      "Batch 5550/19342, Loss: 0.1621, Acc: 92.81%\n",
      "Batch 5600/19342, Loss: 0.1622, Acc: 92.81%\n",
      "Batch 5650/19342, Loss: 0.1622, Acc: 92.80%\n",
      "Batch 5700/19342, Loss: 0.1623, Acc: 92.80%\n",
      "Batch 5750/19342, Loss: 0.1619, Acc: 92.82%\n",
      "Batch 5800/19342, Loss: 0.1623, Acc: 92.80%\n",
      "Batch 5850/19342, Loss: 0.1628, Acc: 92.78%\n",
      "Batch 5900/19342, Loss: 0.1634, Acc: 92.75%\n",
      "Batch 5950/19342, Loss: 0.1634, Acc: 92.75%\n",
      "Batch 6000/19342, Loss: 0.1633, Acc: 92.75%\n",
      "Batch 6050/19342, Loss: 0.1632, Acc: 92.76%\n",
      "Batch 6100/19342, Loss: 0.1634, Acc: 92.76%\n",
      "Batch 6150/19342, Loss: 0.1634, Acc: 92.75%\n",
      "Batch 6200/19342, Loss: 0.1632, Acc: 92.76%\n",
      "Batch 6250/19342, Loss: 0.1633, Acc: 92.77%\n",
      "Batch 6300/19342, Loss: 0.1632, Acc: 92.77%\n",
      "Batch 6350/19342, Loss: 0.1632, Acc: 92.78%\n",
      "Batch 6400/19342, Loss: 0.1630, Acc: 92.79%\n",
      "Batch 6450/19342, Loss: 0.1629, Acc: 92.80%\n",
      "Batch 6500/19342, Loss: 0.1628, Acc: 92.80%\n",
      "Batch 6550/19342, Loss: 0.1630, Acc: 92.79%\n",
      "Batch 6600/19342, Loss: 0.1628, Acc: 92.80%\n",
      "Batch 6650/19342, Loss: 0.1630, Acc: 92.78%\n",
      "Batch 6700/19342, Loss: 0.1630, Acc: 92.78%\n",
      "Batch 6750/19342, Loss: 0.1630, Acc: 92.79%\n",
      "Batch 6800/19342, Loss: 0.1634, Acc: 92.78%\n",
      "Batch 6850/19342, Loss: 0.1633, Acc: 92.78%\n",
      "Batch 6900/19342, Loss: 0.1633, Acc: 92.78%\n",
      "Batch 6950/19342, Loss: 0.1633, Acc: 92.79%\n",
      "Batch 7000/19342, Loss: 0.1630, Acc: 92.81%\n",
      "Batch 7050/19342, Loss: 0.1628, Acc: 92.81%\n",
      "Batch 7100/19342, Loss: 0.1625, Acc: 92.83%\n",
      "Batch 7150/19342, Loss: 0.1624, Acc: 92.83%\n",
      "Batch 7200/19342, Loss: 0.1623, Acc: 92.83%\n",
      "Batch 7250/19342, Loss: 0.1619, Acc: 92.85%\n",
      "Batch 7300/19342, Loss: 0.1621, Acc: 92.83%\n",
      "Batch 7350/19342, Loss: 0.1620, Acc: 92.83%\n",
      "Batch 7400/19342, Loss: 0.1620, Acc: 92.83%\n",
      "Batch 7450/19342, Loss: 0.1622, Acc: 92.82%\n",
      "Batch 7500/19342, Loss: 0.1620, Acc: 92.82%\n",
      "Batch 7550/19342, Loss: 0.1620, Acc: 92.83%\n",
      "Batch 7600/19342, Loss: 0.1619, Acc: 92.84%\n",
      "Batch 7650/19342, Loss: 0.1619, Acc: 92.85%\n",
      "Batch 7700/19342, Loss: 0.1617, Acc: 92.85%\n",
      "Batch 7750/19342, Loss: 0.1617, Acc: 92.85%\n",
      "Batch 7800/19342, Loss: 0.1617, Acc: 92.85%\n",
      "Batch 7850/19342, Loss: 0.1617, Acc: 92.85%\n",
      "Batch 7900/19342, Loss: 0.1617, Acc: 92.84%\n",
      "Batch 7950/19342, Loss: 0.1616, Acc: 92.84%\n",
      "Batch 8000/19342, Loss: 0.1617, Acc: 92.83%\n",
      "Batch 8050/19342, Loss: 0.1615, Acc: 92.84%\n",
      "Batch 8100/19342, Loss: 0.1613, Acc: 92.85%\n",
      "Batch 8150/19342, Loss: 0.1614, Acc: 92.83%\n",
      "Batch 8200/19342, Loss: 0.1615, Acc: 92.83%\n",
      "Batch 8250/19342, Loss: 0.1613, Acc: 92.84%\n",
      "Batch 8300/19342, Loss: 0.1613, Acc: 92.84%\n",
      "Batch 8350/19342, Loss: 0.1616, Acc: 92.82%\n",
      "Batch 8400/19342, Loss: 0.1615, Acc: 92.83%\n",
      "Batch 8450/19342, Loss: 0.1616, Acc: 92.83%\n",
      "Batch 8500/19342, Loss: 0.1614, Acc: 92.84%\n",
      "Batch 8550/19342, Loss: 0.1613, Acc: 92.84%\n",
      "Batch 8600/19342, Loss: 0.1617, Acc: 92.82%\n",
      "Batch 8650/19342, Loss: 0.1620, Acc: 92.81%\n",
      "Batch 8700/19342, Loss: 0.1619, Acc: 92.81%\n",
      "Batch 8750/19342, Loss: 0.1620, Acc: 92.81%\n",
      "Batch 8800/19342, Loss: 0.1619, Acc: 92.82%\n",
      "Batch 8850/19342, Loss: 0.1619, Acc: 92.82%\n",
      "Batch 8900/19342, Loss: 0.1620, Acc: 92.81%\n",
      "Batch 8950/19342, Loss: 0.1619, Acc: 92.82%\n",
      "Batch 9000/19342, Loss: 0.1618, Acc: 92.83%\n",
      "Batch 9050/19342, Loss: 0.1617, Acc: 92.83%\n",
      "Batch 9100/19342, Loss: 0.1620, Acc: 92.82%\n",
      "Batch 9150/19342, Loss: 0.1621, Acc: 92.81%\n",
      "Batch 9200/19342, Loss: 0.1622, Acc: 92.80%\n",
      "Batch 9250/19342, Loss: 0.1622, Acc: 92.80%\n",
      "Batch 9300/19342, Loss: 0.1623, Acc: 92.80%\n",
      "Batch 9350/19342, Loss: 0.1622, Acc: 92.80%\n",
      "Batch 9400/19342, Loss: 0.1624, Acc: 92.80%\n",
      "Batch 9450/19342, Loss: 0.1624, Acc: 92.81%\n",
      "Batch 9500/19342, Loss: 0.1625, Acc: 92.80%\n",
      "Batch 9550/19342, Loss: 0.1623, Acc: 92.81%\n",
      "Batch 9600/19342, Loss: 0.1624, Acc: 92.82%\n",
      "Batch 9650/19342, Loss: 0.1622, Acc: 92.83%\n",
      "Batch 9700/19342, Loss: 0.1625, Acc: 92.82%\n",
      "Batch 9750/19342, Loss: 0.1625, Acc: 92.82%\n",
      "Batch 9800/19342, Loss: 0.1623, Acc: 92.83%\n",
      "Batch 9850/19342, Loss: 0.1625, Acc: 92.81%\n",
      "Batch 9900/19342, Loss: 0.1625, Acc: 92.81%\n",
      "Batch 9950/19342, Loss: 0.1627, Acc: 92.80%\n",
      "Batch 10000/19342, Loss: 0.1628, Acc: 92.79%\n",
      "Batch 10050/19342, Loss: 0.1627, Acc: 92.80%\n",
      "Batch 10100/19342, Loss: 0.1627, Acc: 92.80%\n",
      "Batch 10150/19342, Loss: 0.1629, Acc: 92.78%\n",
      "Batch 10200/19342, Loss: 0.1628, Acc: 92.79%\n",
      "Batch 10250/19342, Loss: 0.1626, Acc: 92.80%\n",
      "Batch 10300/19342, Loss: 0.1627, Acc: 92.80%\n",
      "Batch 10350/19342, Loss: 0.1627, Acc: 92.80%\n",
      "Batch 10400/19342, Loss: 0.1627, Acc: 92.80%\n",
      "Batch 10450/19342, Loss: 0.1627, Acc: 92.81%\n",
      "Batch 10500/19342, Loss: 0.1628, Acc: 92.80%\n",
      "Batch 10550/19342, Loss: 0.1629, Acc: 92.80%\n",
      "Batch 10600/19342, Loss: 0.1628, Acc: 92.80%\n",
      "Batch 10650/19342, Loss: 0.1628, Acc: 92.80%\n",
      "Batch 10700/19342, Loss: 0.1626, Acc: 92.81%\n",
      "Batch 10750/19342, Loss: 0.1627, Acc: 92.80%\n",
      "Batch 10800/19342, Loss: 0.1628, Acc: 92.80%\n",
      "Batch 10850/19342, Loss: 0.1628, Acc: 92.79%\n",
      "Batch 10900/19342, Loss: 0.1629, Acc: 92.79%\n",
      "Batch 10950/19342, Loss: 0.1631, Acc: 92.79%\n",
      "Batch 11000/19342, Loss: 0.1630, Acc: 92.79%\n",
      "Batch 11050/19342, Loss: 0.1631, Acc: 92.79%\n",
      "Batch 11100/19342, Loss: 0.1629, Acc: 92.80%\n",
      "Batch 11150/19342, Loss: 0.1629, Acc: 92.80%\n",
      "Batch 11200/19342, Loss: 0.1629, Acc: 92.80%\n",
      "Batch 11250/19342, Loss: 0.1630, Acc: 92.80%\n",
      "Batch 11300/19342, Loss: 0.1630, Acc: 92.79%\n",
      "Batch 11350/19342, Loss: 0.1629, Acc: 92.80%\n",
      "Batch 11400/19342, Loss: 0.1630, Acc: 92.80%\n",
      "Batch 11450/19342, Loss: 0.1630, Acc: 92.80%\n",
      "Batch 11500/19342, Loss: 0.1631, Acc: 92.80%\n",
      "Batch 11550/19342, Loss: 0.1632, Acc: 92.79%\n",
      "Batch 11600/19342, Loss: 0.1632, Acc: 92.79%\n",
      "Batch 11650/19342, Loss: 0.1632, Acc: 92.79%\n",
      "Batch 11700/19342, Loss: 0.1632, Acc: 92.79%\n",
      "Batch 11750/19342, Loss: 0.1632, Acc: 92.79%\n",
      "Batch 11800/19342, Loss: 0.1633, Acc: 92.79%\n",
      "Batch 11850/19342, Loss: 0.1632, Acc: 92.80%\n",
      "Batch 11900/19342, Loss: 0.1633, Acc: 92.80%\n",
      "Batch 11950/19342, Loss: 0.1633, Acc: 92.80%\n",
      "Batch 12000/19342, Loss: 0.1633, Acc: 92.80%\n",
      "Batch 12050/19342, Loss: 0.1635, Acc: 92.79%\n",
      "Batch 12100/19342, Loss: 0.1634, Acc: 92.80%\n",
      "Batch 12150/19342, Loss: 0.1634, Acc: 92.80%\n",
      "Batch 12200/19342, Loss: 0.1635, Acc: 92.80%\n",
      "Batch 12250/19342, Loss: 0.1636, Acc: 92.79%\n",
      "Batch 12300/19342, Loss: 0.1635, Acc: 92.79%\n",
      "Batch 12350/19342, Loss: 0.1634, Acc: 92.79%\n",
      "Batch 12400/19342, Loss: 0.1635, Acc: 92.79%\n",
      "Batch 12450/19342, Loss: 0.1635, Acc: 92.79%\n",
      "Batch 12500/19342, Loss: 0.1633, Acc: 92.80%\n",
      "Batch 12550/19342, Loss: 0.1632, Acc: 92.80%\n",
      "Batch 12600/19342, Loss: 0.1632, Acc: 92.80%\n",
      "Batch 12650/19342, Loss: 0.1632, Acc: 92.80%\n",
      "Batch 12700/19342, Loss: 0.1630, Acc: 92.81%\n",
      "Batch 12750/19342, Loss: 0.1629, Acc: 92.81%\n",
      "Batch 12800/19342, Loss: 0.1629, Acc: 92.81%\n",
      "Batch 12850/19342, Loss: 0.1630, Acc: 92.81%\n",
      "Batch 12900/19342, Loss: 0.1630, Acc: 92.81%\n",
      "Batch 12950/19342, Loss: 0.1629, Acc: 92.81%\n",
      "Batch 13000/19342, Loss: 0.1631, Acc: 92.80%\n",
      "Batch 13050/19342, Loss: 0.1631, Acc: 92.81%\n",
      "Batch 13100/19342, Loss: 0.1632, Acc: 92.81%\n",
      "Batch 13150/19342, Loss: 0.1631, Acc: 92.81%\n",
      "Batch 13200/19342, Loss: 0.1632, Acc: 92.81%\n",
      "Batch 13250/19342, Loss: 0.1632, Acc: 92.82%\n",
      "Batch 13300/19342, Loss: 0.1632, Acc: 92.81%\n",
      "Batch 13350/19342, Loss: 0.1632, Acc: 92.82%\n",
      "Batch 13400/19342, Loss: 0.1631, Acc: 92.83%\n",
      "Batch 13450/19342, Loss: 0.1631, Acc: 92.82%\n",
      "Batch 13500/19342, Loss: 0.1632, Acc: 92.82%\n",
      "Batch 13550/19342, Loss: 0.1631, Acc: 92.82%\n",
      "Batch 13600/19342, Loss: 0.1631, Acc: 92.82%\n",
      "Batch 13650/19342, Loss: 0.1631, Acc: 92.82%\n",
      "Batch 13700/19342, Loss: 0.1630, Acc: 92.82%\n",
      "Batch 13750/19342, Loss: 0.1629, Acc: 92.83%\n",
      "Batch 13800/19342, Loss: 0.1630, Acc: 92.82%\n",
      "Batch 13850/19342, Loss: 0.1629, Acc: 92.83%\n",
      "Batch 13900/19342, Loss: 0.1627, Acc: 92.84%\n",
      "Batch 13950/19342, Loss: 0.1628, Acc: 92.84%\n",
      "Batch 14000/19342, Loss: 0.1629, Acc: 92.83%\n",
      "Batch 14050/19342, Loss: 0.1628, Acc: 92.84%\n",
      "Batch 14100/19342, Loss: 0.1628, Acc: 92.84%\n",
      "Batch 14150/19342, Loss: 0.1627, Acc: 92.84%\n",
      "Batch 14200/19342, Loss: 0.1628, Acc: 92.84%\n",
      "Batch 14250/19342, Loss: 0.1629, Acc: 92.83%\n",
      "Batch 14300/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 14350/19342, Loss: 0.1628, Acc: 92.84%\n",
      "Batch 14400/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 14450/19342, Loss: 0.1628, Acc: 92.84%\n",
      "Batch 14500/19342, Loss: 0.1629, Acc: 92.83%\n",
      "Batch 14550/19342, Loss: 0.1629, Acc: 92.83%\n",
      "Batch 14600/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 14650/19342, Loss: 0.1627, Acc: 92.84%\n",
      "Batch 14700/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 14750/19342, Loss: 0.1625, Acc: 92.85%\n",
      "Batch 14800/19342, Loss: 0.1624, Acc: 92.85%\n",
      "Batch 14850/19342, Loss: 0.1624, Acc: 92.86%\n",
      "Batch 14900/19342, Loss: 0.1625, Acc: 92.85%\n",
      "Batch 14950/19342, Loss: 0.1625, Acc: 92.85%\n",
      "Batch 15000/19342, Loss: 0.1625, Acc: 92.85%\n",
      "Batch 15050/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 15100/19342, Loss: 0.1625, Acc: 92.85%\n",
      "Batch 15150/19342, Loss: 0.1625, Acc: 92.85%\n",
      "Batch 15200/19342, Loss: 0.1623, Acc: 92.86%\n",
      "Batch 15250/19342, Loss: 0.1623, Acc: 92.87%\n",
      "Batch 15300/19342, Loss: 0.1621, Acc: 92.88%\n",
      "Batch 15350/19342, Loss: 0.1622, Acc: 92.88%\n",
      "Batch 15400/19342, Loss: 0.1621, Acc: 92.88%\n",
      "Batch 15450/19342, Loss: 0.1621, Acc: 92.88%\n",
      "Batch 15500/19342, Loss: 0.1621, Acc: 92.88%\n",
      "Batch 15550/19342, Loss: 0.1621, Acc: 92.87%\n",
      "Batch 15600/19342, Loss: 0.1622, Acc: 92.87%\n",
      "Batch 15650/19342, Loss: 0.1620, Acc: 92.87%\n",
      "Batch 15700/19342, Loss: 0.1621, Acc: 92.87%\n",
      "Batch 15750/19342, Loss: 0.1621, Acc: 92.87%\n",
      "Batch 15800/19342, Loss: 0.1620, Acc: 92.88%\n",
      "Batch 15850/19342, Loss: 0.1620, Acc: 92.87%\n",
      "Batch 15900/19342, Loss: 0.1620, Acc: 92.87%\n",
      "Batch 15950/19342, Loss: 0.1618, Acc: 92.88%\n",
      "Batch 16000/19342, Loss: 0.1620, Acc: 92.87%\n",
      "Batch 16050/19342, Loss: 0.1621, Acc: 92.87%\n",
      "Batch 16100/19342, Loss: 0.1620, Acc: 92.87%\n",
      "Batch 16150/19342, Loss: 0.1622, Acc: 92.87%\n",
      "Batch 16200/19342, Loss: 0.1622, Acc: 92.86%\n",
      "Batch 16250/19342, Loss: 0.1621, Acc: 92.87%\n",
      "Batch 16300/19342, Loss: 0.1621, Acc: 92.86%\n",
      "Batch 16350/19342, Loss: 0.1622, Acc: 92.86%\n",
      "Batch 16400/19342, Loss: 0.1622, Acc: 92.86%\n",
      "Batch 16450/19342, Loss: 0.1622, Acc: 92.86%\n",
      "Batch 16500/19342, Loss: 0.1623, Acc: 92.86%\n",
      "Batch 16550/19342, Loss: 0.1623, Acc: 92.85%\n",
      "Batch 16600/19342, Loss: 0.1622, Acc: 92.86%\n",
      "Batch 16650/19342, Loss: 0.1623, Acc: 92.86%\n",
      "Batch 16700/19342, Loss: 0.1623, Acc: 92.86%\n",
      "Batch 16750/19342, Loss: 0.1622, Acc: 92.87%\n",
      "Batch 16800/19342, Loss: 0.1622, Acc: 92.86%\n",
      "Batch 16850/19342, Loss: 0.1623, Acc: 92.86%\n",
      "Batch 16900/19342, Loss: 0.1623, Acc: 92.86%\n",
      "Batch 16950/19342, Loss: 0.1624, Acc: 92.86%\n",
      "Batch 17000/19342, Loss: 0.1623, Acc: 92.86%\n",
      "Batch 17050/19342, Loss: 0.1624, Acc: 92.85%\n",
      "Batch 17100/19342, Loss: 0.1626, Acc: 92.85%\n",
      "Batch 17150/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17200/19342, Loss: 0.1627, Acc: 92.84%\n",
      "Batch 17250/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17300/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17350/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17400/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 17450/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17500/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17550/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17600/19342, Loss: 0.1625, Acc: 92.85%\n",
      "Batch 17650/19342, Loss: 0.1625, Acc: 92.84%\n",
      "Batch 17700/19342, Loss: 0.1625, Acc: 92.84%\n",
      "Batch 17750/19342, Loss: 0.1626, Acc: 92.83%\n",
      "Batch 17800/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 17850/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17900/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 17950/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 18000/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18050/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18100/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18150/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18200/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 18250/19342, Loss: 0.1626, Acc: 92.83%\n",
      "Batch 18300/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18350/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18400/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18450/19342, Loss: 0.1629, Acc: 92.82%\n",
      "Batch 18500/19342, Loss: 0.1629, Acc: 92.82%\n",
      "Batch 18550/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18600/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18650/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 18700/19342, Loss: 0.1628, Acc: 92.82%\n",
      "Batch 18750/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18800/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18850/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 18900/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 18950/19342, Loss: 0.1628, Acc: 92.83%\n",
      "Batch 19000/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 19050/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 19100/19342, Loss: 0.1626, Acc: 92.84%\n",
      "Batch 19150/19342, Loss: 0.1626, Acc: 92.83%\n",
      "Batch 19200/19342, Loss: 0.1627, Acc: 92.83%\n",
      "Batch 19250/19342, Loss: 0.1626, Acc: 92.83%\n",
      "Batch 19300/19342, Loss: 0.1625, Acc: 92.83%\n",
      "Train Loss: 0.1625, Train Acc: 92.83%\n",
      "Val Loss: 0.3139, Val Acc: 87.18%\n",
      "EarlyStopping counter: 3 out of 5\n",
      "\n",
      "Epoch 9/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.1566, Acc: 93.50%\n",
      "Batch 100/19342, Loss: 0.1556, Acc: 93.12%\n",
      "Batch 150/19342, Loss: 0.1637, Acc: 92.92%\n",
      "Batch 200/19342, Loss: 0.1612, Acc: 93.19%\n",
      "Batch 250/19342, Loss: 0.1615, Acc: 93.20%\n",
      "Batch 300/19342, Loss: 0.1614, Acc: 93.12%\n",
      "Batch 350/19342, Loss: 0.1615, Acc: 92.86%\n",
      "Batch 400/19342, Loss: 0.1650, Acc: 92.91%\n",
      "Batch 450/19342, Loss: 0.1614, Acc: 93.03%\n",
      "Batch 500/19342, Loss: 0.1598, Acc: 93.08%\n",
      "Batch 550/19342, Loss: 0.1563, Acc: 93.23%\n",
      "Batch 600/19342, Loss: 0.1562, Acc: 93.23%\n",
      "Batch 650/19342, Loss: 0.1540, Acc: 93.33%\n",
      "Batch 700/19342, Loss: 0.1529, Acc: 93.38%\n",
      "Batch 750/19342, Loss: 0.1544, Acc: 93.32%\n",
      "Batch 800/19342, Loss: 0.1538, Acc: 93.41%\n",
      "Batch 850/19342, Loss: 0.1562, Acc: 93.25%\n",
      "Batch 900/19342, Loss: 0.1533, Acc: 93.40%\n",
      "Batch 950/19342, Loss: 0.1528, Acc: 93.39%\n",
      "Batch 1000/19342, Loss: 0.1527, Acc: 93.40%\n",
      "Batch 1050/19342, Loss: 0.1526, Acc: 93.40%\n",
      "Batch 1100/19342, Loss: 0.1505, Acc: 93.51%\n",
      "Batch 1150/19342, Loss: 0.1495, Acc: 93.57%\n",
      "Batch 1200/19342, Loss: 0.1494, Acc: 93.55%\n",
      "Batch 1250/19342, Loss: 0.1490, Acc: 93.54%\n",
      "Batch 1300/19342, Loss: 0.1476, Acc: 93.61%\n",
      "Batch 1350/19342, Loss: 0.1482, Acc: 93.60%\n",
      "Batch 1400/19342, Loss: 0.1507, Acc: 93.53%\n",
      "Batch 1450/19342, Loss: 0.1513, Acc: 93.45%\n",
      "Batch 1500/19342, Loss: 0.1509, Acc: 93.44%\n",
      "Batch 1550/19342, Loss: 0.1515, Acc: 93.45%\n",
      "Batch 1600/19342, Loss: 0.1510, Acc: 93.45%\n",
      "Batch 1650/19342, Loss: 0.1515, Acc: 93.42%\n",
      "Batch 1700/19342, Loss: 0.1529, Acc: 93.36%\n",
      "Batch 1750/19342, Loss: 0.1514, Acc: 93.46%\n",
      "Batch 1800/19342, Loss: 0.1526, Acc: 93.40%\n",
      "Batch 1850/19342, Loss: 0.1525, Acc: 93.42%\n",
      "Batch 1900/19342, Loss: 0.1523, Acc: 93.40%\n",
      "Batch 1950/19342, Loss: 0.1518, Acc: 93.42%\n",
      "Batch 2000/19342, Loss: 0.1524, Acc: 93.37%\n",
      "Batch 2050/19342, Loss: 0.1530, Acc: 93.30%\n",
      "Batch 2100/19342, Loss: 0.1530, Acc: 93.25%\n",
      "Batch 2150/19342, Loss: 0.1529, Acc: 93.23%\n",
      "Batch 2200/19342, Loss: 0.1528, Acc: 93.26%\n",
      "Batch 2250/19342, Loss: 0.1529, Acc: 93.25%\n",
      "Batch 2300/19342, Loss: 0.1524, Acc: 93.28%\n",
      "Batch 2350/19342, Loss: 0.1520, Acc: 93.28%\n",
      "Batch 2400/19342, Loss: 0.1517, Acc: 93.28%\n",
      "Batch 2450/19342, Loss: 0.1525, Acc: 93.24%\n",
      "Batch 2500/19342, Loss: 0.1523, Acc: 93.27%\n",
      "Batch 2550/19342, Loss: 0.1523, Acc: 93.25%\n",
      "Batch 2600/19342, Loss: 0.1527, Acc: 93.26%\n",
      "Batch 2650/19342, Loss: 0.1525, Acc: 93.27%\n",
      "Batch 2700/19342, Loss: 0.1527, Acc: 93.23%\n",
      "Batch 2750/19342, Loss: 0.1531, Acc: 93.21%\n",
      "Batch 2800/19342, Loss: 0.1529, Acc: 93.23%\n",
      "Batch 2850/19342, Loss: 0.1529, Acc: 93.23%\n",
      "Batch 2900/19342, Loss: 0.1532, Acc: 93.22%\n",
      "Batch 2950/19342, Loss: 0.1531, Acc: 93.25%\n",
      "Batch 3000/19342, Loss: 0.1532, Acc: 93.26%\n",
      "Batch 3050/19342, Loss: 0.1527, Acc: 93.30%\n",
      "Batch 3100/19342, Loss: 0.1526, Acc: 93.31%\n",
      "Batch 3150/19342, Loss: 0.1522, Acc: 93.33%\n",
      "Batch 3200/19342, Loss: 0.1525, Acc: 93.30%\n",
      "Batch 3250/19342, Loss: 0.1532, Acc: 93.25%\n",
      "Batch 3300/19342, Loss: 0.1530, Acc: 93.26%\n",
      "Batch 3350/19342, Loss: 0.1526, Acc: 93.29%\n",
      "Batch 3400/19342, Loss: 0.1530, Acc: 93.28%\n",
      "Batch 3450/19342, Loss: 0.1527, Acc: 93.30%\n",
      "Batch 3500/19342, Loss: 0.1533, Acc: 93.28%\n",
      "Batch 3550/19342, Loss: 0.1530, Acc: 93.29%\n",
      "Batch 3600/19342, Loss: 0.1531, Acc: 93.27%\n",
      "Batch 3650/19342, Loss: 0.1529, Acc: 93.28%\n",
      "Batch 3700/19342, Loss: 0.1531, Acc: 93.27%\n",
      "Batch 3750/19342, Loss: 0.1532, Acc: 93.27%\n",
      "Batch 3800/19342, Loss: 0.1533, Acc: 93.27%\n",
      "Batch 3850/19342, Loss: 0.1531, Acc: 93.30%\n",
      "Batch 3900/19342, Loss: 0.1536, Acc: 93.28%\n",
      "Batch 3950/19342, Loss: 0.1538, Acc: 93.27%\n",
      "Batch 4000/19342, Loss: 0.1539, Acc: 93.25%\n",
      "Batch 4050/19342, Loss: 0.1541, Acc: 93.23%\n",
      "Batch 4100/19342, Loss: 0.1540, Acc: 93.25%\n",
      "Batch 4150/19342, Loss: 0.1538, Acc: 93.26%\n",
      "Batch 4200/19342, Loss: 0.1538, Acc: 93.26%\n",
      "Batch 4250/19342, Loss: 0.1540, Acc: 93.24%\n",
      "Batch 4300/19342, Loss: 0.1540, Acc: 93.24%\n",
      "Batch 4350/19342, Loss: 0.1540, Acc: 93.25%\n",
      "Batch 4400/19342, Loss: 0.1536, Acc: 93.26%\n",
      "Batch 4450/19342, Loss: 0.1536, Acc: 93.26%\n",
      "Batch 4500/19342, Loss: 0.1537, Acc: 93.25%\n",
      "Batch 4550/19342, Loss: 0.1539, Acc: 93.25%\n",
      "Batch 4600/19342, Loss: 0.1537, Acc: 93.26%\n",
      "Batch 4650/19342, Loss: 0.1534, Acc: 93.26%\n",
      "Batch 4700/19342, Loss: 0.1540, Acc: 93.23%\n",
      "Batch 4750/19342, Loss: 0.1545, Acc: 93.22%\n",
      "Batch 4800/19342, Loss: 0.1545, Acc: 93.23%\n",
      "Batch 4850/19342, Loss: 0.1545, Acc: 93.24%\n",
      "Batch 4900/19342, Loss: 0.1545, Acc: 93.24%\n",
      "Batch 4950/19342, Loss: 0.1546, Acc: 93.24%\n",
      "Batch 5000/19342, Loss: 0.1550, Acc: 93.21%\n",
      "Batch 5050/19342, Loss: 0.1550, Acc: 93.21%\n",
      "Batch 5100/19342, Loss: 0.1546, Acc: 93.23%\n",
      "Batch 5150/19342, Loss: 0.1550, Acc: 93.22%\n",
      "Batch 5200/19342, Loss: 0.1549, Acc: 93.23%\n",
      "Batch 5250/19342, Loss: 0.1550, Acc: 93.23%\n",
      "Batch 5300/19342, Loss: 0.1552, Acc: 93.22%\n",
      "Batch 5350/19342, Loss: 0.1553, Acc: 93.22%\n",
      "Batch 5400/19342, Loss: 0.1555, Acc: 93.21%\n",
      "Batch 5450/19342, Loss: 0.1559, Acc: 93.18%\n",
      "Batch 5500/19342, Loss: 0.1560, Acc: 93.18%\n",
      "Batch 5550/19342, Loss: 0.1559, Acc: 93.18%\n",
      "Batch 5600/19342, Loss: 0.1559, Acc: 93.17%\n",
      "Batch 5650/19342, Loss: 0.1558, Acc: 93.17%\n",
      "Batch 5700/19342, Loss: 0.1555, Acc: 93.19%\n",
      "Batch 5750/19342, Loss: 0.1554, Acc: 93.20%\n",
      "Batch 5800/19342, Loss: 0.1554, Acc: 93.20%\n",
      "Batch 5850/19342, Loss: 0.1553, Acc: 93.21%\n",
      "Batch 5900/19342, Loss: 0.1553, Acc: 93.21%\n",
      "Batch 5950/19342, Loss: 0.1552, Acc: 93.21%\n",
      "Batch 6000/19342, Loss: 0.1551, Acc: 93.22%\n",
      "Batch 6050/19342, Loss: 0.1553, Acc: 93.21%\n",
      "Batch 6100/19342, Loss: 0.1552, Acc: 93.21%\n",
      "Batch 6150/19342, Loss: 0.1552, Acc: 93.20%\n",
      "Batch 6200/19342, Loss: 0.1551, Acc: 93.21%\n",
      "Batch 6250/19342, Loss: 0.1552, Acc: 93.20%\n",
      "Batch 6300/19342, Loss: 0.1553, Acc: 93.21%\n",
      "Batch 6350/19342, Loss: 0.1555, Acc: 93.20%\n",
      "Batch 6400/19342, Loss: 0.1554, Acc: 93.20%\n",
      "Batch 6450/19342, Loss: 0.1555, Acc: 93.19%\n",
      "Batch 6500/19342, Loss: 0.1555, Acc: 93.19%\n",
      "Batch 6550/19342, Loss: 0.1555, Acc: 93.19%\n",
      "Batch 6600/19342, Loss: 0.1553, Acc: 93.20%\n",
      "Batch 6650/19342, Loss: 0.1554, Acc: 93.19%\n",
      "Batch 6700/19342, Loss: 0.1558, Acc: 93.18%\n",
      "Batch 6750/19342, Loss: 0.1557, Acc: 93.18%\n",
      "Batch 6800/19342, Loss: 0.1558, Acc: 93.17%\n",
      "Batch 6850/19342, Loss: 0.1557, Acc: 93.17%\n",
      "Batch 6900/19342, Loss: 0.1558, Acc: 93.17%\n",
      "Batch 6950/19342, Loss: 0.1561, Acc: 93.15%\n",
      "Batch 7000/19342, Loss: 0.1560, Acc: 93.15%\n",
      "Batch 7050/19342, Loss: 0.1560, Acc: 93.15%\n",
      "Batch 7100/19342, Loss: 0.1558, Acc: 93.16%\n",
      "Batch 7150/19342, Loss: 0.1557, Acc: 93.16%\n",
      "Batch 7200/19342, Loss: 0.1559, Acc: 93.15%\n",
      "Batch 7250/19342, Loss: 0.1557, Acc: 93.15%\n",
      "Batch 7300/19342, Loss: 0.1556, Acc: 93.15%\n",
      "Batch 7350/19342, Loss: 0.1557, Acc: 93.16%\n",
      "Batch 7400/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 7450/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 7500/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 7550/19342, Loss: 0.1554, Acc: 93.18%\n",
      "Batch 7600/19342, Loss: 0.1553, Acc: 93.19%\n",
      "Batch 7650/19342, Loss: 0.1552, Acc: 93.19%\n",
      "Batch 7700/19342, Loss: 0.1552, Acc: 93.19%\n",
      "Batch 7750/19342, Loss: 0.1551, Acc: 93.19%\n",
      "Batch 7800/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 7850/19342, Loss: 0.1551, Acc: 93.18%\n",
      "Batch 7900/19342, Loss: 0.1551, Acc: 93.18%\n",
      "Batch 7950/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 8000/19342, Loss: 0.1555, Acc: 93.17%\n",
      "Batch 8050/19342, Loss: 0.1557, Acc: 93.16%\n",
      "Batch 8100/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 8150/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 8200/19342, Loss: 0.1555, Acc: 93.17%\n",
      "Batch 8250/19342, Loss: 0.1553, Acc: 93.18%\n",
      "Batch 8300/19342, Loss: 0.1556, Acc: 93.17%\n",
      "Batch 8350/19342, Loss: 0.1555, Acc: 93.18%\n",
      "Batch 8400/19342, Loss: 0.1556, Acc: 93.18%\n",
      "Batch 8450/19342, Loss: 0.1554, Acc: 93.18%\n",
      "Batch 8500/19342, Loss: 0.1554, Acc: 93.19%\n",
      "Batch 8550/19342, Loss: 0.1555, Acc: 93.18%\n",
      "Batch 8600/19342, Loss: 0.1554, Acc: 93.19%\n",
      "Batch 8650/19342, Loss: 0.1554, Acc: 93.19%\n",
      "Batch 8700/19342, Loss: 0.1553, Acc: 93.20%\n",
      "Batch 8750/19342, Loss: 0.1551, Acc: 93.21%\n",
      "Batch 8800/19342, Loss: 0.1549, Acc: 93.22%\n",
      "Batch 8850/19342, Loss: 0.1546, Acc: 93.23%\n",
      "Batch 8900/19342, Loss: 0.1545, Acc: 93.23%\n",
      "Batch 8950/19342, Loss: 0.1544, Acc: 93.24%\n",
      "Batch 9000/19342, Loss: 0.1543, Acc: 93.23%\n",
      "Batch 9050/19342, Loss: 0.1543, Acc: 93.23%\n",
      "Batch 9100/19342, Loss: 0.1544, Acc: 93.22%\n",
      "Batch 9150/19342, Loss: 0.1544, Acc: 93.22%\n",
      "Batch 9200/19342, Loss: 0.1544, Acc: 93.21%\n",
      "Batch 9250/19342, Loss: 0.1543, Acc: 93.22%\n",
      "Batch 9300/19342, Loss: 0.1545, Acc: 93.22%\n",
      "Batch 9350/19342, Loss: 0.1545, Acc: 93.22%\n",
      "Batch 9400/19342, Loss: 0.1546, Acc: 93.22%\n",
      "Batch 9450/19342, Loss: 0.1546, Acc: 93.22%\n",
      "Batch 9500/19342, Loss: 0.1545, Acc: 93.22%\n",
      "Batch 9550/19342, Loss: 0.1545, Acc: 93.22%\n",
      "Batch 9600/19342, Loss: 0.1545, Acc: 93.22%\n",
      "Batch 9650/19342, Loss: 0.1547, Acc: 93.22%\n",
      "Batch 9700/19342, Loss: 0.1546, Acc: 93.22%\n",
      "Batch 9750/19342, Loss: 0.1546, Acc: 93.21%\n",
      "Batch 9800/19342, Loss: 0.1548, Acc: 93.20%\n",
      "Batch 9850/19342, Loss: 0.1547, Acc: 93.20%\n",
      "Batch 9900/19342, Loss: 0.1547, Acc: 93.20%\n",
      "Batch 9950/19342, Loss: 0.1547, Acc: 93.20%\n",
      "Batch 10000/19342, Loss: 0.1549, Acc: 93.19%\n",
      "Batch 10050/19342, Loss: 0.1549, Acc: 93.20%\n",
      "Batch 10100/19342, Loss: 0.1548, Acc: 93.20%\n",
      "Batch 10150/19342, Loss: 0.1551, Acc: 93.19%\n",
      "Batch 10200/19342, Loss: 0.1551, Acc: 93.18%\n",
      "Batch 10250/19342, Loss: 0.1550, Acc: 93.19%\n",
      "Batch 10300/19342, Loss: 0.1549, Acc: 93.20%\n",
      "Batch 10350/19342, Loss: 0.1549, Acc: 93.20%\n",
      "Batch 10400/19342, Loss: 0.1548, Acc: 93.21%\n",
      "Batch 10450/19342, Loss: 0.1548, Acc: 93.20%\n",
      "Batch 10500/19342, Loss: 0.1547, Acc: 93.20%\n",
      "Batch 10550/19342, Loss: 0.1549, Acc: 93.20%\n",
      "Batch 10600/19342, Loss: 0.1548, Acc: 93.21%\n",
      "Batch 10650/19342, Loss: 0.1547, Acc: 93.21%\n",
      "Batch 10700/19342, Loss: 0.1547, Acc: 93.21%\n",
      "Batch 10750/19342, Loss: 0.1548, Acc: 93.21%\n",
      "Batch 10800/19342, Loss: 0.1549, Acc: 93.20%\n",
      "Batch 10850/19342, Loss: 0.1549, Acc: 93.21%\n",
      "Batch 10900/19342, Loss: 0.1547, Acc: 93.21%\n",
      "Batch 10950/19342, Loss: 0.1547, Acc: 93.21%\n",
      "Batch 11000/19342, Loss: 0.1548, Acc: 93.20%\n",
      "Batch 11050/19342, Loss: 0.1548, Acc: 93.19%\n",
      "Batch 11100/19342, Loss: 0.1550, Acc: 93.17%\n",
      "Batch 11150/19342, Loss: 0.1549, Acc: 93.18%\n",
      "Batch 11200/19342, Loss: 0.1552, Acc: 93.16%\n",
      "Batch 11250/19342, Loss: 0.1551, Acc: 93.16%\n",
      "Batch 11300/19342, Loss: 0.1551, Acc: 93.16%\n",
      "Batch 11350/19342, Loss: 0.1551, Acc: 93.16%\n",
      "Batch 11400/19342, Loss: 0.1551, Acc: 93.16%\n",
      "Batch 11450/19342, Loss: 0.1550, Acc: 93.17%\n",
      "Batch 11500/19342, Loss: 0.1552, Acc: 93.15%\n",
      "Batch 11550/19342, Loss: 0.1554, Acc: 93.15%\n",
      "Batch 11600/19342, Loss: 0.1555, Acc: 93.15%\n",
      "Batch 11650/19342, Loss: 0.1554, Acc: 93.14%\n",
      "Batch 11700/19342, Loss: 0.1554, Acc: 93.14%\n",
      "Batch 11750/19342, Loss: 0.1554, Acc: 93.15%\n",
      "Batch 11800/19342, Loss: 0.1555, Acc: 93.15%\n",
      "Batch 11850/19342, Loss: 0.1556, Acc: 93.14%\n",
      "Batch 11900/19342, Loss: 0.1556, Acc: 93.14%\n",
      "Batch 11950/19342, Loss: 0.1555, Acc: 93.15%\n",
      "Batch 12000/19342, Loss: 0.1554, Acc: 93.16%\n",
      "Batch 12050/19342, Loss: 0.1554, Acc: 93.15%\n",
      "Batch 12100/19342, Loss: 0.1554, Acc: 93.16%\n",
      "Batch 12150/19342, Loss: 0.1554, Acc: 93.16%\n",
      "Batch 12200/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 12250/19342, Loss: 0.1554, Acc: 93.15%\n",
      "Batch 12300/19342, Loss: 0.1553, Acc: 93.16%\n",
      "Batch 12350/19342, Loss: 0.1553, Acc: 93.16%\n",
      "Batch 12400/19342, Loss: 0.1553, Acc: 93.16%\n",
      "Batch 12450/19342, Loss: 0.1551, Acc: 93.17%\n",
      "Batch 12500/19342, Loss: 0.1552, Acc: 93.16%\n",
      "Batch 12550/19342, Loss: 0.1552, Acc: 93.16%\n",
      "Batch 12600/19342, Loss: 0.1551, Acc: 93.16%\n",
      "Batch 12650/19342, Loss: 0.1551, Acc: 93.16%\n",
      "Batch 12700/19342, Loss: 0.1550, Acc: 93.17%\n",
      "Batch 12750/19342, Loss: 0.1549, Acc: 93.17%\n",
      "Batch 12800/19342, Loss: 0.1549, Acc: 93.18%\n",
      "Batch 12850/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 12900/19342, Loss: 0.1547, Acc: 93.18%\n",
      "Batch 12950/19342, Loss: 0.1546, Acc: 93.19%\n",
      "Batch 13000/19342, Loss: 0.1546, Acc: 93.19%\n",
      "Batch 13050/19342, Loss: 0.1547, Acc: 93.18%\n",
      "Batch 13100/19342, Loss: 0.1547, Acc: 93.18%\n",
      "Batch 13150/19342, Loss: 0.1547, Acc: 93.18%\n",
      "Batch 13200/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 13250/19342, Loss: 0.1547, Acc: 93.18%\n",
      "Batch 13300/19342, Loss: 0.1547, Acc: 93.18%\n",
      "Batch 13350/19342, Loss: 0.1548, Acc: 93.19%\n",
      "Batch 13400/19342, Loss: 0.1548, Acc: 93.19%\n",
      "Batch 13450/19342, Loss: 0.1547, Acc: 93.19%\n",
      "Batch 13500/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 13550/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 13600/19342, Loss: 0.1549, Acc: 93.18%\n",
      "Batch 13650/19342, Loss: 0.1549, Acc: 93.18%\n",
      "Batch 13700/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 13750/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 13800/19342, Loss: 0.1547, Acc: 93.19%\n",
      "Batch 13850/19342, Loss: 0.1546, Acc: 93.19%\n",
      "Batch 13900/19342, Loss: 0.1547, Acc: 93.19%\n",
      "Batch 13950/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 14000/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 14050/19342, Loss: 0.1548, Acc: 93.18%\n",
      "Batch 14100/19342, Loss: 0.1550, Acc: 93.18%\n",
      "Batch 14150/19342, Loss: 0.1549, Acc: 93.18%\n",
      "Batch 14200/19342, Loss: 0.1550, Acc: 93.18%\n",
      "Batch 14250/19342, Loss: 0.1550, Acc: 93.18%\n",
      "Batch 14300/19342, Loss: 0.1549, Acc: 93.18%\n",
      "Batch 14350/19342, Loss: 0.1549, Acc: 93.18%\n",
      "Batch 14400/19342, Loss: 0.1550, Acc: 93.18%\n",
      "Batch 14450/19342, Loss: 0.1551, Acc: 93.17%\n",
      "Batch 14500/19342, Loss: 0.1551, Acc: 93.17%\n",
      "Batch 14550/19342, Loss: 0.1550, Acc: 93.18%\n",
      "Batch 14600/19342, Loss: 0.1550, Acc: 93.18%\n",
      "Batch 14650/19342, Loss: 0.1551, Acc: 93.17%\n",
      "Batch 14700/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 14750/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 14800/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 14850/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 14900/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 14950/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 15000/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 15050/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 15100/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 15150/19342, Loss: 0.1554, Acc: 93.17%\n",
      "Batch 15200/19342, Loss: 0.1554, Acc: 93.17%\n",
      "Batch 15250/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 15300/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 15350/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 15400/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 15450/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 15500/19342, Loss: 0.1551, Acc: 93.18%\n",
      "Batch 15550/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 15600/19342, Loss: 0.1553, Acc: 93.18%\n",
      "Batch 15650/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 15700/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 15750/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 15800/19342, Loss: 0.1553, Acc: 93.18%\n",
      "Batch 15850/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 15900/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 15950/19342, Loss: 0.1552, Acc: 93.18%\n",
      "Batch 16000/19342, Loss: 0.1553, Acc: 93.17%\n",
      "Batch 16050/19342, Loss: 0.1552, Acc: 93.17%\n",
      "Batch 16100/19342, Loss: 0.1553, Acc: 93.16%\n",
      "Batch 16150/19342, Loss: 0.1553, Acc: 93.16%\n",
      "Batch 16200/19342, Loss: 0.1554, Acc: 93.16%\n",
      "Batch 16250/19342, Loss: 0.1555, Acc: 93.15%\n",
      "Batch 16300/19342, Loss: 0.1554, Acc: 93.16%\n",
      "Batch 16350/19342, Loss: 0.1556, Acc: 93.15%\n",
      "Batch 16400/19342, Loss: 0.1557, Acc: 93.14%\n",
      "Batch 16450/19342, Loss: 0.1558, Acc: 93.13%\n",
      "Batch 16500/19342, Loss: 0.1557, Acc: 93.14%\n",
      "Batch 16550/19342, Loss: 0.1557, Acc: 93.14%\n",
      "Batch 16600/19342, Loss: 0.1558, Acc: 93.13%\n",
      "Batch 16650/19342, Loss: 0.1557, Acc: 93.14%\n",
      "Batch 16700/19342, Loss: 0.1556, Acc: 93.14%\n",
      "Batch 16750/19342, Loss: 0.1556, Acc: 93.15%\n",
      "Batch 16800/19342, Loss: 0.1556, Acc: 93.15%\n",
      "Batch 16850/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 16900/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 16950/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17000/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17050/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17100/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17150/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17200/19342, Loss: 0.1554, Acc: 93.17%\n",
      "Batch 17250/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 17300/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17350/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 17400/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17450/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17500/19342, Loss: 0.1555, Acc: 93.17%\n",
      "Batch 17550/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17600/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17650/19342, Loss: 0.1555, Acc: 93.16%\n",
      "Batch 17700/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 17750/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 17800/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 17850/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 17900/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 17950/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 18000/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 18050/19342, Loss: 0.1557, Acc: 93.16%\n",
      "Batch 18100/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 18150/19342, Loss: 0.1556, Acc: 93.16%\n",
      "Batch 18200/19342, Loss: 0.1557, Acc: 93.15%\n",
      "Batch 18250/19342, Loss: 0.1557, Acc: 93.15%\n",
      "Batch 18300/19342, Loss: 0.1557, Acc: 93.15%\n",
      "Batch 18350/19342, Loss: 0.1556, Acc: 93.15%\n",
      "Batch 18400/19342, Loss: 0.1557, Acc: 93.15%\n",
      "Batch 18450/19342, Loss: 0.1557, Acc: 93.15%\n",
      "Batch 18500/19342, Loss: 0.1558, Acc: 93.15%\n",
      "Batch 18550/19342, Loss: 0.1558, Acc: 93.14%\n",
      "Batch 18600/19342, Loss: 0.1559, Acc: 93.14%\n",
      "Batch 18650/19342, Loss: 0.1559, Acc: 93.13%\n",
      "Batch 18700/19342, Loss: 0.1559, Acc: 93.13%\n",
      "Batch 18750/19342, Loss: 0.1559, Acc: 93.13%\n",
      "Batch 18800/19342, Loss: 0.1559, Acc: 93.13%\n",
      "Batch 18850/19342, Loss: 0.1559, Acc: 93.13%\n",
      "Batch 18900/19342, Loss: 0.1558, Acc: 93.13%\n",
      "Batch 18950/19342, Loss: 0.1558, Acc: 93.13%\n",
      "Batch 19000/19342, Loss: 0.1557, Acc: 93.14%\n",
      "Batch 19050/19342, Loss: 0.1557, Acc: 93.14%\n",
      "Batch 19100/19342, Loss: 0.1557, Acc: 93.14%\n",
      "Batch 19150/19342, Loss: 0.1558, Acc: 93.13%\n",
      "Batch 19200/19342, Loss: 0.1558, Acc: 93.13%\n",
      "Batch 19250/19342, Loss: 0.1559, Acc: 93.13%\n",
      "Batch 19300/19342, Loss: 0.1559, Acc: 93.12%\n",
      "Train Loss: 0.1559, Train Acc: 93.13%\n",
      "Val Loss: 0.3304, Val Acc: 85.66%\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\n",
      "Epoch 10/30\n",
      "------------------------------------------------------------\n",
      "Batch 50/19342, Loss: 0.1403, Acc: 94.25%\n",
      "Batch 100/19342, Loss: 0.1290, Acc: 94.50%\n",
      "Batch 150/19342, Loss: 0.1265, Acc: 94.75%\n",
      "Batch 200/19342, Loss: 0.1270, Acc: 94.50%\n",
      "Batch 250/19342, Loss: 0.1342, Acc: 94.30%\n",
      "Batch 300/19342, Loss: 0.1299, Acc: 94.58%\n",
      "Batch 350/19342, Loss: 0.1304, Acc: 94.50%\n",
      "Batch 400/19342, Loss: 0.1301, Acc: 94.41%\n",
      "Batch 450/19342, Loss: 0.1292, Acc: 94.44%\n",
      "Batch 500/19342, Loss: 0.1326, Acc: 94.45%\n",
      "Batch 550/19342, Loss: 0.1314, Acc: 94.39%\n",
      "Batch 600/19342, Loss: 0.1334, Acc: 94.23%\n",
      "Batch 650/19342, Loss: 0.1355, Acc: 94.12%\n",
      "Batch 700/19342, Loss: 0.1360, Acc: 94.05%\n",
      "Batch 750/19342, Loss: 0.1381, Acc: 93.97%\n",
      "Batch 800/19342, Loss: 0.1384, Acc: 93.89%\n",
      "Batch 850/19342, Loss: 0.1367, Acc: 93.99%\n",
      "Batch 900/19342, Loss: 0.1378, Acc: 93.90%\n",
      "Batch 950/19342, Loss: 0.1369, Acc: 93.99%\n",
      "Batch 1000/19342, Loss: 0.1368, Acc: 94.01%\n",
      "Batch 1050/19342, Loss: 0.1359, Acc: 94.05%\n",
      "Batch 1100/19342, Loss: 0.1368, Acc: 93.99%\n",
      "Batch 1150/19342, Loss: 0.1360, Acc: 94.05%\n",
      "Batch 1200/19342, Loss: 0.1354, Acc: 94.05%\n",
      "Batch 1250/19342, Loss: 0.1355, Acc: 94.03%\n",
      "Batch 1300/19342, Loss: 0.1349, Acc: 94.07%\n",
      "Batch 1350/19342, Loss: 0.1341, Acc: 94.11%\n",
      "Batch 1400/19342, Loss: 0.1339, Acc: 94.06%\n",
      "Batch 1450/19342, Loss: 0.1331, Acc: 94.09%\n",
      "Batch 1500/19342, Loss: 0.1326, Acc: 94.08%\n",
      "Batch 1550/19342, Loss: 0.1323, Acc: 94.10%\n",
      "Batch 1600/19342, Loss: 0.1321, Acc: 94.16%\n",
      "Batch 1650/19342, Loss: 0.1311, Acc: 94.20%\n",
      "Batch 1700/19342, Loss: 0.1314, Acc: 94.21%\n",
      "Batch 1750/19342, Loss: 0.1319, Acc: 94.19%\n",
      "Batch 1800/19342, Loss: 0.1314, Acc: 94.24%\n",
      "Batch 1850/19342, Loss: 0.1321, Acc: 94.23%\n",
      "Batch 1900/19342, Loss: 0.1324, Acc: 94.20%\n",
      "Batch 1950/19342, Loss: 0.1322, Acc: 94.20%\n",
      "Batch 2000/19342, Loss: 0.1324, Acc: 94.18%\n",
      "Batch 2050/19342, Loss: 0.1324, Acc: 94.16%\n",
      "Batch 2100/19342, Loss: 0.1328, Acc: 94.11%\n",
      "Batch 2150/19342, Loss: 0.1329, Acc: 94.08%\n",
      "Batch 2200/19342, Loss: 0.1331, Acc: 94.06%\n",
      "Batch 2250/19342, Loss: 0.1330, Acc: 94.06%\n",
      "Batch 2300/19342, Loss: 0.1332, Acc: 94.04%\n",
      "Batch 2350/19342, Loss: 0.1331, Acc: 94.04%\n",
      "Batch 2400/19342, Loss: 0.1335, Acc: 94.04%\n",
      "Batch 2450/19342, Loss: 0.1330, Acc: 94.06%\n",
      "Batch 2500/19342, Loss: 0.1343, Acc: 94.01%\n",
      "Batch 2550/19342, Loss: 0.1343, Acc: 94.01%\n",
      "Batch 2600/19342, Loss: 0.1334, Acc: 94.07%\n",
      "Batch 2650/19342, Loss: 0.1330, Acc: 94.10%\n",
      "Batch 2700/19342, Loss: 0.1329, Acc: 94.11%\n",
      "Batch 2750/19342, Loss: 0.1330, Acc: 94.10%\n",
      "Batch 2800/19342, Loss: 0.1332, Acc: 94.09%\n",
      "Batch 2850/19342, Loss: 0.1331, Acc: 94.11%\n",
      "Batch 2900/19342, Loss: 0.1324, Acc: 94.13%\n",
      "Batch 2950/19342, Loss: 0.1323, Acc: 94.15%\n",
      "Batch 3000/19342, Loss: 0.1328, Acc: 94.10%\n",
      "Batch 3050/19342, Loss: 0.1328, Acc: 94.08%\n",
      "Batch 3100/19342, Loss: 0.1324, Acc: 94.10%\n",
      "Batch 3150/19342, Loss: 0.1321, Acc: 94.10%\n",
      "Batch 3200/19342, Loss: 0.1318, Acc: 94.12%\n",
      "Batch 3250/19342, Loss: 0.1319, Acc: 94.12%\n",
      "Batch 3300/19342, Loss: 0.1319, Acc: 94.12%\n",
      "Batch 3350/19342, Loss: 0.1321, Acc: 94.12%\n",
      "Batch 3400/19342, Loss: 0.1320, Acc: 94.12%\n",
      "Batch 3450/19342, Loss: 0.1319, Acc: 94.13%\n",
      "Batch 3500/19342, Loss: 0.1316, Acc: 94.15%\n",
      "Batch 3550/19342, Loss: 0.1316, Acc: 94.17%\n",
      "Batch 3600/19342, Loss: 0.1315, Acc: 94.16%\n",
      "Batch 3650/19342, Loss: 0.1314, Acc: 94.16%\n",
      "Batch 3700/19342, Loss: 0.1313, Acc: 94.17%\n",
      "Batch 3750/19342, Loss: 0.1313, Acc: 94.19%\n",
      "Batch 3800/19342, Loss: 0.1308, Acc: 94.22%\n",
      "Batch 3850/19342, Loss: 0.1304, Acc: 94.24%\n",
      "Batch 3900/19342, Loss: 0.1302, Acc: 94.25%\n",
      "Batch 3950/19342, Loss: 0.1299, Acc: 94.28%\n",
      "Batch 4000/19342, Loss: 0.1300, Acc: 94.28%\n",
      "Batch 4050/19342, Loss: 0.1302, Acc: 94.28%\n",
      "Batch 4100/19342, Loss: 0.1305, Acc: 94.26%\n",
      "Batch 4150/19342, Loss: 0.1307, Acc: 94.25%\n",
      "Batch 4200/19342, Loss: 0.1307, Acc: 94.26%\n",
      "Batch 4250/19342, Loss: 0.1306, Acc: 94.26%\n",
      "Batch 4300/19342, Loss: 0.1303, Acc: 94.28%\n",
      "Batch 4350/19342, Loss: 0.1300, Acc: 94.30%\n",
      "Batch 4400/19342, Loss: 0.1301, Acc: 94.28%\n",
      "Batch 4450/19342, Loss: 0.1306, Acc: 94.26%\n",
      "Batch 4500/19342, Loss: 0.1301, Acc: 94.28%\n",
      "Batch 4550/19342, Loss: 0.1300, Acc: 94.30%\n",
      "Batch 4600/19342, Loss: 0.1302, Acc: 94.29%\n",
      "Batch 4650/19342, Loss: 0.1303, Acc: 94.28%\n",
      "Batch 4700/19342, Loss: 0.1306, Acc: 94.27%\n",
      "Batch 4750/19342, Loss: 0.1302, Acc: 94.29%\n",
      "Batch 4800/19342, Loss: 0.1301, Acc: 94.29%\n",
      "Batch 4850/19342, Loss: 0.1300, Acc: 94.31%\n",
      "Batch 4900/19342, Loss: 0.1300, Acc: 94.30%\n",
      "Batch 4950/19342, Loss: 0.1302, Acc: 94.31%\n",
      "Batch 5000/19342, Loss: 0.1301, Acc: 94.31%\n",
      "Batch 5050/19342, Loss: 0.1302, Acc: 94.30%\n",
      "Batch 5100/19342, Loss: 0.1302, Acc: 94.29%\n",
      "Batch 5150/19342, Loss: 0.1299, Acc: 94.31%\n",
      "Batch 5200/19342, Loss: 0.1295, Acc: 94.33%\n",
      "Batch 5250/19342, Loss: 0.1296, Acc: 94.32%\n",
      "Batch 5300/19342, Loss: 0.1295, Acc: 94.33%\n",
      "Batch 5350/19342, Loss: 0.1297, Acc: 94.31%\n",
      "Batch 5400/19342, Loss: 0.1297, Acc: 94.31%\n",
      "Batch 5450/19342, Loss: 0.1296, Acc: 94.32%\n",
      "Batch 5500/19342, Loss: 0.1298, Acc: 94.31%\n",
      "Batch 5550/19342, Loss: 0.1298, Acc: 94.31%\n",
      "Batch 5600/19342, Loss: 0.1297, Acc: 94.31%\n",
      "Batch 5650/19342, Loss: 0.1297, Acc: 94.31%\n",
      "Batch 5700/19342, Loss: 0.1297, Acc: 94.32%\n",
      "Batch 5750/19342, Loss: 0.1300, Acc: 94.30%\n",
      "Batch 5800/19342, Loss: 0.1299, Acc: 94.30%\n",
      "Batch 5850/19342, Loss: 0.1301, Acc: 94.29%\n",
      "Batch 5900/19342, Loss: 0.1301, Acc: 94.28%\n",
      "Batch 5950/19342, Loss: 0.1303, Acc: 94.28%\n",
      "Batch 6000/19342, Loss: 0.1302, Acc: 94.28%\n",
      "Batch 6050/19342, Loss: 0.1302, Acc: 94.28%\n",
      "Batch 6100/19342, Loss: 0.1301, Acc: 94.28%\n",
      "Batch 6150/19342, Loss: 0.1302, Acc: 94.28%\n",
      "Batch 6200/19342, Loss: 0.1301, Acc: 94.28%\n",
      "Batch 6250/19342, Loss: 0.1300, Acc: 94.29%\n",
      "Batch 6300/19342, Loss: 0.1299, Acc: 94.28%\n",
      "Batch 6350/19342, Loss: 0.1300, Acc: 94.28%\n",
      "Batch 6400/19342, Loss: 0.1298, Acc: 94.29%\n",
      "Batch 6450/19342, Loss: 0.1297, Acc: 94.31%\n",
      "Batch 6500/19342, Loss: 0.1294, Acc: 94.32%\n",
      "Batch 6550/19342, Loss: 0.1295, Acc: 94.31%\n",
      "Batch 6600/19342, Loss: 0.1295, Acc: 94.32%\n",
      "Batch 6650/19342, Loss: 0.1295, Acc: 94.32%\n",
      "Batch 6700/19342, Loss: 0.1296, Acc: 94.33%\n",
      "Batch 6750/19342, Loss: 0.1295, Acc: 94.33%\n",
      "Batch 6800/19342, Loss: 0.1294, Acc: 94.34%\n",
      "Batch 6850/19342, Loss: 0.1295, Acc: 94.34%\n",
      "Batch 6900/19342, Loss: 0.1296, Acc: 94.33%\n",
      "Batch 6950/19342, Loss: 0.1297, Acc: 94.33%\n",
      "Batch 7000/19342, Loss: 0.1299, Acc: 94.33%\n",
      "Batch 7050/19342, Loss: 0.1298, Acc: 94.33%\n",
      "Batch 7100/19342, Loss: 0.1298, Acc: 94.33%\n",
      "Batch 7150/19342, Loss: 0.1297, Acc: 94.33%\n",
      "Batch 7200/19342, Loss: 0.1297, Acc: 94.33%\n",
      "Batch 7250/19342, Loss: 0.1297, Acc: 94.32%\n",
      "Batch 7300/19342, Loss: 0.1297, Acc: 94.33%\n",
      "Batch 7350/19342, Loss: 0.1298, Acc: 94.33%\n",
      "Batch 7400/19342, Loss: 0.1297, Acc: 94.34%\n",
      "Batch 7450/19342, Loss: 0.1295, Acc: 94.36%\n",
      "Batch 7500/19342, Loss: 0.1294, Acc: 94.36%\n",
      "Batch 7550/19342, Loss: 0.1291, Acc: 94.37%\n",
      "Batch 7600/19342, Loss: 0.1290, Acc: 94.38%\n",
      "Batch 7650/19342, Loss: 0.1290, Acc: 94.37%\n",
      "Batch 7700/19342, Loss: 0.1290, Acc: 94.38%\n",
      "Batch 7750/19342, Loss: 0.1291, Acc: 94.37%\n",
      "Batch 7800/19342, Loss: 0.1289, Acc: 94.38%\n",
      "Batch 7850/19342, Loss: 0.1288, Acc: 94.39%\n",
      "Batch 7900/19342, Loss: 0.1285, Acc: 94.41%\n",
      "Batch 7950/19342, Loss: 0.1286, Acc: 94.41%\n",
      "Batch 8000/19342, Loss: 0.1286, Acc: 94.41%\n",
      "Batch 8050/19342, Loss: 0.1284, Acc: 94.42%\n",
      "Batch 8100/19342, Loss: 0.1284, Acc: 94.41%\n",
      "Batch 8150/19342, Loss: 0.1284, Acc: 94.42%\n",
      "Batch 8200/19342, Loss: 0.1283, Acc: 94.42%\n",
      "Batch 8250/19342, Loss: 0.1281, Acc: 94.42%\n",
      "Batch 8300/19342, Loss: 0.1283, Acc: 94.42%\n",
      "Batch 8350/19342, Loss: 0.1283, Acc: 94.41%\n",
      "Batch 8400/19342, Loss: 0.1284, Acc: 94.41%\n",
      "Batch 8450/19342, Loss: 0.1284, Acc: 94.41%\n",
      "Batch 8500/19342, Loss: 0.1282, Acc: 94.42%\n",
      "Batch 8550/19342, Loss: 0.1281, Acc: 94.43%\n",
      "Batch 8600/19342, Loss: 0.1279, Acc: 94.43%\n",
      "Batch 8650/19342, Loss: 0.1279, Acc: 94.44%\n",
      "Batch 8700/19342, Loss: 0.1278, Acc: 94.44%\n",
      "Batch 8750/19342, Loss: 0.1279, Acc: 94.44%\n",
      "Batch 8800/19342, Loss: 0.1280, Acc: 94.44%\n",
      "Batch 8850/19342, Loss: 0.1279, Acc: 94.44%\n",
      "Batch 8900/19342, Loss: 0.1281, Acc: 94.42%\n",
      "Batch 8950/19342, Loss: 0.1280, Acc: 94.42%\n",
      "Batch 9000/19342, Loss: 0.1279, Acc: 94.42%\n",
      "Batch 9050/19342, Loss: 0.1279, Acc: 94.42%\n",
      "Batch 9100/19342, Loss: 0.1279, Acc: 94.42%\n",
      "Batch 9150/19342, Loss: 0.1277, Acc: 94.43%\n",
      "Batch 9200/19342, Loss: 0.1277, Acc: 94.42%\n",
      "Batch 9250/19342, Loss: 0.1278, Acc: 94.42%\n",
      "Batch 9300/19342, Loss: 0.1278, Acc: 94.42%\n",
      "Batch 9350/19342, Loss: 0.1279, Acc: 94.43%\n",
      "Batch 9400/19342, Loss: 0.1281, Acc: 94.41%\n",
      "Batch 9450/19342, Loss: 0.1280, Acc: 94.42%\n",
      "Batch 9500/19342, Loss: 0.1282, Acc: 94.41%\n",
      "Batch 9550/19342, Loss: 0.1281, Acc: 94.40%\n",
      "Batch 9600/19342, Loss: 0.1283, Acc: 94.40%\n",
      "Batch 9650/19342, Loss: 0.1284, Acc: 94.40%\n",
      "Batch 9700/19342, Loss: 0.1284, Acc: 94.40%\n",
      "Batch 9750/19342, Loss: 0.1283, Acc: 94.40%\n",
      "Batch 9800/19342, Loss: 0.1282, Acc: 94.41%\n",
      "Batch 9850/19342, Loss: 0.1283, Acc: 94.41%\n",
      "Batch 9900/19342, Loss: 0.1283, Acc: 94.41%\n",
      "Batch 9950/19342, Loss: 0.1283, Acc: 94.41%\n",
      "Batch 10000/19342, Loss: 0.1283, Acc: 94.41%\n",
      "Batch 10050/19342, Loss: 0.1285, Acc: 94.39%\n",
      "Batch 10100/19342, Loss: 0.1286, Acc: 94.39%\n",
      "Batch 10150/19342, Loss: 0.1286, Acc: 94.38%\n",
      "Batch 10200/19342, Loss: 0.1288, Acc: 94.38%\n",
      "Batch 10250/19342, Loss: 0.1288, Acc: 94.38%\n",
      "Batch 10300/19342, Loss: 0.1288, Acc: 94.38%\n",
      "Batch 10350/19342, Loss: 0.1286, Acc: 94.39%\n",
      "Batch 10400/19342, Loss: 0.1286, Acc: 94.38%\n",
      "Batch 10450/19342, Loss: 0.1285, Acc: 94.39%\n",
      "Batch 10500/19342, Loss: 0.1285, Acc: 94.39%\n",
      "Batch 10550/19342, Loss: 0.1286, Acc: 94.38%\n",
      "Batch 10600/19342, Loss: 0.1286, Acc: 94.38%\n",
      "Batch 10650/19342, Loss: 0.1285, Acc: 94.39%\n",
      "Batch 10700/19342, Loss: 0.1286, Acc: 94.38%\n",
      "Batch 10750/19342, Loss: 0.1285, Acc: 94.38%\n",
      "Batch 10800/19342, Loss: 0.1285, Acc: 94.38%\n",
      "Batch 10850/19342, Loss: 0.1284, Acc: 94.39%\n",
      "Batch 10900/19342, Loss: 0.1284, Acc: 94.39%\n",
      "Batch 10950/19342, Loss: 0.1284, Acc: 94.38%\n",
      "Batch 11000/19342, Loss: 0.1284, Acc: 94.39%\n",
      "Batch 11050/19342, Loss: 0.1283, Acc: 94.40%\n",
      "Batch 11100/19342, Loss: 0.1283, Acc: 94.39%\n",
      "Batch 11150/19342, Loss: 0.1283, Acc: 94.39%\n",
      "Batch 11200/19342, Loss: 0.1283, Acc: 94.39%\n",
      "Batch 11250/19342, Loss: 0.1283, Acc: 94.39%\n",
      "Batch 11300/19342, Loss: 0.1282, Acc: 94.39%\n",
      "Batch 11350/19342, Loss: 0.1280, Acc: 94.40%\n",
      "Batch 11400/19342, Loss: 0.1280, Acc: 94.40%\n",
      "Batch 11450/19342, Loss: 0.1281, Acc: 94.40%\n",
      "Batch 11500/19342, Loss: 0.1280, Acc: 94.40%\n",
      "Batch 11550/19342, Loss: 0.1281, Acc: 94.40%\n",
      "Batch 11600/19342, Loss: 0.1280, Acc: 94.41%\n",
      "Batch 11650/19342, Loss: 0.1280, Acc: 94.41%\n",
      "Batch 11700/19342, Loss: 0.1279, Acc: 94.42%\n",
      "Batch 11750/19342, Loss: 0.1280, Acc: 94.41%\n",
      "Batch 11800/19342, Loss: 0.1279, Acc: 94.41%\n",
      "Batch 11850/19342, Loss: 0.1280, Acc: 94.41%\n",
      "Batch 11900/19342, Loss: 0.1279, Acc: 94.41%\n",
      "Batch 11950/19342, Loss: 0.1279, Acc: 94.42%\n",
      "Batch 12000/19342, Loss: 0.1278, Acc: 94.43%\n",
      "Batch 12050/19342, Loss: 0.1278, Acc: 94.43%\n",
      "Batch 12100/19342, Loss: 0.1277, Acc: 94.43%\n",
      "Batch 12150/19342, Loss: 0.1275, Acc: 94.44%\n",
      "Batch 12200/19342, Loss: 0.1275, Acc: 94.43%\n",
      "Batch 12250/19342, Loss: 0.1275, Acc: 94.43%\n",
      "Batch 12300/19342, Loss: 0.1274, Acc: 94.44%\n",
      "Batch 12350/19342, Loss: 0.1274, Acc: 94.43%\n",
      "Batch 12400/19342, Loss: 0.1274, Acc: 94.44%\n",
      "Batch 12450/19342, Loss: 0.1273, Acc: 94.43%\n",
      "Batch 12500/19342, Loss: 0.1274, Acc: 94.44%\n",
      "Batch 12550/19342, Loss: 0.1273, Acc: 94.44%\n",
      "Batch 12600/19342, Loss: 0.1273, Acc: 94.44%\n",
      "Batch 12650/19342, Loss: 0.1273, Acc: 94.44%\n",
      "Batch 12700/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 12750/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 12800/19342, Loss: 0.1271, Acc: 94.45%\n",
      "Batch 12850/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 12900/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 12950/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 13000/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 13050/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 13100/19342, Loss: 0.1273, Acc: 94.44%\n",
      "Batch 13150/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 13200/19342, Loss: 0.1271, Acc: 94.45%\n",
      "Batch 13250/19342, Loss: 0.1271, Acc: 94.45%\n",
      "Batch 13300/19342, Loss: 0.1271, Acc: 94.45%\n",
      "Batch 13350/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 13400/19342, Loss: 0.1273, Acc: 94.45%\n",
      "Batch 13450/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 13500/19342, Loss: 0.1272, Acc: 94.45%\n",
      "Batch 13550/19342, Loss: 0.1271, Acc: 94.45%\n",
      "Batch 13600/19342, Loss: 0.1270, Acc: 94.46%\n",
      "Batch 13650/19342, Loss: 0.1270, Acc: 94.45%\n",
      "Batch 13700/19342, Loss: 0.1270, Acc: 94.45%\n",
      "Batch 13750/19342, Loss: 0.1269, Acc: 94.46%\n",
      "Batch 13800/19342, Loss: 0.1269, Acc: 94.45%\n",
      "Batch 13850/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 13900/19342, Loss: 0.1269, Acc: 94.45%\n",
      "Batch 13950/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14000/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14050/19342, Loss: 0.1269, Acc: 94.45%\n",
      "Batch 14100/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14150/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14200/19342, Loss: 0.1267, Acc: 94.46%\n",
      "Batch 14250/19342, Loss: 0.1267, Acc: 94.46%\n",
      "Batch 14300/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14350/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14400/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14450/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14500/19342, Loss: 0.1268, Acc: 94.45%\n",
      "Batch 14550/19342, Loss: 0.1267, Acc: 94.45%\n",
      "Batch 14600/19342, Loss: 0.1267, Acc: 94.46%\n",
      "Batch 14650/19342, Loss: 0.1268, Acc: 94.46%\n",
      "Batch 14700/19342, Loss: 0.1269, Acc: 94.45%\n",
      "Batch 14750/19342, Loss: 0.1269, Acc: 94.45%\n",
      "Batch 14800/19342, Loss: 0.1269, Acc: 94.45%\n",
      "Batch 14850/19342, Loss: 0.1269, Acc: 94.46%\n",
      "Batch 14900/19342, Loss: 0.1268, Acc: 94.46%\n",
      "Batch 14950/19342, Loss: 0.1268, Acc: 94.46%\n",
      "Batch 15000/19342, Loss: 0.1268, Acc: 94.46%\n",
      "Batch 15050/19342, Loss: 0.1266, Acc: 94.47%\n",
      "Batch 15100/19342, Loss: 0.1267, Acc: 94.47%\n",
      "Batch 15150/19342, Loss: 0.1267, Acc: 94.47%\n",
      "Batch 15200/19342, Loss: 0.1266, Acc: 94.48%\n",
      "Batch 15250/19342, Loss: 0.1266, Acc: 94.48%\n",
      "Batch 15300/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 15350/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 15400/19342, Loss: 0.1266, Acc: 94.48%\n",
      "Batch 15450/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 15500/19342, Loss: 0.1266, Acc: 94.49%\n",
      "Batch 15550/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 15600/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 15650/19342, Loss: 0.1266, Acc: 94.49%\n",
      "Batch 15700/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 15750/19342, Loss: 0.1265, Acc: 94.50%\n",
      "Batch 15800/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 15850/19342, Loss: 0.1266, Acc: 94.49%\n",
      "Batch 15900/19342, Loss: 0.1266, Acc: 94.49%\n",
      "Batch 15950/19342, Loss: 0.1265, Acc: 94.49%\n",
      "Batch 16000/19342, Loss: 0.1264, Acc: 94.50%\n",
      "Batch 16050/19342, Loss: 0.1263, Acc: 94.50%\n",
      "Batch 16100/19342, Loss: 0.1263, Acc: 94.50%\n",
      "Batch 16150/19342, Loss: 0.1263, Acc: 94.50%\n",
      "Batch 16200/19342, Loss: 0.1263, Acc: 94.50%\n",
      "Batch 16250/19342, Loss: 0.1263, Acc: 94.51%\n",
      "Batch 16300/19342, Loss: 0.1262, Acc: 94.51%\n",
      "Batch 16350/19342, Loss: 0.1262, Acc: 94.51%\n",
      "Batch 16400/19342, Loss: 0.1262, Acc: 94.51%\n",
      "Batch 16450/19342, Loss: 0.1263, Acc: 94.51%\n",
      "Batch 16500/19342, Loss: 0.1261, Acc: 94.52%\n",
      "Batch 16550/19342, Loss: 0.1260, Acc: 94.52%\n",
      "Batch 16600/19342, Loss: 0.1261, Acc: 94.52%\n",
      "Batch 16650/19342, Loss: 0.1260, Acc: 94.52%\n",
      "Batch 16700/19342, Loss: 0.1260, Acc: 94.52%\n",
      "Batch 16750/19342, Loss: 0.1259, Acc: 94.52%\n",
      "Batch 16800/19342, Loss: 0.1260, Acc: 94.52%\n",
      "Batch 16850/19342, Loss: 0.1259, Acc: 94.53%\n",
      "Batch 16900/19342, Loss: 0.1258, Acc: 94.53%\n",
      "Batch 16950/19342, Loss: 0.1258, Acc: 94.53%\n",
      "Batch 17000/19342, Loss: 0.1259, Acc: 94.53%\n",
      "Batch 17050/19342, Loss: 0.1258, Acc: 94.53%\n",
      "Batch 17100/19342, Loss: 0.1258, Acc: 94.54%\n",
      "Batch 17150/19342, Loss: 0.1258, Acc: 94.54%\n",
      "Batch 17200/19342, Loss: 0.1258, Acc: 94.54%\n",
      "Batch 17250/19342, Loss: 0.1259, Acc: 94.54%\n",
      "Batch 17300/19342, Loss: 0.1259, Acc: 94.54%\n",
      "Batch 17350/19342, Loss: 0.1259, Acc: 94.53%\n",
      "Batch 17400/19342, Loss: 0.1260, Acc: 94.53%\n",
      "Batch 17450/19342, Loss: 0.1260, Acc: 94.53%\n",
      "Batch 17500/19342, Loss: 0.1260, Acc: 94.53%\n",
      "Batch 17550/19342, Loss: 0.1259, Acc: 94.53%\n",
      "Batch 17600/19342, Loss: 0.1259, Acc: 94.53%\n",
      "Batch 17650/19342, Loss: 0.1260, Acc: 94.53%\n",
      "Batch 17700/19342, Loss: 0.1261, Acc: 94.52%\n",
      "Batch 17750/19342, Loss: 0.1260, Acc: 94.53%\n",
      "Batch 17800/19342, Loss: 0.1259, Acc: 94.53%\n",
      "Batch 17850/19342, Loss: 0.1259, Acc: 94.54%\n",
      "Batch 17900/19342, Loss: 0.1258, Acc: 94.54%\n",
      "Batch 17950/19342, Loss: 0.1257, Acc: 94.54%\n",
      "Batch 18000/19342, Loss: 0.1257, Acc: 94.55%\n",
      "Batch 18050/19342, Loss: 0.1256, Acc: 94.55%\n",
      "Batch 18100/19342, Loss: 0.1256, Acc: 94.55%\n",
      "Batch 18150/19342, Loss: 0.1257, Acc: 94.54%\n",
      "Batch 18200/19342, Loss: 0.1256, Acc: 94.54%\n",
      "Batch 18250/19342, Loss: 0.1256, Acc: 94.54%\n",
      "Batch 18300/19342, Loss: 0.1256, Acc: 94.54%\n",
      "Batch 18350/19342, Loss: 0.1257, Acc: 94.54%\n",
      "Batch 18400/19342, Loss: 0.1256, Acc: 94.54%\n",
      "Batch 18450/19342, Loss: 0.1256, Acc: 94.54%\n",
      "Batch 18500/19342, Loss: 0.1255, Acc: 94.55%\n",
      "Batch 18550/19342, Loss: 0.1255, Acc: 94.55%\n",
      "Batch 18600/19342, Loss: 0.1254, Acc: 94.55%\n",
      "Batch 18650/19342, Loss: 0.1254, Acc: 94.55%\n",
      "Batch 18700/19342, Loss: 0.1254, Acc: 94.55%\n",
      "Batch 18750/19342, Loss: 0.1254, Acc: 94.55%\n",
      "Batch 18800/19342, Loss: 0.1255, Acc: 94.55%\n",
      "Batch 18850/19342, Loss: 0.1255, Acc: 94.55%\n",
      "Batch 18900/19342, Loss: 0.1255, Acc: 94.55%\n",
      "Batch 18950/19342, Loss: 0.1255, Acc: 94.55%\n",
      "Batch 19000/19342, Loss: 0.1254, Acc: 94.56%\n",
      "Batch 19050/19342, Loss: 0.1253, Acc: 94.56%\n",
      "Batch 19100/19342, Loss: 0.1253, Acc: 94.56%\n",
      "Batch 19150/19342, Loss: 0.1254, Acc: 94.56%\n",
      "Batch 19200/19342, Loss: 0.1254, Acc: 94.56%\n",
      "Batch 19250/19342, Loss: 0.1254, Acc: 94.56%\n",
      "Batch 19300/19342, Loss: 0.1254, Acc: 94.57%\n",
      "Train Loss: 0.1254, Train Acc: 94.57%\n",
      "Val Loss: 0.4461, Val Acc: 84.36%\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping triggered\n",
      "Training completed in 15h 0m 56s\n",
      "\n",
      "Evaluating on test set...\n",
      "Loaded best model from best_densenet_model.pth\n",
      "Test Accuracy: 85.62%\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Healthy Control       0.79      0.93      0.85     18834\n",
      "Parkinson's Disease       0.94      0.79      0.86     23052\n",
      "\n",
      "           accuracy                           0.86     41886\n",
      "          macro avg       0.86      0.86      0.86     41886\n",
      "       weighted avg       0.87      0.86      0.86     41886\n",
      "\n",
      "Complete! Check the saved model and evaluation outputs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "CHECKPOINT_PATH = 'best_densenet_model.pth'\n",
    "\n",
    "# Model definition\n",
    "def create_densenet_model(num_classes=2):\n",
    "    # Using DenseNet121 as base model (smaller than DenseNet169/201 to conserve VRAM)\n",
    "    model = models.densenet121(weights='DEFAULT')\n",
    "    \n",
    "    # Modify first conv layer to accept grayscale images (1 channel)\n",
    "    # We'll convert the first conv layer to accept 1 channel instead of 3\n",
    "    first_conv = model.features.conv0\n",
    "    model.features.conv0 = nn.Conv2d(\n",
    "        1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "    )\n",
    "    # Initialize with weight averaging from the pre-trained 3-channel weights\n",
    "    with torch.no_grad():\n",
    "        model.features.conv0.weight.data = first_conv.weight.data.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    # Replace classifier\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),  # Adding dropout for regularization\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Trainer class\n",
    "class ParkinsonsTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, device, \n",
    "                 criterion=nn.CrossEntropyLoss(), learning_rate=1e-4, \n",
    "                 weight_decay=1e-5, checkpoint_path='best_model.pth'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.best_val_acc = 0.0\n",
    "        self.early_stopping_counter = 0\n",
    "        \n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Clear memory\n",
    "            if batch_idx % 20 == 0:  # Clear every 20 batches\n",
    "                del inputs, targets, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Print progress every 50 batches\n",
    "            if batch_idx % 50 == 49:\n",
    "                print(f'Batch {batch_idx+1}/{len(self.train_loader)}, Loss: {running_loss/(batch_idx+1):.4f}, '\n",
    "                      f'Acc: {100.*correct/total:.2f}%')\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.val_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                # Clear memory\n",
    "                del inputs, targets, outputs\n",
    "                \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        val_loss = running_loss / len(self.val_loader)\n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print(f\"Saving checkpoint with validation accuracy: {self.best_val_acc:.2f}%\")\n",
    "        torch.save(self.model.state_dict(), self.checkpoint_path)\n",
    "    \n",
    "    def train(self, num_epochs, early_stopping_patience=5):\n",
    "        start_time = time.time()\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs = [], []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print('-' * 60)\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_one_epoch()\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = self.validate()\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            # Update learning rate based on validation accuracy\n",
    "            self.scheduler.step(val_acc)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Save checkpoint if validation accuracy improves\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.save_checkpoint()\n",
    "                self.early_stopping_counter = 0\n",
    "            else:\n",
    "                self.early_stopping_counter += 1\n",
    "                print(f\"EarlyStopping counter: {self.early_stopping_counter} out of {early_stopping_patience}\")\n",
    "                \n",
    "                if self.early_stopping_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        hours, remainder = divmod(training_time, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        print(f\"Training completed in {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "        \n",
    "        # Plot training/validation metrics\n",
    "        self.plot_training_metrics(train_losses, val_losses, train_accs, val_accs)\n",
    "        \n",
    "        return train_losses, train_accs, val_losses, val_accs\n",
    "    \n",
    "    def plot_training_metrics(self, train_losses, val_losses, train_accs, val_accs):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot accuracies\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='Train Accuracy')\n",
    "        plt.plot(val_accs, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def load_best_model(self):\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "            print(f\"Loaded best model from {self.checkpoint_path}\")\n",
    "            return True\n",
    "        except:\n",
    "            print(\"Could not load checkpoint. Using current model state.\")\n",
    "            return False\n",
    "    \n",
    "    def test(self):\n",
    "        # Load best model for testing\n",
    "        self.load_best_model()\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.test_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                probs = nn.Softmax(dim=1)(outputs)\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = outputs.max(1)\n",
    "                \n",
    "                # Store for evaluation\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                all_probs.extend(probs[:, 1].cpu().numpy())  # Store probability of class 1 (PD)\n",
    "                \n",
    "                # Clear memory\n",
    "                del inputs, targets, outputs\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_acc = 100. * np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "        print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        report = classification_report(all_targets, all_preds, \n",
    "                                       target_names=['Healthy Control', 'Parkinson\\'s Disease'])\n",
    "        print(report)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        self.plot_confusion_matrix(cm)\n",
    "        \n",
    "        # ROC Curve\n",
    "        self.plot_roc_curve(all_targets, all_probs)\n",
    "        \n",
    "        return test_acc, all_preds, all_targets, all_probs\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Healthy Control', 'Parkinson\\'s Disease'],\n",
    "                    yticklabels=['Healthy Control', 'Parkinson\\'s Disease'])\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_roc_curve(self, y_true, y_score):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "# Memory optimization functions\n",
    "def optimize_memory():\n",
    "    # Empty CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # Collect garbage\n",
    "    gc.collect()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Check available memory before starting\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Available GPU memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB reserved\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_densenet_model(num_classes=2)\n",
    "    print(\"Model created\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ParkinsonsTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=DEVICE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        checkpoint_path=CHECKPOINT_PATH\n",
    "    )\n",
    "    print(\"Trainer initialized\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train(num_epochs=NUM_EPOCHS, early_stopping_patience=EARLY_STOPPING_PATIENCE)\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_acc, all_preds, all_targets, all_probs = trainer.test()\n",
    "    \n",
    "    # Save predictions for further analysis\n",
    "    np.save('test_predictions.npy', {\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probs\n",
    "    })\n",
    "    \n",
    "    print(\"Complete! Check the saved model and evaluation outputs.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Run main function\n",
    "    try:\n",
    "        main()\n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e):\n",
    "            print(\"\\n🚨 CUDA OUT OF MEMORY ERROR\")\n",
    "            print(\"Try one of these solutions:\")\n",
    "            print(\"1. Reduce batch size (e.g., BATCH_SIZE = 8 or 4)\")\n",
    "            print(\"2. Use a smaller DenseNet variant (e.g., DenseNet121 instead of 169/201)\")\n",
    "            print(\"3. Resize images to smaller dimensions in your data transforms\")\n",
    "            print(\"4. Add gradient accumulation to simulate larger batch sizes\")\n",
    "            optimize_memory()\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686693f0-eab7-4491-9f5e-85b5448d6661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9040129-0f64-4509-9fc2-117c4f399825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9245ed0-3541-4135-bd46-227ff6142375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027eea1b-e0ad-4bdb-b3bb-af5a45b186de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846af152-33c9-4968-bf02-d40a6ffde296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd322c4-6954-4b47-bc00-fab652b8fcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
