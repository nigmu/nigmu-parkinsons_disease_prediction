{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa093fb-9951-46c2-9f5b-f8faf785e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41744 samples for train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7743c90ccd40>>       | 3399/41744 [06:47<1:03:45, 10.02it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nigmu/pytorch_env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7743c90ccd40>>       | 3429/41744 [06:50<1:03:26, 10.06it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nigmu/pytorch_env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7743c90ccd40>>       | 3431/41744 [06:50<1:04:51,  9.85it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nigmu/pytorch_env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7743c90ccd40>>       | 3435/41744 [06:51<1:04:19,  9.93it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nigmu/pytorch_env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Calculating stats:   8%|██████████▎                                                                                                                 | 3468/41744 [06:54<1:08:57,  9.25it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "class MDVRDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.classes = ['HC', 'PD']\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        self._load_paths()\n",
    "        \n",
    "    def _load_paths(self):\n",
    "        split_dir = os.path.join(self.root_dir, self.split)\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            for patient_folder in os.listdir(class_dir):\n",
    "                patient_path = os.path.join(class_dir, patient_folder)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    self.samples.extend([\n",
    "                        (os.path.join(patient_path, img_name), self.class_to_idx[class_name])\n",
    "                        for img_name in os.listdir(patient_path)\n",
    "                        if img_name.endswith(('.png', '.jpg', '.jpeg'))\n",
    "                    ])\n",
    "        print(f\"Found {len(self.samples)} samples for {self.split} split\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class SimpleAugment:\n",
    "    def __init__(self, freq_mask=15, time_mask=25):\n",
    "        self.freq_mask = freq_mask\n",
    "        self.time_mask = time_mask\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        _, h, w = img_tensor.shape\n",
    "        \n",
    "        # Frequency masking\n",
    "        f = np.random.randint(1, self.freq_mask)\n",
    "        f0 = np.random.randint(0, h - f)\n",
    "        img_tensor[:, f0:f0+f, :] = 0\n",
    "            \n",
    "        # Time masking\n",
    "        t = np.random.randint(1, self.time_mask)\n",
    "        t0 = np.random.randint(0, w - t)\n",
    "        img_tensor[:, :, t0:t0+t] = 0\n",
    "            \n",
    "        return transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "class LightCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def compute_stats_safely(dataset):\n",
    "    mean = 0.0\n",
    "    mean2 = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for img, _ in tqdm(dataset, desc=\"Calculating stats\"):\n",
    "        img = img.view(-1).float()\n",
    "        batch_pixels = img.numel()\n",
    "        batch_mean = torch.mean(img)\n",
    "        batch_mean2 = torch.mean(img**2)\n",
    "        \n",
    "        delta = batch_mean - mean\n",
    "        mean += delta * batch_pixels / (count + batch_pixels)\n",
    "        \n",
    "        delta2 = batch_mean2 - mean2\n",
    "        mean2 += delta2 * batch_pixels / (count + batch_pixels)\n",
    "        \n",
    "        count += batch_pixels\n",
    "        \n",
    "        del img, batch_mean, batch_mean2\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    std = np.sqrt(max(mean2 - mean**2, 0.0))\n",
    "    return mean.item(), max(std, 0.5)\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, device='cuda', patience=5):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            pbar = tqdm(dataloaders[phase], desc=f'{phase} phase')\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with autocast():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, 1)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels).item()\n",
    "                    \n",
    "                    del outputs, preds\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                del inputs, labels, loss\n",
    "                gc.collect()\n",
    "                pbar.set_postfix({'loss': running_loss/((pbar.n+1)*inputs.size(0))})\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_acc)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "                    early_stop_counter = 0\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "            \n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc)\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "            \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, history\n",
    "\n",
    "def run_experiment(data_dir, batch_size=16, num_epochs=50):\n",
    "    # Calculate dataset statistics safely\n",
    "    temp_dataset = MDVRDataset(data_dir, 'train', transforms.Compose([\n",
    "        transforms.Resize((496, 200)), \n",
    "        transforms.ToTensor()\n",
    "    ]))\n",
    "    mean, std = compute_stats_safely(temp_dataset)\n",
    "    print(f\"Dataset stats - Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "    del temp_dataset\n",
    "    gc.collect()\n",
    "    \n",
    "    # Simplified transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((496, 200)),\n",
    "        SimpleAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((496, 200)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MDVRDataset(data_dir, 'train', train_transform)\n",
    "    val_dataset = MDVRDataset(data_dir, 'val', test_transform)\n",
    "    test_dataset = MDVRDataset(data_dir, 'test', test_transform)\n",
    "    \n",
    "    # Memory-safe dataloaders\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True, persistent_workers=False),\n",
    "        'val': DataLoader(val_dataset, batch_size, num_workers=2),\n",
    "        'test': DataLoader(test_dataset, batch_size, num_workers=2)\n",
    "    }\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LightCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    # Train with memory monitoring\n",
    "    model, history = train_model(\n",
    "        model, dataloaders, criterion, optimizer, \n",
    "        num_epochs=num_epochs, patience=7\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Val')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Val')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history = run_experiment(\n",
    "        # data_dir=\"C:/NPersonal/Projects/SDP/Prediction Stuff/Dataset/MDVR\",\n",
    "        data_dir=r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR\",\n",
    "        batch_size=16,\n",
    "        num_epochs=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78de53-787a-41fa-a0b1-8e8ca4533f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7a093-87bc-49b9-b392-d2797abd3cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
