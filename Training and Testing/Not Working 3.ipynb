{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c22cfc-f6ad-4889-af87-bf9c7d728cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 154732 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/train', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/train']\n",
      "✅ Loaded 37807 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/val', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/val']\n",
      "✅ Loaded 41886 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/test', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/test']\n",
      "✅ DataLoaders ready! Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOTS = [\n",
    "    r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR\",\n",
    "    r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian\"\n",
    "]\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4  # Optimize based on CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset Class|\n",
    "class ParkinsonSpectrogramDataset(Dataset):\n",
    "    def __init__(self, root_dirs, transform=None):\n",
    "        if isinstance(root_dirs, str):  # If a single path is given, convert it to a list\n",
    "            root_dirs = [root_dirs]\n",
    "        self.root_dirs = root_dirs\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for root_dir in self.root_dirs:\n",
    "            for class_name in ['HC', 'PD']:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                if not os.path.exists(class_dir):\n",
    "                    continue\n",
    "\n",
    "                # Traverse subfolders inside HC/PD\n",
    "                for patient_folder in os.listdir(class_dir):\n",
    "                    patient_path = os.path.join(class_dir, patient_folder)\n",
    "                    if os.path.isdir(patient_path):  # Ensure it's a directory\n",
    "                        for img_file in os.listdir(patient_path):\n",
    "                            if img_file.lower().endswith('.png'):  # Only PNG images\n",
    "                                img_path = os.path.join(patient_path, img_file)\n",
    "                                samples.append((img_path, 0 if class_name == 'HC' else 1))\n",
    "\n",
    "        print(f\"✅ Loaded {len(samples)} samples from {self.root_dirs}\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Data Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #Image reduced from 496x200 to 224x224\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Only slight translations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize between -1 and 1\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #Image reduced from 496x200 to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Create Datasets (Loading from both MDVR & Italian datasets)\n",
    "train_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'train') for root in DATA_ROOTS], transform=train_transform)\n",
    "val_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'val') for root in DATA_ROOTS], transform=test_transform)\n",
    "test_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'test') for root in DATA_ROOTS], transform=test_transform)\n",
    "\n",
    "# Optimized DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"✅ DataLoaders ready! Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbc495e-f79a-4258-8b86-6a6e3687e6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1650\n",
      "Total GPU memory: 3.90 GB\n",
      "Available GPU memory: 0.00 GB used, 3.90 GB free\n",
      "Loading data loaders...\n",
      "Calculated class weights: [1.111978337050016, 0.9085111920874545]\n",
      "Model created\n",
      "Using weighted CrossEntropyLoss with weights: tensor([1.1120, 0.9085], device='cuda:0')\n",
      "Trainer initialized\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/40\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%| | 42/4836 [01:07<2:08:27,  1.61s/it, loss=0.605, acc=66.4, time/\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU memory\n",
    "# LEARNING_RATE = 3e-4\n",
    "LEARNING_RATE = 1e-4  # Reduced from 3e-4\n",
    "WEIGHT_DECAY = 2e-4\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "CHECKPOINT_PATH = 'best_densenet_model3.pth'\n",
    "USE_MIXED_PRECISION = True  # Enable mixed precision training\n",
    "USE_DATA_AUGMENTATION = True  # Enable more data augmentation\n",
    "USE_WEIGHTED_LOSS = True  # Handle class imbalance\n",
    "\n",
    "# Model definition\n",
    "def create_densenet_model(num_classes=2, dropout_rate=0.3):\n",
    "    # Using DenseNet121 as base model\n",
    "    model = models.densenet121(weights='DEFAULT')\n",
    "    \n",
    "    # Modify first conv layer to accept grayscale images (1 channel)\n",
    "    first_conv = model.features.conv0\n",
    "    model.features.conv0 = nn.Conv2d(\n",
    "        1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "    )\n",
    "    # Initialize with weight averaging from the pre-trained 3-channel weights\n",
    "    with torch.no_grad():\n",
    "        model.features.conv0.weight.data = first_conv.weight.data.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    # Replace classifier with more robust version\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.BatchNorm1d(in_features),  # Add batch normalization for better stability\n",
    "        nn.Dropout(dropout_rate),  # Increased dropout for regularization\n",
    "        nn.Linear(in_features, 512),  # Additional FC layer\n",
    "        # nn.ReLU(inplace=True),\n",
    "        # nn.BatchNorm1d(512),\n",
    "        # nn.Dropout(dropout_rate * 0.8),  # Slightly reduced dropout\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "\n",
    "    # model.classifier = nn.Linear(in_features, num_classes)  # Simple linear layer\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Trainer class\n",
    "class ParkinsonsTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, device, \n",
    "                 criterion=None, learning_rate=3e-4, \n",
    "                 weight_decay=2e-4, checkpoint_path='best_model3.pth',\n",
    "                 class_weights=None, use_mixed_precision=True):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Set criterion based on class weights if provided\n",
    "        if criterion is None:\n",
    "            if class_weights is not None and USE_WEIGHTED_LOSS:\n",
    "                weights = torch.tensor(class_weights).to(device)\n",
    "                self.criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "                print(f\"Using weighted CrossEntropyLoss with weights: {weights}\")\n",
    "            else:\n",
    "                self.criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.criterion = criterion\n",
    "            \n",
    "        # Use AdamW optimizer (better weight decay handling)\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=weight_decay,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # # Use cosine annealing scheduler with warm restarts\n",
    "        # self.scheduler = CosineAnnealingWarmRestarts(\n",
    "        #     self.optimizer, \n",
    "        #     T_0=5,  # Restart every 5 epochs\n",
    "        #     T_mult=2,  # Double the restart interval after each restart\n",
    "        #     eta_min=1e-6  # Minimum learning rate\n",
    "        # )\n",
    "        \n",
    "        # Modify scheduler\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, \n",
    "            T_0=3,  # Shorter initial period\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_val_f1 = 0.0\n",
    "        self.early_stopping_counter = 0\n",
    "        self.use_mixed_precision = use_mixed_precision\n",
    "        \n",
    "        # Initialize gradient scaler for mixed precision training\n",
    "        self.scaler = GradScaler() if use_mixed_precision else None\n",
    "        \n",
    "        # Create directory for checkpoints if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(checkpoint_path) if os.path.dirname(checkpoint_path) else '.', exist_ok=True)\n",
    "        \n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Use tqdm for progress tracking\n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "\n",
    "            data_time = time.time()\n",
    "            \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.use_mixed_precision:\n",
    "                # Mixed precision forward pass\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # Scale loss and backward pass\n",
    "                self.scaler.scale(loss).backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                # Standard precision\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # # Statistics\n",
    "            # running_loss += loss.item()\n",
    "            # _, predicted = outputs.max(1)\n",
    "            # total += targets.size(0)\n",
    "            # correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Statistics\n",
    "            if not torch.isnan(loss):\n",
    "                running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            \n",
    "            batch_time = time.time() - data_time\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss/(batch_idx+1),\n",
    "                'acc': 100.*correct/total,\n",
    "                'time/batch': f\"{batch_time:.2f}s\"\n",
    "            })\n",
    "            \n",
    "            # Clear memory every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                del inputs, targets, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch completed in {epoch_time:.2f}s ({epoch_time/60:.2f}min)\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader) if len(self.train_loader) > 0 else float('inf')\n",
    "        epoch_acc = 100. * correct / total if total > 0 else 0\n",
    "    \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(self.val_loader, desc=\"Validating\"):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                if self.use_mixed_precision:\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        loss = self.criterion(outputs, targets)\n",
    "                else:\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                # Store for F1 score calculation\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "                # Clear memory\n",
    "                del inputs, targets, outputs\n",
    "                \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        val_loss = running_loss / len(self.val_loader)\n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        # Calculate F1 score for validation\n",
    "        from sklearn.metrics import f1_score\n",
    "        val_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        return val_loss, val_acc, val_f1\n",
    "    \n",
    "    def save_checkpoint(self, is_best=True):\n",
    "        state = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'best_val_f1': self.best_val_f1\n",
    "        }\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Saving best checkpoint with validation accuracy: {self.best_val_acc:.2f}%, F1: {self.best_val_f1:.4f}\")\n",
    "            torch.save(state, self.checkpoint_path)\n",
    "        \n",
    "        # Also save periodic checkpoint every 5 epochs\n",
    "        epoch = state.get('epoch', 0)\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(state, f'checkpoint_epoch_{epoch}.pth')\n",
    "    \n",
    "    def train(self, num_epochs, early_stopping_patience=7):\n",
    "        start_time = time.time()\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs, val_f1s = [], [], []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print('-' * 60)\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_one_epoch()\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Step the scheduler\n",
    "            self.scheduler.step()\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current learning rate: {current_lr:.8f}\")\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc, val_f1 = self.validate()\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            val_f1s.append(val_f1)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Save checkpoint if validation metrics improve\n",
    "            is_best = False\n",
    "            if val_acc > self.best_val_acc or (val_acc == self.best_val_acc and val_f1 > self.best_val_f1):\n",
    "                self.best_val_acc = val_acc\n",
    "                self.best_val_f1 = val_f1\n",
    "                is_best = True\n",
    "                self.save_checkpoint(is_best=True)\n",
    "                self.early_stopping_counter = 0\n",
    "            else:\n",
    "                self.early_stopping_counter += 1\n",
    "                print(f\"EarlyStopping counter: {self.early_stopping_counter} out of {early_stopping_patience}\")\n",
    "                \n",
    "                if self.early_stopping_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        hours, remainder = divmod(training_time, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        print(f\"Training completed in {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "        \n",
    "        # Plot training/validation metrics\n",
    "        self.plot_training_metrics(train_losses, val_losses, train_accs, val_accs, val_f1s)\n",
    "        \n",
    "        return train_losses, train_accs, val_losses, val_accs, val_f1s\n",
    "    \n",
    "    def plot_training_metrics(self, train_losses, val_losses, train_accs, val_accs, val_f1s=None):\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot accuracies\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(train_accs, label='Train Accuracy')\n",
    "        plt.plot(val_accs, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot F1 scores if available\n",
    "        if val_f1s:\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(val_f1s, label='Validation F1 Score', color='green')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.title('Validation F1 Score')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def load_best_model(self):\n",
    "        try:\n",
    "            checkpoint = torch.load(self.checkpoint_path)\n",
    "            self.model.load_state_dict(checkpoint['model'])\n",
    "            print(f\"Loaded best model from {self.checkpoint_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load checkpoint: {e}. Using current model state.\")\n",
    "            return False\n",
    "    \n",
    "    def test(self, save_predictions=True):\n",
    "        # Load best model for testing\n",
    "        self.load_best_model()\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(self.test_loader, desc=\"Testing\"):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                if self.use_mixed_precision:\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        outputs = self.model(inputs)\n",
    "                else:\n",
    "                    outputs = self.model(inputs)\n",
    "                \n",
    "                probs = nn.Softmax(dim=1)(outputs)\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = outputs.max(1)\n",
    "                \n",
    "                # Store for evaluation\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                all_probs.extend(probs[:, 1].cpu().numpy())  # Store probability of class 1 (PD)\n",
    "                \n",
    "                # Clear memory\n",
    "                del inputs, targets, outputs\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_probs = np.array(all_probs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_acc = 100. * np.mean(all_preds == all_targets)\n",
    "        print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        report = classification_report(all_targets, all_preds, \n",
    "                                      target_names=['Healthy Control', 'Parkinson\\'s Disease'])\n",
    "        print(report)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        self.plot_confusion_matrix(cm)\n",
    "        \n",
    "        # ROC Curve\n",
    "        self.plot_roc_curve(all_targets, all_probs)\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        self.plot_pr_curve(all_targets, all_probs)\n",
    "        \n",
    "        # Save predictions if requested\n",
    "        if save_predictions:\n",
    "            np.savez('test_predictions.npz', \n",
    "                    predictions=all_preds,\n",
    "                    targets=all_targets,\n",
    "                    probabilities=all_probs)\n",
    "        \n",
    "        return test_acc, all_preds, all_targets, all_probs\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Healthy Control', 'Parkinson\\'s Disease'],\n",
    "                   yticklabels=['Healthy Control', 'Parkinson\\'s Disease'])\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_roc_curve(self, y_true, y_score):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curve.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_pr_curve(self, y_true, y_score):\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        pr_auc = average_precision_score(y_true, y_score)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(recall, precision, color='green', lw=2, \n",
    "                label=f'Precision-Recall curve (AP = {pr_auc:.3f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pr_curve.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "# Memory optimization functions\n",
    "def optimize_memory():\n",
    "    # Empty CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # Collect garbage\n",
    "    gc.collect()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Set device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Check available memory before starting\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Available GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB used, \"\n",
    "              f\"{(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB free\")\n",
    "\n",
    "    \n",
    "    # Load your data loaders here\n",
    "    # Assuming train_loader, val_loader, test_loader are defined elsewhere\n",
    "    # If they're not available, replace with placeholders\n",
    "    try:\n",
    "        print(\"Loading data loaders...\")\n",
    "        # train_loader, val_loader, test_loader should be defined here or imported\n",
    "        assert 'train_loader' in globals(), \"train_loader not defined\"\n",
    "        assert 'val_loader' in globals(), \"val_loader not defined\"\n",
    "        assert 'test_loader' in globals(), \"test_loader not defined\"\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "        print(\"You'll need to define your data loaders before running this script\")\n",
    "        \n",
    "    # Calculate class weights for balanced loss (if applicable)\n",
    "    class_weights = None\n",
    "    if USE_WEIGHTED_LOSS:\n",
    "        try:\n",
    "            # This is a simplified approach - adjust based on your actual class distribution\n",
    "            # You might need to iterate through your dataset to get accurate counts\n",
    "            num_healthy = 18834  # Based on your test set statistics\n",
    "            num_parkinsons = 23052\n",
    "            total = num_healthy + num_parkinsons\n",
    "            \n",
    "            # Calculate inverse frequency\n",
    "            weight_healthy = total / (2 * num_healthy)\n",
    "            weight_parkinsons = total / (2 * num_parkinsons)\n",
    "            \n",
    "            class_weights = [weight_healthy, weight_parkinsons]\n",
    "            print(f\"Calculated class weights: {class_weights}\")\n",
    "        except:\n",
    "            print(\"Could not calculate class weights, using unweighted loss\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_densenet_model(num_classes=2, dropout_rate=0.3)\n",
    "    print(\"Model created\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ParkinsonsTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=DEVICE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        checkpoint_path=CHECKPOINT_PATH,\n",
    "        class_weights=class_weights,\n",
    "        use_mixed_precision=USE_MIXED_PRECISION\n",
    "    )\n",
    "    print(\"Trainer initialized\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train(num_epochs=NUM_EPOCHS, early_stopping_patience=EARLY_STOPPING_PATIENCE)\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_acc, all_preds, all_targets, all_probs = trainer.test()\n",
    "    \n",
    "    print(\"Complete! Check the saved model and evaluation outputs.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Run main function\n",
    "    try:\n",
    "        main() \n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e):\n",
    "            print(\"\\n🚨 CUDA OUT OF MEMORY ERROR\")\n",
    "            print(\"Try one of these solutions:\")\n",
    "            print(\"1. Reduce batch size (e.g., BATCH_SIZE = 16 or 8)\")\n",
    "            print(\"2. Disable mixed precision training (USE_MIXED_PRECISION = False)\")\n",
    "            print(\"3. Use a smaller DenseNet variant\")\n",
    "            print(\"4. Resize images to smaller dimensions\")\n",
    "            optimize_memory()\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9190c-d7b9-4157-8233-fcd20379d74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
