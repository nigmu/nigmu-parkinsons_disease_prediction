{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181fe4aa-98f5-4a0f-99fc-29309dc995bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 154732 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/train', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/train']\n",
      "✅ Loaded 37807 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/val', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/val']\n",
      "✅ Loaded 41886 samples from ['/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR/test', '/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian/test']\n",
      "✅ DataLoaders ready! Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOTS = [\n",
    "    r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR\",\n",
    "    r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/Italian\"\n",
    "]\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4  # Optimize based on CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ParkinsonSpectrogramDataset(Dataset):\n",
    "    def __init__(self, root_dirs, transform=None):\n",
    "        if isinstance(root_dirs, str):  # If a single path is given, convert it to a list\n",
    "            root_dirs = [root_dirs]\n",
    "        self.root_dirs = root_dirs\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for root_dir in self.root_dirs:\n",
    "            for class_name in ['HC', 'PD']:\n",
    "                class_dir = os.path.join(root_dir, class_name)\n",
    "                if not os.path.exists(class_dir):\n",
    "                    continue\n",
    "\n",
    "                # Traverse subfolders inside HC/PD\n",
    "                for patient_folder in os.listdir(class_dir):\n",
    "                    patient_path = os.path.join(class_dir, patient_folder)\n",
    "                    if os.path.isdir(patient_path):  # Ensure it's a directory\n",
    "                        for img_file in os.listdir(patient_path):\n",
    "                            if img_file.lower().endswith('.png'):  # Only PNG images\n",
    "                                img_path = os.path.join(patient_path, img_file)\n",
    "                                samples.append((img_path, 0 if class_name == 'HC' else 1))\n",
    "\n",
    "        print(f\"✅ Loaded {len(samples)} samples from {self.root_dirs}\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Data Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Only slight translations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize between -1 and 1\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Create Datasets (Loading from both MDVR & Italian datasets)\n",
    "train_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'train') for root in DATA_ROOTS], transform=train_transform)\n",
    "val_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'val') for root in DATA_ROOTS], transform=test_transform)\n",
    "test_dataset = ParkinsonSpectrogramDataset([os.path.join(root, 'test') for root in DATA_ROOTS], transform=test_transform)\n",
    "\n",
    "# Optimized DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"✅ DataLoaders ready! Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ab6068-e9b2-47b7-ab6d-15c6e9e14655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f07f0fb-08fc-4851-8791-24660c2ded14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 5)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_capability(device))\n",
    "# Should return (7, 5) or higher for good AMP support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212f386-906e-40b0-a4b2-5c3b0bece99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%| | 57/4836 [01:24<1:29:17,  1.12s/it"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ==================\n",
    "# Model Definition\n",
    "# ==================\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Load with weights=None and disable unnecessary features\n",
    "        self.densenet = models.densenet121(weights=None, progress=False)\n",
    "        \n",
    "        # Optimized conv0 modification\n",
    "        self.densenet.features.conv0 = nn.Conv2d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        # Initialize with better memory alignment\n",
    "        nn.init.kaiming_normal_(self.densenet.features.conv0.weight, mode='fan_out')\n",
    "        \n",
    "        # Replace classifier with more efficient version\n",
    "        in_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.amp.autocast(device_type = 'cuda'):  # Built-in autocast\n",
    "            return self.densenet(x)\n",
    "\n",
    "\n",
    "# ==================\n",
    "# Training Setup\n",
    "# ==================\n",
    "\n",
    "def create_model(num_classes, device):\n",
    "    model = DenseNet121(num_classes).to(device)\n",
    "    \n",
    "    # Weight initialization\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ==================\n",
    "# Training Loop\n",
    "# ==================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_classes, device, \n",
    "                input_size=(224, 224), mean=0.5, std=0.5, num_epochs=30):\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Hyperparameters\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Time tracking\n",
    "    total_start = time.time()\n",
    "    epoch_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):  # Adjust epochs as needed\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Progress bar setup\n",
    "        batch_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}', \n",
    "                         leave=False, dynamic_ncols=True)\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(batch_pbar):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # On-the-fly preprocessing\n",
    "            inputs = torch.nn.functional.interpolate(inputs, size=input_size, \n",
    "                                                    mode='bicubic', \n",
    "                                                    align_corners=False)\n",
    "            inputs = (inputs - mean) / std  # Normalize\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast(device_type = 'cuda'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            batch_pbar.set_postfix({\n",
    "                'Loss': f\"{loss.item():.4f}\",\n",
    "                'Acc': f\"{100.*correct/total:.2f}%\",\n",
    "                'GPU': f\"{torch.cuda.memory_allocated()/1e9:.2f}GB\"\n",
    "            })\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate_model(model, val_loader, num_classes, device, \n",
    "                                    input_size, mean, std)\n",
    "        \n",
    "        # Epoch timing\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_times.append(epoch_time)\n",
    "        avg_time = np.mean(epoch_times)\n",
    "        remaining = avg_time * (50 - epoch - 1)\n",
    "        \n",
    "        # Progress update\n",
    "        print(f\"\\nEpoch {epoch+1:02d} Summary:\")\n",
    "        print(f\"Train Loss: {train_loss/(batch_idx+1):.4f} | Acc: {100.*correct/total:.2f}%\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f} | Acc: {100.*val_metrics['accuracy']:.2f}%\")\n",
    "        print(f\"Time: {epoch_time:.1f}s | Total: {time.time()-total_start:.1f}s | \"\n",
    "              f\"ETA: {remaining:.1f}s\\n\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_acc:\n",
    "            best_acc = val_metrics['accuracy']\n",
    "            torch.save(model.state_dict(), 'best_model4.pth')\n",
    "        \n",
    "        scheduler.step(val_metrics['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ==================\n",
    "# Evaluation\n",
    "# ==================\n",
    "\n",
    "def evaluate_model(model, loader, num_classes, device, input_size, mean, std):\n",
    "    model.eval()\n",
    "    loss_total = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc='Evaluating', leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Preprocessing\n",
    "            inputs = torch.nn.functional.interpolate(inputs, size=input_size, \n",
    "                                                    mode='bicubic', \n",
    "                                                    align_corners=False)\n",
    "            inputs = (inputs - mean) / std\n",
    "            \n",
    "            # Inference\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store results\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            loss_total += nn.CrossEntropyLoss()(outputs, labels).item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "    return {\n",
    "        'loss': loss_total/len(loader),\n",
    "        'accuracy': report['accuracy'],\n",
    "        'precision': report['weighted avg']['precision'],\n",
    "        'recall': report['weighted avg']['recall'],\n",
    "        'f1': report['weighted avg']['f1-score'],\n",
    "        'report': report\n",
    "    }\n",
    "\n",
    "# ==================\n",
    "# Main Execution\n",
    "# ==================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.backends.cudnn.benchmark = True  # Optimize for fixed input size\n",
    "    \n",
    "    # User configuration\n",
    "    num_classes = 10  # Set according to your dataset\n",
    "    input_size = (224, 224)  # Optimal size determined through experiments\n",
    "    mean = 0.5  # Calculate from your dataset\n",
    "    std = 0.5   # Calculate from your dataset\n",
    "    \n",
    "    # Initialize model\n",
    "    model = create_model(num_classes, device)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model = train_model(model, train_loader, val_loader, num_classes, \n",
    "                               device, input_size, mean, std)\n",
    "    \n",
    "    # Final evaluation\n",
    "    test_metrics = evaluate_model(trained_model, test_loader, num_classes,\n",
    "                                 device, input_size, mean, std)\n",
    "    \n",
    "    print(\"\\nFinal Test Results:\")\n",
    "    print(f\"Accuracy: {100.*test_metrics['accuracy']:.2f}%\")\n",
    "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da895683-7b21-467e-945e-9b8786db652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794048fa-2238-4909-9c33-7fcf1d987332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45799a24-a014-4b60-b428-af7f3a1e5651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e5470-1243-4817-8b1d-a52a89cda769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
