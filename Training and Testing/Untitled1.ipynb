{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955978bb-a860-4b28-90a3-2e3cbdffeb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41744 samples for train split\n",
      "Loaded 13509 samples for val split\n",
      "Loaded 8929 samples for test split\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CustomCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 522\u001b[39m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history, (test_loss, test_acc)\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# To run the experiment:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m model, history, test_results = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# data_dir=\"C:/NPersonal/Projects/SDP/Prediction Stuff/Dataset/MDVR\",\u001b[39;49;00m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_augmentation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmixed_freq_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or 'none', 'mixup', 'freq_mask'\u001b[39;49;00m\n\u001b[32m    528\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 464\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(data_dir, batch_size, num_epochs, use_augmentation)\u001b[39m\n\u001b[32m    458\u001b[39m dataloaders = {\n\u001b[32m    459\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m: train_loader,\n\u001b[32m    460\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m: val_loader\n\u001b[32m    461\u001b[39m }\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# Create model, loss function, and optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m model = \u001b[43mCustomCNN\u001b[49m(num_classes=\u001b[32m2\u001b[39m)\n\u001b[32m    465\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m    466\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'CustomCNN' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Custom dataset class for MDVR dataset\n",
    "class MDVRDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the data.\n",
    "            split (string): 'train', 'test', or 'val'\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.classes = ['HC', 'PD']  # healthy control, Parkinson's disease\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        self.samples = []\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        split_dir = os.path.join(self.root_dir, self.split)\n",
    "        for class_name in self.classes:\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            \n",
    "            # Iterate through patient folders\n",
    "            for patient_folder in os.listdir(class_dir):\n",
    "                patient_path = os.path.join(class_dir, patient_folder)\n",
    "                if os.path.isdir(patient_path):\n",
    "                    # Iterate through image files in patient folder\n",
    "                    for img_name in os.listdir(patient_path):\n",
    "                        if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            img_path = os.path.join(patient_path, img_name)\n",
    "                            self.samples.append((img_path, class_idx, patient_folder))\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} samples for {self.split} split\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_idx, patient_id = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        # image = Image.open(img_path).convert('RGB')\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, class_idx, patient_id\n",
    "\n",
    "\n",
    "# Augmentation techniques based on the paper\n",
    "\n",
    "# 1. Enhanced Augmentation Classes\n",
    "class SpecAugment:\n",
    "    \"\"\"Combined time and frequency masking\"\"\"\n",
    "    def __init__(self, freq_mask=30, time_mask=50, num_masks=2):\n",
    "        self.freq_mask = freq_mask\n",
    "        self.time_mask = time_mask\n",
    "        self.num_masks = num_masks\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        _, n_mels, n_time = img_tensor.shape\n",
    "        \n",
    "        # Frequency masking\n",
    "        for _ in range(self.num_masks):\n",
    "            f = np.random.randint(0, self.freq_mask)\n",
    "            f0 = np.random.randint(0, n_mels - f)\n",
    "            img_tensor[:, f0:f0+f, :] = 0\n",
    "            \n",
    "        # Time masking\n",
    "        for _ in range(self.num_masks):\n",
    "            t = np.random.randint(0, self.time_mask)\n",
    "            t0 = np.random.randint(0, n_time - t)\n",
    "            img_tensor[:, :, t0:t0+t] = 0\n",
    "            \n",
    "        return transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "class GaussianNoise:\n",
    "    \"\"\"Add Gaussian noise to spectrograms\"\"\"\n",
    "    def __init__(self, std=0.01):\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std\n",
    "\n",
    "        \n",
    "\n",
    "# 1. Frequency Masking\n",
    "class FrequencyMasking:\n",
    "    def __init__(self, max_width=30, num_masks=1):\n",
    "        self.max_width = max_width\n",
    "        self.num_masks = num_masks\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        _, h, w = img_tensor.shape\n",
    "        \n",
    "        for _ in range(self.num_masks):\n",
    "            f = np.random.randint(0, self.max_width)\n",
    "            f0 = np.random.randint(0, h - f)\n",
    "            \n",
    "            # Apply frequency mask\n",
    "            img_tensor[:, f0:f0+f, :] = 0\n",
    "        \n",
    "        return transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "# 2. Mixup\n",
    "class Mixup:\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, batch_x, batch_y):\n",
    "        \"\"\"Apply mixup to a batch of images and labels\"\"\"\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        batch_size = batch_x.size(0)\n",
    "        index = torch.randperm(batch_size).to(batch_x.device)\n",
    "        \n",
    "        mixed_x = lam * batch_x + (1 - lam) * batch_x[index]\n",
    "        y_a, y_b = batch_y, batch_y[index]\n",
    "        \n",
    "        return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# 3. Mixed Frequency Masking\n",
    "class MixedFrequencyMasking:\n",
    "    def __init__(self, max_width=30, num_masks=1):\n",
    "        self.max_width = max_width\n",
    "        self.num_masks = num_masks\n",
    "    \n",
    "    def __call__(self, batch_x, batch_y):\n",
    "        \"\"\"Apply mixed frequency masking to a batch of images and labels\"\"\"\n",
    "        batch_size = batch_x.size(0)\n",
    "        device = batch_x.device\n",
    "        _, h, w = batch_x[0].shape\n",
    "        \n",
    "        # Create copies for mixing\n",
    "        mixed_x = batch_x.clone()\n",
    "        \n",
    "        # For storing mixing ratios\n",
    "        mixing_ratios = torch.ones(batch_size, device=device)\n",
    "        \n",
    "        # For each image in the batch\n",
    "        for i in range(batch_size):\n",
    "            # Select another random image to mix with\n",
    "            j = (i + torch.randint(1, batch_size, (1,)).item()) % batch_size\n",
    "            \n",
    "            # Apply frequency replacements\n",
    "            total_freq_replaced = 0\n",
    "            \n",
    "            for _ in range(self.num_masks):\n",
    "                # Random frequency mask width\n",
    "                f = torch.randint(1, self.max_width + 1, (1,)).item()\n",
    "                # Random starting frequency\n",
    "                f0 = torch.randint(0, h - f, (1,)).item()\n",
    "                \n",
    "                # Replace frequency band in image i with the same band from image j\n",
    "                mixed_x[i, :, f0:f0+f, :] = batch_x[j, :, f0:f0+f, :]\n",
    "                \n",
    "                # Accumulate total frequency replaced\n",
    "                total_freq_replaced += f\n",
    "            \n",
    "            # Calculate mixing ratio based on proportion of frequency replaced\n",
    "            mixing_ratio = total_freq_replaced / h\n",
    "            mixing_ratios[i] = 1 - mixing_ratio\n",
    "            \n",
    "        # Get indices for mixing labels\n",
    "        indices = torch.remainder(torch.arange(batch_size) + 1, batch_size).to(device)\n",
    "        y_a, y_b = batch_y, batch_y[indices]\n",
    "        \n",
    "        return mixed_x, y_a, y_b, mixing_ratios\n",
    "\n",
    "# 2. Enhanced CNN Architecture\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        \n",
    "        # Input: 1x496x200 (mel spectrogram dimensions)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 32x248x100\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 64x124x50\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d((2,4)),  # 128x62x12\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Training function with augmentation\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, device='cuda', \n",
    "                use_mixup=False, use_mixed_freq_mask=False):\n",
    "    \"\"\"\n",
    "    Training function with support for different augmentation techniques\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Mixed precision\n",
    "    \n",
    "    \n",
    "    # Initialize mixup and mixed frequency masking if used\n",
    "    mixup_fn = Mixup(alpha=0.2) if use_mixup else None\n",
    "    mixed_freq_mask_fn = MixedFrequencyMasking(max_width=30, num_masks=1) if use_mixed_freq_mask else None\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    # Track total training time\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                \n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            phase_start_time = time.time()\n",
    "            \n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels, _ in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Apply augmentation techniques during training\n",
    "                    if phase == 'train':\n",
    "                        if use_mixup:\n",
    "                            inputs, labels_a, labels_b, lam = mixup_fn(inputs, labels)\n",
    "                            outputs = model(inputs)\n",
    "                            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                        elif use_mixed_freq_mask:\n",
    "                            inputs, labels_a, labels_b, mixing_ratios = mixed_freq_mask_fn(inputs, labels)\n",
    "                            outputs = model(inputs)\n",
    "                            # Apply per-sample mixing ratios\n",
    "                            batch_loss = 0\n",
    "                            for i in range(inputs.size(0)):\n",
    "                                ratio = mixing_ratios[i]\n",
    "                                batch_loss += ratio * criterion(outputs[i:i+1], labels_a[i:i+1]) + \\\n",
    "                                             (1 - ratio) * criterion(outputs[i:i+1], labels_b[i:i+1])\n",
    "                            loss = batch_loss / inputs.size(0)\n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            phase_time = time.time() - phase_start_time\n",
    "            print(f'{phase} phase completed in {phase_time:.2f}s - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Record history\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                \n",
    "                # Save best model\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch+1} completed in {epoch_time:.2f} seconds')\n",
    "        print('=' * 50)\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f'\\nTotal training time: {total_time:.2f} seconds')\n",
    "    \n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, history\n",
    "\n",
    "# Function to test the model\n",
    "def test_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    phase_start_time = time.time()\n",
    "    \n",
    "    # For confusion matrix\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in tqdm(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "    \n",
    "    return test_loss, test_acc, all_preds, all_labels\n",
    "\n",
    "# Main execution function\n",
    "def run_experiment(data_dir, batch_size=32, num_epochs=50, use_augmentation='mixed_freq_mask'):\n",
    "    \"\"\"\n",
    "    Run the complete experiment\n",
    "    Args:\n",
    "        data_dir: Root directory of the MDVR dataset\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        use_augmentation: Type of augmentation to use ('none', 'mixup', 'freq_mask', 'mixed_freq_mask')\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_start = time.time()\n",
    "    # Define transforms\n",
    "    # Base transform (no augmentation)\n",
    "    \n",
    "    # base_transform = transforms.Compose([\n",
    "    #     transforms.Resize((496, 200)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize((496, 200)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Single channel normalization\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Transform with frequency masking\n",
    "    # freq_mask_transform = transforms.Compose([\n",
    "    #     transforms.Resize((496, 200)),\n",
    "    #     FrequencyMasking(max_width=30, num_masks=1),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # ])\n",
    "\n",
    "    freq_mask_transform = transforms.Compose([\n",
    "        transforms.Resize((496, 200)),\n",
    "        FrequencyMasking(max_width=30, num_masks=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Single channel normalization\n",
    "    ])\n",
    "    \n",
    "    # Choose the appropriate transform based on augmentation type\n",
    "    if use_augmentation == 'freq_mask':\n",
    "        train_transform = freq_mask_transform\n",
    "    else:\n",
    "        train_transform = base_transform\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MDVRDataset(data_dir, split='train', transform=train_transform)\n",
    "    val_dataset = MDVRDataset(data_dir, split='val', transform=base_transform)\n",
    "    test_dataset = MDVRDataset(data_dir, split='test', transform=base_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    # Create model, loss function, and optimizer\n",
    "    model = CustomCNN(num_classes=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set augmentation flags\n",
    "    use_mixup = (use_augmentation == 'mixup')\n",
    "    use_mixed_freq_mask = (use_augmentation == 'mixed_freq_mask')\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training with augmentation: {use_augmentation}\")\n",
    "    model, history = train_model(\n",
    "        model, dataloaders, criterion, optimizer, \n",
    "        num_epochs=num_epochs, device=device,\n",
    "        use_mixup=use_mixup, use_mixed_freq_mask=use_mixed_freq_mask\n",
    "    )\n",
    "    \n",
    "    # Test model\n",
    "    # test_loss, test_acc, all_preds, all_labels = test_model(model, test_loader, criterion, device)\n",
    "\n",
    "    # Test model\n",
    "    test_start = time.time()\n",
    "    test_loss, test_acc, all_preds, all_labels = test_model(model, test_loader, criterion, device)\n",
    "    test_time = time.time() - test_start\n",
    "    print(f'\\nTesting completed in {test_time:.2f} seconds')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Training')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Training')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "\n",
    "    total_time = time.time() - experiment_start\n",
    "    print(f'\\nTotal execution time: {total_time:.2f} seconds')\n",
    "    \n",
    "    return model, history, (test_loss, test_acc)\n",
    "\n",
    "# Example usage\n",
    "# To run the experiment:\n",
    "model, history, test_results = run_experiment(\n",
    "    # data_dir=\"C:/NPersonal/Projects/SDP/Prediction Stuff/Dataset/MDVR\",\n",
    "    data_dir=r\"/home/nigmu/NPersonal/Projects/SDP/nigmu-parkinsons_disease_prediction/Dataset/MDVR\",\n",
    "    batch_size=32, \n",
    "    num_epochs=50,\n",
    "    use_augmentation='mixed_freq_mask'  # or 'none', 'mixup', 'freq_mask'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a164488-90e1-4652-b583-29b58ebb9bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba8ee3-128f-468a-b548-1dc8e93fe8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f210b3-f32e-4bff-a3b7-924c2f8f1e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff14bf3-523f-47eb-9c6d-0fee161e6f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c2b97-781f-4940-8d33-5e6a96844bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
